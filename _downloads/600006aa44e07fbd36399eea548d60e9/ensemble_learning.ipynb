{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Non-Linear Ensemble Learning\n\nSources:\n\n- [Scikit-learn API](https://scikit-learn.org/stable/api/sklearn.ensemble.html)\n- [Scikit-learn doc](https://scikit-learn.org/stable/modules/ensemble.html)\n\n\n## Introduction to Ensemble Learning\n\nEnsemble learning is a powerful machine learning technique that combines multiple models to achieve better performance than any individual model. By aggregating predictions from diverse learners, ensemble methods enhance accuracy, reduce variance, and improve generalization. The main advantages of ensemble learning include:\n\n- **Reduced overfitting**: By averaging multiple models, ensemble methods mitigate overfitting risks.\n- **Increased robustness**: The diversity of models enhances stability, making the approach more resistant to noise and biases.\n\nThere are three main types of ensemble learning techniques: **Bagging, Boosting, and Stacking**. Each method follows a unique strategy to combine multiple models and improve overall performance.\n\nConclusion\n\nEnsemble learning is a fundamental approach in machine learning that significantly enhances predictive performance. **Bagging** helps reduce variance, **boosting** improves bias, and **stacking** leverages multiple models to optimize performance. By carefully selecting and tuning ensemble techniques, practitioners can build powerful and robust machine learning models suitable for various real-world applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Breast cancer dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "breast_cancer = datasets.load_breast_cancer()\nX, y = breast_cancer.data, breast_cancer.target\nprint(breast_cancer.feature_names)\n\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.5, stratify=y, random_state=42)\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision tree\n\nA tree can be \"learned\" by splitting the training dataset into subsets based on an features value test.\nEach internal node represents a \"test\" on an feature resulting on the split of the current sample. At each step the algorithm selects the feature and a cutoff value that maximises a given metric. Different metrics exist for regression tree (target is continuous) or classification tree (the target is qualitative).\nThis process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This general principle is implemented by many recursive partitioning tree algorithms.\n\n.. figure:: ../ml_supervised/images/classification_tree.png\n   :width: 400\n   :alt: Classification tree.\n\nDecision trees are simple to understand and interpret however they tend to overfit the data. However decision trees tend to overfit the training set.  Leo Breiman propose random forest to deal with this issue.\n\nA single decision tree is usually overfits the data it is learning from because it learn from only one pathway of decisions. Predictions from a single decision tree usually don\u2019t make accurate predictions on new data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)\n\ny_pred = tree.predict(X_test)\ny_prob = tree.predict_proba(X_test)[:, 1]\nprint(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bagging (Bootstrap Aggregating): Random forest\n\nBagging is an ensemble method that aims to reduce variance by training multiple models on different subsets of the training data. It follows these steps:\n\n1. Generate multiple bootstrap samples (randomly drawn with replacement) from the original dataset.\n2. Train an independent model (typically a weak learner like a decision tree) on each bootstrap sample.\n3. Aggregate predictions using majority voting (for classification) or averaging (for regression).\n\n**Example:** The **Random Forest** algorithm is a widely used bagging method that constructs multiple decision trees and combines their predictions.\n\n**Key Benefits:**\n\n- Reduces variance and improves stability.\n- Works well with high-dimensional data.\n- Effective for handling noisy datasets.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bagging_tree = BaggingClassifier(DecisionTreeClassifier())\nbagging_tree.fit(X_train, y_train)\n\ny_pred = bagging_tree.predict(X_test)\ny_prob = bagging_tree.predict_proba(X_test)[:, 1]\nprint(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest\n\nA random forest is a meta estimator that fits a number of **decision tree learners** on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.\nRandom forest models reduce the risk of overfitting by introducing randomness by:\n\n.. figure:: ../ml_supervised/images/random_forest.png\n   :width: 300\n   :alt: Random forest.\n\n- building multiple trees (n_estimators)\n- drawing observations with replacement (i.e., a bootstrapped sample)\n- splitting nodes on the best split among a random subset of the features selected at every node\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "forest = RandomForestClassifier(n_estimators=100)\nforest.fit(X_train, y_train)\n\ny_pred = forest.predict(X_test)\ny_prob = forest.predict_proba(X_test)[:, 1]\nprint(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Boosting and Gradient boosting\n\nBoosting is an ensemble method that focuses on reducing bias by training models sequentially, where each new model corrects the errors of its predecessors. The process includes:\n\n1. Train an initial weak model on the training data.\n2. Assign higher weights to misclassified instances to emphasize difficult cases.\n3. Train a new model on the updated dataset, repeating the process iteratively.\n4. Combine the predictions of all models using a weighted sum.\n%%\nGradient boosting\n~~~~~~~~~~~~~~~~~\n\nPopular boosting algorithms include **AdaBoost**, **Gradient Boosting Machines (GBM)**, **XGBoost**, and **LightGBM**.\n\n**Key Benefits:**\n\n- Improves accuracy by focusing on difficult instances.\n- Works well with structured data and tabular datasets.\n- Reduces bias while maintaining interpretability.\n\nThe two main hyper-parameters are:\n\n- The **learning rate** (*lr*) controls over-fitting:\n  decreasing the *lr* limits the capacity of a learner to overfit the residuals, ie,\n  it slows down the learning speed and thus increases the **regularization**.\n\n- The **sub-sampling fraction** controls the fraction of samples to be used for\n  fitting the learners. Values smaller than 1 leads to **Stochastic Gradient Boosting**.\n  It thus controls for over-fitting reducing variance and increasing bias.\n\n.. figure:: ../ml_supervised/images/gradient_boosting.png\n   :width: 500\n   :alt: Gradient boosting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1,\n                                subsample=0.5, random_state=0)\ngb.fit(X_train, y_train)\n\ny_pred = gb.predict(X_test)\ny_prob = gb.predict_proba(X_test)[:, 1]\n\nprint(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stacking\n\nStacking (or stacked generalization) is a more complex ensemble technique that combines predictions from multiple base models using a **meta-model**. The process follows:\n\n1. Train several base models (e.g., decision trees, SVMs, neural networks) on the same dataset.\n2. Collect predictions from all base models and use them as new features.\n3. Train a meta-model (often a simple regression or classification model) to learn how to best combine the base predictions.\n\n**Example:** Stacking can combine weak and strong learners, such as decision trees, logistic regression, and deep learning models, to create a robust final model.\n\n**Key Benefits:**\n\n- Allows different types of models to complement each other.\n- Captures complex relationships between models.\n- Can outperform traditional ensemble methods when well-tuned.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import LinearSVC\nfrom sklearn.pipeline import make_pipeline\n\nestimators = [\n    ('rf', RandomForestClassifier(n_estimators=10, random_state=42)),\n    ('svr', make_pipeline(StandardScaler(),\n                          LinearSVC(random_state=42)))]\n    \nstacked_trees = StackingClassifier(estimators)\nstacked_trees.fit(X_train, y_train)\n\ny_pred = stacked_trees.predict(X_test)\ny_prob = stacked_trees.predict_proba(X_test)[:, 1]\nprint(\"bAcc: %.2f, AUC: %.2f \" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}