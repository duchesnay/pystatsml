{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Non-Linear Kernel Methods and Support Vector Machines (SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot parameters\nplt.style.use('seaborn-v0_8-whitegrid')\nfig_w, fig_h = plt.rcParams.get('figure.figsize')\nplt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel algorithms\n\nKernel Machine are based kernel methods require only a user-specified kernel function\n$K(x_i, x_j)$, i.e., a **similarity function** over pairs of data\npoints $(x_i, x_j)$ into kernel (dual) space on which learning\nalgorithms operate linearly, i.e. every operation on points is a linear\ncombination of $K(x_i, x_j)$.\nOutline of the SVM algorithm:\n\n1. **Map points**  $x$ into **kernel space** using a **kernel function**:\n   $x \\rightarrow K(x, .)$.\n   Learning algorithms operates linearly by dot product into high-kernel\n   space: $K(., x_i) \\cdot K(., x_j)$.\n    - Using the kernel trick (Mercer\u2019s Theorem) replaces dot product in high\n      dimensional space by a simpler operation such that\n      $K(., x_i) \\cdot K(., x_j) = K(x_i, x_j)$.\n    - Thus we only need to compute a similarity measure $K(x_i, x_j)$ for each pairs of\n      point and store in a $N \\times N$ Gram matrix of.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM\n\n2. **The learning process** consist of estimating the $\\alpha_i$\nof the decision function that maximizes the hinge loss (of $f(x)$)\nplus some penalty when applied on all training points.\n\n3. **Prediction** of a new point $x$ using the decision function.\n\n\\begin{align}f(x) = \\text{sign} \\left(\\sum_i^N \\alpha_i~y_i~K(x_i, x)\\right).\\end{align}\n\n.. figure:: ../ml_supervised/images/svm_rbf_kernel_mapping_and_decision_function.png\n   :alt: Support Vector Machines.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Kernel function\n\nOne of the most commonly used kernel is the **Radial Basis Function (RBF) Kernel**.\nFor a pair of points $x_i, x_j$ the RBF kernel is defined as:\n\n.. raw:: latex\n\n   \\begin{align}\n      K(x_i, x_j) &= \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right)\\\\\n      &= \\exp\\left(-\\gamma~\\|x_i - x_j\\|^2\\right)\n   \\end{align}\n\nWhere $\\sigma$ (or $\\gamma$)  defines the kernel width parameter.\nBasically, we consider a Gaussian function centered on each training sample\n$x_i$.  it has a ready interpretation as a similarity measure as it\ndecreases with squared Euclidean distance between the two feature vectors.\n\nNon linear SVM also exists for regression problems.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = datasets.load_breast_cancer(return_X_y=True)\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.5, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preprocessing: unequal variance of input features, requires scaling for svm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ax = sns.displot(x=X_train.std(axis=0), kind=\"kde\", bw_adjust=.2, cut=0,\n                 fill=True, height=3, aspect=1.5,)\n_ = ax.set_xlabels(\"Std-dev\").tight_layout()\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Scikit-learn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n(Support Vector Classification) with probalility function applying a logistic of\nthe decision_function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "svm = SVC(kernel='rbf', probability=True).fit(X_train, y_train)\ny_pred = svm.predict(X_test)\ny_score = svm.decision_function(X_test)\ny_prob = svm.predict_proba(X_test)[:, 1]\n\nax = sns.relplot(x=y_score, y=y_prob, hue=y_pred, height=2, aspect=1.5)\n_ = ax.set_axis_labels(\"decision function\", \"Probability\").tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"bAcc: %.2f, AUC: %.2f (AUC with proba: %.2f)\" % (\n      metrics.balanced_accuracy_score(y_true=y_test, y_pred=y_pred),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_score),\n      metrics.roc_auc_score(y_true=y_test, y_score=y_prob)))\n\n# Usefull internals: indices of support vectors within original X\nnp.all(X_train[svm.support_, :] == svm.support_vectors_)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}