{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hands-On: Validation of Supervised Classification Pipelines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# System\nimport os\nimport os.path\nimport tempfile\nimport time\n\n# Scientific python\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Univariate statistics\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport statsmodels.stats.api as sms\n\n# Dataset\nfrom sklearn.datasets import make_classification\n\n# Models\nfrom sklearn.decomposition import PCA\nimport sklearn.linear_model as lm\nimport sklearn.svm as svm\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Metrics\nimport sklearn.metrics as metrics\n\n# Resampling\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_predict\n# from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n# from sklearn.model_selection import KFold\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input/Output and working directory\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "WD = os.path.join(tempfile.gettempdir(), \"ml_supervised_classif\")\nos.makedirs(WD, exist_ok=True)\nINPUT_DIR = os.path.join(WD, \"data\")\nOUTPUT_DIR = os.path.join(WD, \"models\")\nos.makedirs(INPUT_DIR, exist_ok=True)\nos.makedirs(OUTPUT_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation scheme here [cross_validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_splits_test = 5\ncv_test = StratifiedKFold(n_splits=n_splits_test, shuffle=True, random_state=42)\n\nn_splits_val = 5\ncv_val = StratifiedKFold(n_splits=n_splits_val, shuffle=True, random_state=42)\n\nmetrics_names = ['accuracy', 'balanced_accuracy', 'roc_auc']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = make_classification(n_samples=200, n_features=100,\n                           n_informative=10, n_redundant=10,\n                           random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mlp_param_grid = {\"hidden_layer_sizes\":\n        [(100, ), (50, ), (25, ), (10, ), (5, ),         # 1 hidden layer\n        (100, 50, ), (50, 25, ), (25, 10,), (10, 5, ),   # 2 hidden layers\n        (100, 50, 25, ), (50, 25, 10, ), (25, 10, 5, )], # 3 hidden layers\n        \"activation\": [\"relu\"], \"solver\": [\"sgd\"], 'alpha': [0.0001]}\n\nmodels = dict(\n    lrl2_cv=make_pipeline(\n        preprocessing.StandardScaler(),\n        # preprocessing.MinMaxScaler(),\n        GridSearchCV(lm.LogisticRegression(),\n                     {'C': 10. ** np.arange(-3, 1)},\n                     cv=cv_val, n_jobs=n_splits_val)),\n\n    lrenet_cv=make_pipeline(\n        preprocessing.StandardScaler(),\n        # preprocessing.MinMaxScaler(),\n        GridSearchCV(estimator=lm.SGDClassifier(loss='log_loss',\n                    penalty='elasticnet'),\n                    param_grid={'alpha': 10. ** np.arange(-1, 3),\n                                 'l1_ratio': [.1, .5, .9]},\n                    cv=cv_val, n_jobs=n_splits_val)),\n\n    svmrbf_cv=make_pipeline(\n        # preprocessing.StandardScaler(),\n        preprocessing.MinMaxScaler(),\n        GridSearchCV(svm.SVC(),\n                     # {'kernel': ['poly', 'rbf'], 'C': 10. ** np.arange(-3, 3)},\n                     {'kernel': ['rbf'], 'C': 10. ** np.arange(-1, 2)},\n                     cv=cv_val, n_jobs=n_splits_val)),\n\n    forest_cv=make_pipeline(\n        # preprocessing.StandardScaler(),\n        preprocessing.MinMaxScaler(),\n        GridSearchCV(RandomForestClassifier(random_state=1),\n                     {\"n_estimators\": [10, 100]},\n                     cv=cv_val, n_jobs=n_splits_val)),\n\n    gb_cv=make_pipeline(\n        preprocessing.MinMaxScaler(),\n        GridSearchCV(estimator=GradientBoostingClassifier(random_state=1),\n                     param_grid={\"n_estimators\": [10, 100]},\n                     cv=cv_val, n_jobs=n_splits_val)),\n\n    mlp_cv=make_pipeline(\n        # preprocessing.StandardScaler(),\n        preprocessing.MinMaxScaler(),\n        GridSearchCV(estimator=MLPClassifier(random_state=1, max_iter=200, tol=0.0001),\n                     param_grid=mlp_param_grid,\n                     cv=cv_val, n_jobs=n_splits_val)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit/Predict and Compute Test Score (CV)\n\nFit/predict models and return scores on folds using: [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n\nHere we set:\n\n- ``return_estimator=True`` to return the estimator fitted on each training set\n- ``return_indices=True`` to return the training and testing indices used to split the dataset into train and test sets for each cv split.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models_scores = dict()\n\nfor name, model in models.items():\n    # name, model = \"lrl2_cv\", models[\"lrl2_cv\"]\n    start_time = time.time()\n    models_scores_ = cross_validate(estimator=model, X=X, y=y, cv=cv_test,\n                                  n_jobs=n_splits_test,\n                                  scoring=metrics_names,\n                                  return_estimator=True,\n                                  return_indices=True)\n    print(name, 'Elapsed time: \\t%.3f sec' % (time.time() - start_time))\n    models_scores[name] = models_scores_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Average Test Scores (CV) and save it to a file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_stat = [[name] + [res[\"test_\" + metric].mean() for metric in metrics_names]\n             for name, res in models_scores.items()]\n\ntest_stat = pd.DataFrame(test_stat, columns=[\"model\"]+metrics_names)\ntest_stat.to_csv(os.path.join(OUTPUT_DIR, \"test_stat.csv\"))\nprint(test_stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve Individuals Predictions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1. Retrieve individuals predictions and save individuals predictions in csv file**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Iterate over models\npredictions = pd.DataFrame()\nfor name, model in models_scores.items():\n    # name, model = \"lrl2_cv\", models_scores[\"lrl2_cv\"]\n    # model_scores = models_scores[\"lrl2_cv\"]\n\n    pred_vals_test = np.full(y.shape, np.nan) # Predicted values before threshold\n    pred_vals_train = np.full(y.shape, np.nan) # Predicted values before threshold\n    pred_labs_test = np.full(y.shape, np.nan) # Predicted labels\n    pred_labs_train = np.full(y.shape, np.nan) # Predicted labels\n    true_labs = np.full(y.shape, np.nan) # True labels\n    fold_nb = np.full(y.shape, np.nan) # True labels\n\n    # Iterate over folds\n    for fold in range(len(model['estimator'])):\n        est = model['estimator'][fold]\n        test_idx = model['indices']['test'][fold]\n        train_idx = model['indices']['train'][fold]\n        X_test = X[test_idx]\n        X_train = X[train_idx]\n\n        # Predicted labels\n        pred_labs_test[test_idx] = est.predict(X_test)\n        pred_labs_train[train_idx] = est.predict(X_train)\n        fold_nb[test_idx] = fold\n        \n        # Predicted values before threshold\n        try:\n            pred_vals_test[test_idx] = est.predict_proba(X_test)[:, 1]\n            pred_vals_train[train_idx] = est.predict_proba(X_train)[:, 1]\n        except AttributeError:\n            pred_vals_test[test_idx] = est.decision_function(X_test)\n            pred_vals_train[train_idx] = est.decision_function(X_train)\n\n        true_labs[test_idx] = y[test_idx]\n\n    predictions_ = pd.DataFrame(dict(model=name, fold=fold_nb.astype(int),\n                        pred_vals_test=pred_vals_test,\n                        pred_labs_test=pred_labs_test.astype(int),\n                        true_labs=y))\n    assert np.all(true_labs == y)\n\n    predictions = pd.concat([predictions, predictions_])\n\n\npredictions.to_csv(os.path.join(OUTPUT_DIR, \"predictions.csv\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2. Recompute scores from saved predictions**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "models_scores_cv = [[mod, fold,\n    metrics.balanced_accuracy_score(df[\"true_labs\"], df[\"pred_labs_test\"]),\n    metrics.roc_auc_score(df[\"true_labs\"], df[\"pred_vals_test\"])]\n for (mod, fold), df in predictions.groupby([\"model\", \"fold\"])]\n\nmodels_scores_cv = pd.DataFrame(models_scores_cv, columns=[\"model\", \"fold\", 'balanced_accuracy', 'roc_auc'])\n\nmodels_scores = models_scores_cv.groupby(\"model\").mean()\nmodels_scores = models_scores.drop(\"fold\", axis=1)\nprint(models_scores)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}