<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Non-Linear Ensemble Learning &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Out-of-sample Validation for Model Selection and Evaluation" href="resampling.html" />
    <link rel="prev" title="Non-Linear Kernel Methods and Support Vector Machines (SVM)" href="kernel_svm.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-gallery-ensemble-learning-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="non-linear-ensemble-learning">
<span id="sphx-glr-auto-gallery-ensemble-learning-py"></span><h1>Non-Linear Ensemble Learning<a class="headerlink" href="#non-linear-ensemble-learning" title="Link to this heading">¶</a></h1>
<p>Sources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/api/sklearn.ensemble.html">Scikit-learn API</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/ensemble.html">Scikit-learn doc</a></p></li>
</ul>
<section id="introduction-to-ensemble-learning">
<h2>Introduction to Ensemble Learning<a class="headerlink" href="#introduction-to-ensemble-learning" title="Link to this heading">¶</a></h2>
<p>Ensemble learning is a powerful machine learning technique that combines multiple models to achieve better performance than any individual model. By aggregating predictions from diverse learners, ensemble methods enhance accuracy, reduce variance, and improve generalization. The main advantages of ensemble learning include:</p>
<ul class="simple">
<li><p><strong>Reduced overfitting</strong>: By averaging multiple models, ensemble methods mitigate overfitting risks.</p></li>
<li><p><strong>Increased robustness</strong>: The diversity of models enhances stability, making the approach more resistant to noise and biases.</p></li>
</ul>
<p>There are three main types of ensemble learning techniques: <strong>Bagging, Boosting, and Stacking</strong>. Each method follows a unique strategy to combine multiple models and improve overall performance.</p>
<p>Conclusion</p>
<p>Ensemble learning is a fundamental approach in machine learning that significantly enhances predictive performance. <strong>Bagging</strong> helps reduce variance, <strong>boosting</strong> improves bias, and <strong>stacking</strong> leverages multiple models to optimize performance. By carefully selecting and tuning ensemble techniques, practitioners can build powerful and robust machine learning models suitable for various real-world applications.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">StackingClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
</pre></div>
</div>
<p>Breast cancer dataset</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">breast_cancer</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">breast_cancer</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">breast_cancer</span><span class="o">.</span><span class="n">target</span>
<span class="nb">print</span><span class="p">(</span><span class="n">breast_cancer</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> \
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[&#39;mean radius&#39; &#39;mean texture&#39; &#39;mean perimeter&#39; &#39;mean area&#39;
 &#39;mean smoothness&#39; &#39;mean compactness&#39; &#39;mean concavity&#39;
 &#39;mean concave points&#39; &#39;mean symmetry&#39; &#39;mean fractal dimension&#39;
 &#39;radius error&#39; &#39;texture error&#39; &#39;perimeter error&#39; &#39;area error&#39;
 &#39;smoothness error&#39; &#39;compactness error&#39; &#39;concavity error&#39;
 &#39;concave points error&#39; &#39;symmetry error&#39; &#39;fractal dimension error&#39;
 &#39;worst radius&#39; &#39;worst texture&#39; &#39;worst perimeter&#39; &#39;worst area&#39;
 &#39;worst smoothness&#39; &#39;worst compactness&#39; &#39;worst concavity&#39;
 &#39;worst concave points&#39; &#39;worst symmetry&#39; &#39;worst fractal dimension&#39;]
</pre></div>
</div>
</section>
<section id="decision-tree">
<h2>Decision tree<a class="headerlink" href="#decision-tree" title="Link to this heading">¶</a></h2>
<p>A tree can be “learned” by splitting the training dataset into subsets based on an features value test.
Each internal node represents a “test” on an feature resulting on the split of the current sample. At each step the algorithm selects the feature and a cutoff value that maximises a given metric. Different metrics exist for regression tree (target is continuous) or classification tree (the target is qualitative).
This process is repeated on each derived subset in a recursive manner called recursive partitioning. The recursion is completed when the subset at a node has all the same value of the target variable, or when splitting no longer adds value to the predictions. This general principle is implemented by many recursive partitioning tree algorithms.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/classification_tree.png"><img alt="Classification tree." src="../_images/classification_tree.png" style="width: 400px;" />
</a>
</figure>
<p>Decision trees are simple to understand and interpret however they tend to overfit the data. However decision trees tend to overfit the training set.  Leo Breiman propose random forest to deal with this issue.</p>
<p>A single decision tree is usually overfits the data it is learning from because it learn from only one pathway of decisions. Predictions from a single decision tree usually don’t make accurate predictions on new data.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.90, AUC: 0.90
</pre></div>
</div>
</section>
<section id="bagging-bootstrap-aggregating-random-forest">
<h2>Bagging (Bootstrap Aggregating): Random forest<a class="headerlink" href="#bagging-bootstrap-aggregating-random-forest" title="Link to this heading">¶</a></h2>
<p>Bagging is an ensemble method that aims to reduce variance by training multiple models on different subsets of the training data. It follows these steps:</p>
<ol class="arabic simple">
<li><p>Generate multiple bootstrap samples (randomly drawn with replacement) from the original dataset.</p></li>
<li><p>Train an independent model (typically a weak learner like a decision tree) on each bootstrap sample.</p></li>
<li><p>Aggregate predictions using majority voting (for classification) or averaging (for regression).</p></li>
</ol>
<p><strong>Example:</strong> The <strong>Random Forest</strong> algorithm is a widely used bagging method that constructs multiple decision trees and combines their predictions.</p>
<p><strong>Key Benefits:</strong></p>
<ul class="simple">
<li><p>Reduces variance and improves stability.</p></li>
<li><p>Works well with high-dimensional data.</p></li>
<li><p>Effective for handling noisy datasets.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">bagging_tree</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">())</span>
<span class="n">bagging_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bagging_tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">bagging_tree</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.95, AUC: 0.98
</pre></div>
</div>
<section id="random-forest">
<h3>Random Forest<a class="headerlink" href="#random-forest" title="Link to this heading">¶</a></h3>
<p>A random forest is a meta estimator that fits a number of <strong>decision tree learners</strong> on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting.
Random forest models reduce the risk of overfitting by introducing randomness by:</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/random_forest.png"><img alt="Random forest." src="../_images/random_forest.png" style="width: 300px;" />
</a>
</figure>
<ul class="simple">
<li><p>building multiple trees (n_estimators)</p></li>
<li><p>drawing observations with replacement (i.e., a bootstrapped sample)</p></li>
<li><p>splitting nodes on the best split among a random subset of the features selected at every node</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">forest</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">forest</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">forest</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.95, AUC: 0.99
</pre></div>
</div>
</section>
</section>
<section id="boosting-and-gradient-boosting">
<h2>Boosting and Gradient boosting<a class="headerlink" href="#boosting-and-gradient-boosting" title="Link to this heading">¶</a></h2>
<p>Boosting is an ensemble method that focuses on reducing bias by training models sequentially, where each new model corrects the errors of its predecessors. The process includes:</p>
<ol class="arabic simple">
<li><p>Train an initial weak model on the training data.</p></li>
<li><p>Assign higher weights to misclassified instances to emphasize difficult cases.</p></li>
<li><p>Train a new model on the updated dataset, repeating the process iteratively.</p></li>
<li><p>Combine the predictions of all models using a weighted sum.</p></li>
</ol>
<section id="gradient-boosting">
<h3>Gradient boosting<a class="headerlink" href="#gradient-boosting" title="Link to this heading">¶</a></h3>
<p>Popular boosting algorithms include <strong>AdaBoost</strong>, <strong>Gradient Boosting Machines (GBM)</strong>, <strong>XGBoost</strong>, and <strong>LightGBM</strong>.</p>
<p><strong>Key Benefits:</strong></p>
<ul class="simple">
<li><p>Improves accuracy by focusing on difficult instances.</p></li>
<li><p>Works well with structured data and tabular datasets.</p></li>
<li><p>Reduces bias while maintaining interpretability.</p></li>
</ul>
<p>The two main hyper-parameters are:</p>
<ul class="simple">
<li><p>The <strong>learning rate</strong> (<em>lr</em>) controls over-fitting:
decreasing the <em>lr</em> limits the capacity of a learner to overfit the residuals, ie,
it slows down the learning speed and thus increases the <strong>regularization</strong>.</p></li>
<li><p>The <strong>sub-sampling fraction</strong> controls the fraction of samples to be used for
fitting the learners. Values smaller than 1 leads to <strong>Stochastic Gradient Boosting</strong>.
It thus controls for over-fitting reducing variance and increasing bias.</p></li>
</ul>
<figure class="align-default">
<a class="reference internal image-reference" href="../_images/gradient_boosting.png"><img alt="Gradient boosting." src="../_images/gradient_boosting.png" style="width: 500px;" />
</a>
</figure>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">gb</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                                <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">gb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.94, AUC: 0.99
</pre></div>
</div>
</section>
</section>
<section id="stacking">
<h2>Stacking<a class="headerlink" href="#stacking" title="Link to this heading">¶</a></h2>
<p>Stacking (or stacked generalization) is a more complex ensemble technique that combines predictions from multiple base models using a <strong>meta-model</strong>. The process follows:</p>
<ol class="arabic simple">
<li><p>Train several base models (e.g., decision trees, SVMs, neural networks) on the same dataset.</p></li>
<li><p>Collect predictions from all base models and use them as new features.</p></li>
<li><p>Train a meta-model (often a simple regression or classification model) to learn how to best combine the base predictions.</p></li>
</ol>
<p><strong>Example:</strong> Stacking can combine weak and strong learners, such as decision trees, logistic regression, and deep learning models, to create a robust final model.</p>
<p><strong>Key Benefits:</strong></p>
<ul class="simple">
<li><p>Allows different types of models to complement each other.</p></li>
<li><p>Captures complex relationships between models.</p></li>
<li><p>Can outperform traditional ensemble methods when well-tuned.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearSVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_pipeline</span>

<span class="n">estimators</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
    <span class="p">(</span><span class="s1">&#39;svr&#39;</span><span class="p">,</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
                          <span class="n">LinearSVC</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)))]</span>

<span class="n">stacked_trees</span> <span class="o">=</span> <span class="n">StackingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="p">)</span>
<span class="n">stacked_trees</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">stacked_trees</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_prob</span> <span class="o">=</span> <span class="n">stacked_trees</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;bAcc: </span><span class="si">%.2f</span><span class="s2">, AUC: </span><span class="si">%.2f</span><span class="s2"> &quot;</span> <span class="o">%</span> <span class="p">(</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">y_pred</span><span class="p">),</span>
      <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="o">=</span><span class="n">y_prob</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>bAcc: 0.97, AUC: 0.99
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 1.056 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-gallery-ensemble-learning-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/600006aa44e07fbd36399eea548d60e9/ensemble_learning.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">ensemble_learning.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/aafdd1ea70e5191d28a4e5e20dbfe3f1/ensemble_learning.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">ensemble_learning.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/2fe0db929321628fc3e0c2222f8fe3a0/ensemble_learning.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">ensemble_learning.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Non-Linear Ensemble Learning</a><ul>
<li><a class="reference internal" href="#introduction-to-ensemble-learning">Introduction to Ensemble Learning</a></li>
<li><a class="reference internal" href="#decision-tree">Decision tree</a></li>
<li><a class="reference internal" href="#bagging-bootstrap-aggregating-random-forest">Bagging (Bootstrap Aggregating): Random forest</a><ul>
<li><a class="reference internal" href="#random-forest">Random Forest</a></li>
</ul>
</li>
<li><a class="reference internal" href="#boosting-and-gradient-boosting">Boosting and Gradient boosting</a><ul>
<li><a class="reference internal" href="#gradient-boosting">Gradient boosting</a></li>
</ul>
</li>
<li><a class="reference internal" href="#stacking">Stacking</a></li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/auto_gallery/ensemble_learning.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/auto_gallery/ensemble_learning.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>