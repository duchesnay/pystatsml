{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Hands-On: Faces recognition using various learning models\n\nThis lab is inspired by a scikit-learn lab:\n[Faces recognition example using eigenfaces and SVMs](https://scikit-learn.org/stable/auto_examples/applications/plot_face_recognition.html)\n\nIt uses scikit-learan and pytorch models using [skorch](https://github.com/skorch-dev/skorch)\n([slides](https://fr.slideshare.net/ThomasFan6/pydata-dc-2018-skorch-a-union-of-scikitlearn-and-pytorch)).\n  * skorch provides scikit-learn compatible neural network library that wraps PyTorch.\n  * skorch abstracts away the training loop, making a lot of boilerplate code obsolete.\n    A simple `net.fit(X, y)` is enough.\n\nNote that more sophisticated models can be used,\n[see](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)\nfor a overview.\n\nModels:\n\n- Eigenfaces unsupervised exploratory analysis.\n- [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with L2 regularization (includes model selection with 5CV`_\n- [SVM-RBF](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)  (includes model selection with 5CV.\n- [MLP using sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) using sklearn (includes model selection with 5CV)\n- [MLP using skorch classifier](https://skorch.readthedocs.io/en/stable/classifier.html)\n- Basic Convnet (ResNet18) using skorch.\n- Pretrained ResNet18 using skorch.\n\nPipelines:\n\n- Univariate feature filtering (Anova) with Logistic-L2\n- PCA with LogisticRegression with L2 regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom time import time\nimport pandas as pd\n\n# Plot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Plot parameters\nplt.style.use('seaborn-v0_8-whitegrid')\nfig_w, fig_h = plt.rcParams.get('figure.figsize')\nplt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n\n# ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\n# Preprocesing\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\n# Dataset\nfrom sklearn.datasets import fetch_lfw_people\n\n# Models\nfrom sklearn.decomposition import PCA\nimport sklearn.manifold as manifold\nimport sklearn.linear_model as lm\nimport sklearn.svm as svm\nfrom sklearn.neural_network import MLPClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.ensemble import GradientBoostingClassifier\n\n# Pytorch Models\nimport torch\nimport torchvision\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom skorch import NeuralNetClassifier\nimport skorch\n\n# Use [skorch](https://github.com/skorch-dev/skorch). Install:\n# `conda install -c conda-forge skorch`\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Utils\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def plot_gallery(images, titles, h, w, n_row=3, n_col=4):\n    \"\"\"Plot a gallery of portraits.\"\"\"\n    plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n    plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n    for i in range(min(images.shape[0], n_row * n_col)):\n        plt.subplot(n_row, n_col, i + 1)\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.title(titles[i], size=12)\n        plt.xticks(())\n        plt.yticks(())\n\n\ndef title(y_pred, y_test, target_names, i):\n    \"\"\"Plot the result of the prediction on a portion of the test set.\"\"\"\n    pred_name = target_names[y_pred[i]].rsplit(' ', 1)[-1]\n    true_name = target_names[y_test[i]].rsplit(' ', 1)[-1]\n    return 'predicted: %s\\ntrue:      %s' % (pred_name, true_name)\n\n\ndef label_proportion(x, decimals=2):\n    \"\"\"Labels's proportions.\"\"\"\n    unique, counts = np.unique(x, return_counts=True)\n    return dict(zip(unique, np.round(counts / len(x), decimals)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n\n# introspect the images arrays to find the shapes (for plotting)\nn_samples, h, w = lfw_people.images.shape\n\n# for machine learning we use the 2 data directly (as relative pixel\n# positions info is ignored by this model)\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\n\nprint(\"Total dataset size:\")\nprint(\"n_samples: %d\" % n_samples)\nprint(\"n_features: %d\" % n_features)\nprint(\"n_classes: %d\" % n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split into a training and testing set in stratified way\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=1, stratify=y)\n\nprint({target_names[lab]: prop for lab, prop in\n       label_proportion(y_train).items()})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot mean faces and 4 samples of each individual\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "single_faces = [X_train[y_train == lab][:5] for lab in np.unique(y_train)]\nsingle_faces = np.vstack(single_faces).reshape((5 * n_classes, h, w))\n\nmean_faces = [X_train[y_train == lab].mean(axis=0) for lab in\n              np.unique(y_train)]\nmean_faces = np.vstack(mean_faces).reshape((n_classes, h, w))\n\nsingle_faces[::5, :, :] = mean_faces\ntitles = [n for name in target_names for n in [name] * 5]\nplot_gallery(single_faces, titles, h, w, n_row=n_classes, n_col=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Eigenfaces\n\nCompute a PCA (eigenfaces) on the face dataset (treated as unlabeled\ndataset): unsupervised feature extraction / dimensionality reduction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_components = 150\n\nprint(\"Extracting the top %d eigenfaces from %d faces\"\n      % (n_components, X_train.shape[0]))\nt0 = time()\npca = PCA(n_components=n_components, svd_solver='randomized',\n          whiten=True).fit(X_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\neigenfaces = pca.components_.reshape((n_components, h, w))\n\nprint(\"Explained variance\", pca.explained_variance_ratio_[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "T-SNE\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tsne = manifold.TSNE(n_components=2, init='pca', random_state=0)\nX_tsne = tsne.fit_transform(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\ndf = pd.DataFrame(dict(lab=y_train,\n                       PC1=X_train_pca[:, 0],\n                       PC2=X_train_pca[:, 1],\n                       TSNE1=X_tsne[:, 0],\n                       TSNE2=X_tsne[:, 1]))\n\nsns.relplot(x=\"PC1\", y=\"PC2\", hue=\"lab\", data=df)\n\nsns.relplot(x=\"TSNE1\", y=\"TSNE2\", hue=\"lab\", data=df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot eigenfaces:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "eigenface_titles = [\"eigenface %d\" % i for i in range(eigenfaces.shape[0])]\nplot_gallery(eigenfaces, eigenface_titles, h, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LogisticRegression with L2 penalty (with CV-based model selection)\n\nOur goal is to obtain a good balanced accuracy, ie, the macro average\n(`macro avg`) of classes' recalls. In this perspective, the good practices\nare:\n\n- Scale input features using either `StandardScaler()` or `MinMaxScaler()`\n  \"It doesn't harm\".\n- Re-balance classes' contributions `class_weight='balanced'`\n- Do not include an intercept (`fit_intercept=False`) in the model.\n  This should reduce the global accuracy `weighted avg`. But remember that\n  we decided to maximize the balanced accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "lrl2_cv = make_pipeline(\n    preprocessing.StandardScaler(),\n    # preprocessing.MinMaxScaler(),  # Would have done the job either\n    GridSearchCV(lm.LogisticRegression(max_iter=1000, class_weight='balanced',\n                                       fit_intercept=False),\n                 {'C': 10. ** np.arange(-3, 3)},\n                 cv=5, n_jobs=5))\n\nt0 = time()\nlrl2_cv.fit(X=X_train, y=y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best params found by grid search:\")\nprint(lrl2_cv.steps[-1][1].best_params_)\n\ny_pred = lrl2_cv.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coefficients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coefs = lrl2_cv.steps[-1][1].best_estimator_.coef_\ncoefs = coefs.reshape(-1, h, w)\nplot_gallery(coefs, target_names, h, w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM (with CV-based model selection)\n\nRemarks:\n- RBF generally requires \"large\" C (>1)\n- Poly generally requires \"small\" C (<1)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "svm_cv = make_pipeline(\n    # preprocessing.StandardScaler(),\n    preprocessing.MinMaxScaler(),\n    GridSearchCV(svm.SVC(class_weight='balanced'),\n                 {'kernel': ['poly', 'rbf'], 'C': 10. ** np.arange(-2, 3)},\n                 # {'kernel': ['rbf'], 'C': 10. ** np.arange(-1, 4)},\n                 cv=5, n_jobs=5))\n\nt0 = time()\nsvm_cv.fit(X_train, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best params found by grid search:\")\nprint(svm_cv.steps[-1][1].best_params_)\n\ny_pred = svm_cv.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP with sklearn and CV-based model selection\n\nDefault parameters:\n- alpha, default=0.0001 L2 penalty (regularization term) parameter.\n- batch_size=min(200, n_samples)\n- learning_rate_init = 0.001 (the important one since we uses adam)\n- solver default='adam'\n  * sgd: momentum=0.9\n  * adam: beta_1, beta_2 default=0.9, 0.999 Exponential decay rates for\n    the first and second moment.\n- L2 penalty (regularization term) parameter, `alpha` default=0.0001\n- tol, default=1e-4\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mlp_param_grid = {\"hidden_layer_sizes\":\n                  # Configurations with 1 hidden layer:\n                  [(100, ), (50, ), (25, ), (10, ), (5, ),\n                   # Configurations with 2 hidden layers:\n                   (100, 50, ), (50, 25, ), (25, 10, ), (10, 5, ),\n                   # Configurations with 3 hidden layers:\n                   (100, 50, 25, ), (50, 25, 10, ), (25, 10, 5, )],\n                  \"activation\": [\"relu\"], \"solver\": [\"adam\"], 'alpha': [0.0001]}\n\nmlp_cv = make_pipeline(\n    # preprocessing.StandardScaler(),\n    preprocessing.MinMaxScaler(),\n    GridSearchCV(estimator=MLPClassifier(random_state=1, max_iter=400),\n                 param_grid=mlp_param_grid,\n                 cv=5, n_jobs=5))\n\nt0 = time()\nmlp_cv.fit(X_train, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\nprint(\"Best params found by grid search:\")\nprint(mlp_cv.steps[-1][1].best_params_)\n\ny_pred = mlp_cv.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MLP with pytorch and no model selection\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SimpleMLPClassifierPytorch(nn.Module):\n    \"\"\"Simple (one hidden layer) MLP Classifier with Pytorch.\"\"\"\n\n    def __init__(self):\n        super(SimpleMLPClassifierPytorch, self).__init__()\n\n        self.dense0 = nn.Linear(1850, 100)\n        self.nonlin = nn.ReLU()\n        self.output = nn.Linear(100, 7)\n        self.softmax = nn.Softmax(dim=-1)\n\n    def forward(self, X, **kwargs):\n        X = self.nonlin(self.dense0(X))\n        X = self.softmax(self.output(X))\n        return X\n\n\nmlp = NeuralNetClassifier(  # Match the parameters with sklearn\n    SimpleMLPClassifierPytorch,\n    criterion=torch.nn.NLLLoss,\n    max_epochs=100,\n    batch_size=200,\n    optimizer=torch.optim.Adam,\n    # optimizer=torch.optim.SGD,\n    optimizer__lr=0.001,\n    optimizer__betas=(0.9, 0.999),\n    optimizer__eps=1e-4,\n    optimizer__weight_decay=0.0001,  # L2 regularization\n    # Shuffle training data on each epoch\n    iterator_train__shuffle=True,\n    device=device,\n    verbose=0)\n\nscaler = preprocessing.MinMaxScaler()\nX_train_s = scaler.fit_transform(X_train)\nX_test_s = scaler.transform(X_test)\n\nt0 = time()\nmlp.fit(X_train_s, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\ny_pred = mlp.predict(X_test_s)\nprint(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Univariate feature filtering (Anova) with Logistic-L2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "anova_l2lr = Pipeline([\n    ('standardscaler', preprocessing.StandardScaler()),\n    ('anova', SelectKBest(f_classif)),\n    ('l2lr', lm.LogisticRegression(max_iter=1000, class_weight='balanced',\n                                   fit_intercept=False))\n])\n\nparam_grid = {'anova__k': [50, 100, 500, 1000, 1500, X_train.shape[1]],\n              'l2lr__C': 10. ** np.arange(-3, 3)}\nanova_l2lr_cv = GridSearchCV(anova_l2lr, cv=5,  param_grid=param_grid,\n                             n_jobs=5)\n\nt0 = time()\nanova_l2lr_cv.fit(X=X_train, y=y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nprint(\"Best params found by grid search:\")\nprint(anova_l2lr_cv.best_params_)\n\ny_pred = anova_l2lr_cv.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PCA with LogisticRegression with L2 regularization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pca_lrl2_cv = make_pipeline(\n    PCA(n_components=150, svd_solver='randomized', whiten=True),\n    GridSearchCV(lm.LogisticRegression(max_iter=1000, class_weight='balanced',\n                                       fit_intercept=False),\n                 {'C': 10. ** np.arange(-3, 3)},\n                 cv=5, n_jobs=5))\n\nt0 = time()\npca_lrl2_cv.fit(X=X_train, y=y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\nprint(\"Best params found by grid search:\")\nprint(pca_lrl2_cv.steps[-1][1].best_params_)\n\ny_pred = pca_lrl2_cv.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))\nprint(confusion_matrix(y_test, y_pred, labels=range(n_classes)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic ConvNet\n\nNote that to simplify, do not use pipeline (scaler + CNN) here.\nBut it would have been simple to do so, since pytorch is wrapped in skorch\nobject that is compatible with sklearn.\n\nSources:\n\n- [ConvNet on MNIST](https://github.com/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb)\n- [NeuralNetClassifier](https://skorch.readthedocs.io/en/stable/classifier.html)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Cnn(nn.Module):\n    \"\"\"Basic ConvNet Conv(1, 32, 64) -> FC(100, 7) -> softmax.\"\"\"\n\n    def __init__(self, dropout=0.5, fc_size=4928, n_outputs=7, debug=False):\n        super(Cnn, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n        self.conv2_drop = nn.Dropout2d(p=dropout)\n        self.fc1 = nn.Linear(fc_size, 100)\n        self.fc2 = nn.Linear(100, n_outputs)\n        self.fc1_drop = nn.Dropout(p=dropout)\n        self.debug = debug\n\n    def forward(self, x):\n        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n\n        # flatten over channel, height and width = 1600\n        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n\n        if self.debug:  # trick to get the size of the first FC\n            print(\"### DEBUG: Shape of last convnet=\", x.shape,\n                  \". FC size=\", np.prod(x.shape[1:]))\n\n        x = torch.relu(self.fc1_drop(self.fc1(x)))\n        x = torch.softmax(self.fc2(x), dim=-1)\n        return x\n\n\ntorch.manual_seed(0)\ncnn = NeuralNetClassifier(\n        Cnn,\n        max_epochs=100,\n        lr=0.001,\n        optimizer=torch.optim.Adam,\n        device=device,\n        train_split=skorch.dataset.ValidSplit(cv=5, stratified=True),\n        verbose=0)\n\nscaler = preprocessing.MinMaxScaler()\nX_train_s = scaler.fit_transform(X_train).reshape(-1, 1, h, w)\nX_test_s = scaler.transform(X_test).reshape(-1, 1, h, w)\n\nt0 = time()\ncnn.fit(X_train_s, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\ny_pred = cnn.predict(X_test_s)\nprint(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ConvNet with Resnet18\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Resnet18(nn.Module):\n    \"\"\"ResNet 18, pretrained, with one input chanel and 7 outputs.\"\"\"\n\n    def __init__(self, in_channels=1, n_outputs=7):\n        super(Resnet18, self).__init__()\n\n        # self.model = torchvision.models.resnet18()\n        self.model = torchvision.models.resnet18(pretrained=True)\n\n        # original definition of the first layer on the resnet class\n        # self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n        #                        bias=False)\n        # one channel input (grayscale):\n        self.model.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2,\n                                     padding=3, bias=False)\n\n        # Last layer\n        num_ftrs = self.model.fc.in_features\n        self.model.fc = nn.Linear(num_ftrs, n_outputs)\n\n    def forward(self, x):\n        return self.model(x)\n\n\ntorch.manual_seed(0)\nresnet = NeuralNetClassifier(\n    Resnet18,\n    # `CrossEntropyLoss` combines `LogSoftmax and `NLLLoss`\n    criterion=nn.CrossEntropyLoss,\n    max_epochs=50,\n    batch_size=128,  # default value\n    optimizer=torch.optim.Adam,\n    # optimizer=torch.optim.SGD,\n    optimizer__lr=0.001,\n    optimizer__betas=(0.9, 0.999),\n    optimizer__eps=1e-4,\n    optimizer__weight_decay=0.0001,  # L2 regularization\n    # Shuffle training data on each epoch\n    # iterator_train__shuffle=True,\n    train_split=skorch.dataset.ValidSplit(cv=5, stratified=True),\n    device=device,\n    verbose=0)\n\nscaler = preprocessing.MinMaxScaler()\nX_train_s = scaler.fit_transform(X_train).reshape(-1, 1, h, w)\nX_test_s = scaler.transform(X_test).reshape(-1, 1, h, w)\n\nt0 = time()\nresnet.fit(X_train_s, y_train)\nprint(\"done in %0.3fs\" % (time() - t0))\n\n# Continue training a model (warm re-start):\n# resnet.partial_fit(X_train_s, y_train)\n\ny_pred = resnet.predict(X_test_s)\nprint(classification_report(y_test, y_pred, target_names=target_names))\n\nepochs = np.arange(len(resnet.history[:, 'train_loss'])) + 1\nplt.plot(epochs, resnet.history[:, 'train_loss'], '-b', label='train_loss')\nplt.plot(epochs, resnet.history[:, 'valid_loss'], '-r', label='valid_loss')\nplt.plot(epochs, resnet.history[:, 'valid_acc'], '--r', label='valid_acc')\nplt.legend()\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}