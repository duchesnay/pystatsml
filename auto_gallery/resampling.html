<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Out-of-sample Validation for Model Selection and Evaluation &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Backpropagation" href="../deep_learning/dl_backprop_numpy-pytorch-sklearn.html" />
    <link rel="prev" title="Non-Linear Ensemble Learning" href="ensemble_learning.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p><a class="reference internal" href="#sphx-glr-download-auto-gallery-resampling-py"><span class="std std-ref">Go to the end</span></a>
to download the full example code.</p>
</div>
<section class="sphx-glr-example-title" id="out-of-sample-validation-for-model-selection-and-evaluation">
<span id="sphx-glr-auto-gallery-resampling-py"></span><h1>Out-of-sample Validation for Model Selection and Evaluation<a class="headerlink" href="#out-of-sample-validation-for-model-selection-and-evaluation" title="Link to this heading">¶</a></h1>
<p><a class="reference external" href="https://scikit-learn.org/stable/model_selection.html">Source scikit-learn model selection and evaluation</a></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.base</span><span class="w"> </span><span class="kn">import</span> <span class="n">is_classifier</span><span class="p">,</span> <span class="n">clone</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">joblib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">StratifiedKFold</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_validate</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">KFold</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">KFold</span><span class="p">,</span> <span class="n">PredefinedSplit</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">GridSearchCV</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">metrics</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<section id="train-validation-and-test-sets">
<h2>Train, validation and test sets<a class="headerlink" href="#train-validation-and-test-sets" title="Link to this heading">¶</a></h2>
<p>Machine learning algorithms tend to overfit training data. Predictive performances <strong>MUST</strong> be evaluated on independant hold-out dataset.
A split of into a training test and an independent test set mandatory.
However to set the hyperparameters the dataset is generally splitted into three sets:</p>
<ol class="arabic simple">
<li><p><strong>Training Set (Fitting the Model and Learning Parameters)</strong></p></li>
</ol>
<ul class="simple">
<li><p>The training set is used to fit the model by learning its parameters (e.g., weights in a neural network, coefficients in a regression model).</p></li>
<li><p>The algorithm adjusts its parameters to minimize a chosen loss function (e.g., MSE for regression, cross-entropy for classification).</p></li>
<li><p>The model learns patterns from this data, but using only the training set risks overfitting—where the model memorizes data instead of generalizing.</p></li>
<li><p>Role: Learn the parameters of the model.</p></li>
</ul>
<ol class="arabic simple" start="2">
<li><p><strong>Validation Set (Hyperparameter Tuning and Model Selection)</strong></p></li>
</ol>
<ul class="simple">
<li><p>The validation set is used to fine-tune the model’s hyperparameters (e.g., learning rate, number of layers, number of clusters).</p></li>
<li><p>Hyperparameters are not directly learned from data but are instead set before training.</p></li>
<li><p>The validation set helps to assess different model configurations, preventing overfitting by ensuring that the model generalizes beyond the training set.</p></li>
<li><p>If we see high performance on the training set but poor performance on the validation set, we are likely overfitting.</p></li>
<li><p>The process of choosing the best hyperparameters based on the validation set is called <strong>model selection</strong>.</p></li>
<li><p>Role: Tune hyperparameters and select the best model configuration.</p></li>
<li><p>Data Leakage Risk: If we tune hyperparameters too much on the validation set, it essentially becomes part of training, leading to potential overfitting on it.</p></li>
</ul>
<ol class="arabic simple" start="3">
<li><p><strong>Test Set (Final Independent Evaluation)</strong></p></li>
</ol>
<ul class="simple">
<li><p>The test set is an independent dataset used to evaluate the final model after training and hyperparameter tuning.</p></li>
<li><p>This provides an unbiased estimate of how the model will perform on completely new data.</p></li>
<li><p>The model should never be trained or tuned using the test set to ensure a fair evaluation.</p></li>
<li><p>Performance metrics (e.g., accuracy, F1-score, ROC-AUC) on the test set indicate how well the model is expected to perform in real-world scenarios.</p></li>
<li><p>Role: Evaluate the final model’s performance on unseen data.</p></li>
</ul>
<figure class="align-default">
<img alt="Train, validation and test sets." src="../_images/train_val_test_cv.png" />
</figure>
<p>Summary:</p>
<ul class="simple">
<li><dl class="simple">
<dt>Training set</dt><dd><ul>
<li><p>Fits model parameters.</p></li>
<li><p>High risk of overfitting if the model is too complex.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Validation set</dt><dd><ul>
<li><p>Tunes hyperparameters and selects the best model.</p></li>
<li><p>Risk of of overfitting if tuning too much.</p></li>
</ul>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Test set</dt><dd><ul>
<li><p>Provides a final evaluation on unseen data.</p></li>
</ul>
</dd>
</dl>
</li>
</ul>
<p>Split dataset in train/test sets to train and assess the the final model after training and hyperparameter tuning.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>\
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test R2: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test R2: 0.74
</pre></div>
</div>
</section>
<section id="cross-validation-cv">
<h2>Cross-Validation (CV)<a class="headerlink" href="#cross-validation-cv" title="Link to this heading">¶</a></h2>
<p>If sample size is limited, train/validation/test split may be impossible:</p>
<ul class="simple">
<li><p>Large training+validation set (80%) small test set (20%) might provide a poor estimation of the
predictive performances on few test samples. The same argument stands for train vs validation samples.</p></li>
<li><p>On the contrary, large test set and small training set might produce a poorly estimated learner.</p></li>
</ul>
<p><strong>Cross Validation (CV)</strong> (<a class="reference external" href="https://scikit-learn.org/stable/modules/cross_validation.html">Scikit-learn</a>)can be used to replace train/validation split
and/or train+validation / test split. Main procedure:</p>
<ol class="arabic simple">
<li><p>The dataset is divided into k equal-sized subsets (folds).</p></li>
<li><p>The model is trained k times, each time using k-1 folds as the training set and 1 fold as the validation set.</p></li>
<li><p>The final performance is the average of the k validation scores.</p></li>
</ol>
<p>For 10-fold we can either average over 10 values (Macro measure) or
concatenate the 10 experiments and compute the micro measures.</p>
<p>Two strategies [micro vs macro estimates](<a class="reference external" href="https://stats.stackexchange.com/questions/34611/meanscores-vs-scoreconcatenation-in-cross-validation">https://stats.stackexchange.com/questions/34611/meanscores-vs-scoreconcatenation-in-cross-validation</a>):</p>
<ul class="simple">
<li><p><strong>Micro measure: average(individual scores)</strong>: compute a score
<span class="math notranslate nohighlight">\(\mathcal{S}\)</span> for each sample and average over all samples.
It is similar to <strong>average score(concatenation)</strong>: an averaged score
computed over all concatenated samples.</p></li>
<li><p><strong>Macro measure mean(CV scores)</strong> (the most commonly used method):
compute a score <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> on each each fold <em>k</em> and average
across folds:</p></li>
</ul>
<p>These two measures (an average of average vs. a global average) are generally
similar. They may differ slightly is folds are of different sizes.
This validation scheme is known as the <strong>K-Fold CV</strong>.
Typical choices of <em>K</em> are 5 or 10, [Kohavi 1995].
The extreme case where <em>K = N</em> is known as <strong>Leave-One-Out Cross-Validation,
LOO-CV</strong>.</p>
<section id="cv-for-regression">
<h3>CV for regression<a class="headerlink" href="#cv-for-regression" title="Link to this heading">¶</a></h3>
<p>Usually the error function <span class="math notranslate nohighlight">\(\mathcal{L}()\)</span> is the r-squared score.
However other function (MAE, MSE) can be used.</p>
<p><strong>CV with explicit loop:</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">r2_train</span><span class="p">,</span> <span class="n">r2_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">r2_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:])))</span>
    <span class="n">r2_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:])))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r2_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">r2_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Train r2:0.99
Test  r2:0.67
</pre></div>
</div>
<p>Scikit-learn provides user-friendly function to perform CV</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html">cross_val_score</a>: single metric</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test  r2:0.73
Test  r2:0.67
</pre></div>
</div>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html">cross_validate</a>: multi metric, + time, etc.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span> <span class="s1">&#39;neg_mean_absolute_error&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test R2:</span><span class="si">%.2f</span><span class="s2">; MAE:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_r2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                                  <span class="o">-</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_neg_mean_absolute_error&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test R2:0.67; MAE:55.27
</pre></div>
</div>
</section>
<section id="cv-for-classification-stratify-for-the-target-label">
<h3>CV for classification: stratify for the target label<a class="headerlink" href="#cv-for-classification-stratify-for-the-target-label" title="Link to this heading">¶</a></h3>
<p>With classification problems it is essential to sample folds where each
set contains approximately the same percentage of samples of each target
class as the complete set. This is called <strong>stratification</strong>.
In this case, we will use <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html">StratifiedKFold</a>
with is a variation of k-fold which returns stratified folds.
As error function we recommend:</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score">balanced accuracy</a></p></li>
<li><p>The <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score">ROC-AUC</a></p></li>
</ul>
<p><strong>CV with explicit loop</strong>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                    <span class="n">n_informative</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>

<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Lists to store scores by folds (for macro measure only)</span>
<span class="n">bacc</span><span class="p">,</span> <span class="n">auc</span> <span class="o">=</span> <span class="p">[],</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">mod</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span>
    <span class="n">bacc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span>
        <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">mod</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:])))</span>
    <span class="n">auc</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span>
        <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">mod</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:])))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test AUC:</span><span class="si">%.2f</span><span class="s2">; bACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">bacc</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">auc</span><span class="p">)))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test AUC:0.86; bACC:0.79
</pre></div>
</div>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html">cross_val_score</a>: single metric</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test  ACC:0.79
</pre></div>
</div>
<p>Provide your own CV and score</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">balanced_acc</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Balanced acuracy scorer.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>


<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                         <span class="n">scoring</span><span class="o">=</span><span class="n">balanced_acc</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  bACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test  bACC:0.79
</pre></div>
</div>
<p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html">cross_validate</a>: multi metric, + time, etc.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test AUC:</span><span class="si">%.2f</span><span class="s2">; bACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                                    <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_balanced_accuracy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test AUC:0.86; bACC:0.79
</pre></div>
</div>
</section>
<section id="cross-validation-for-model-selection-gridsearchcv">
<h3>Cross-validation for model selection (GridSearchCV)<a class="headerlink" href="#cross-validation-for-model-selection-gridsearchcv" title="Link to this heading">¶</a></h3>
<p>Combine CV and grid search:
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a>
perform hyperparameter tuning (model selection) by systematically searching the best combination of hyperparameters
evaluating all possible combinations (over a grid of possible values) using cross-validation:</p>
<ol class="arabic simple">
<li><p>Define the model: Choose a machine learning model (e.g., SVM, Random Forest).</p></li>
<li><p>Specify hyperparameters: Create a dictionary of hyperparameters and their possible values.</p></li>
<li><p>Perform exhaustive search: GridSearchCV trains the model with every possible combination of hyperparameters.</p></li>
<li><p>Cross-validation: For each combination, it uses k-fold cross-validation (default cv=5).</p></li>
<li><p>Select the best model: The combination with the highest validation performance is chosen.
By default, refit an estimator using the best found parameters on the whole dataset.</p></li>
</ol>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Outer, tain/test, split:</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span>\
    <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">cv_inner</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Inner Cross-Validation (tain/validation, splits) for model selection</span>
<span class="n">lm_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="mf">10.</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span>
                     <span class="n">cv</span><span class="o">=</span><span class="n">cv_inner</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Fit, including model selection with internal CV</span>
<span class="n">lm_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict</span>
<span class="n">y_pred_test</span> <span class="o">=</span> <span class="n">lm_cv</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test bACC: </span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test bACC: 0.75
</pre></div>
</div>
</section>
<section id="cross-validation-for-both-model-outer-evaluation-and-model-inner-selection">
<h3>Cross-validation for both model (outer) evaluation and model (inner) selection<a class="headerlink" href="#cross-validation-for-both-model-outer-evaluation-and-model-inner-selection" title="Link to this heading">¶</a></h3>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">cv_outer</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">cv_inner</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Cross-validation for model (inner) selection</span>
<span class="n">lm_cv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lm</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(),</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">10.</span> <span class="o">**</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)},</span>
                     <span class="n">cv</span><span class="o">=</span><span class="n">cv_inner</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Cross-validation for model (outer) evaluation</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">mod</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv_outer</span><span class="p">,</span>
                        <span class="n">scoring</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span> <span class="s1">&#39;roc_auc&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test AUC:</span><span class="si">%.2f</span><span class="s2">; bACC:</span><span class="si">%.2f</span><span class="s2">, Time: </span><span class="si">%.2f</span><span class="s2">s&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_roc_auc&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span>
                                                 <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;test_balanced_accuracy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span>
<span class="p">),</span>
    <span class="n">scores</span><span class="p">[</span><span class="s1">&#39;fit_time&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">sum</span><span class="p">()))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Test AUC:0.85; bACC:0.74, Time: 0.05s
</pre></div>
</div>
</section>
<section id="models-with-built-in-cross-validation">
<h3>Models with built-in cross-validation<a class="headerlink" href="#models-with-built-in-cross-validation" title="Link to this heading">¶</a></h3>
<p>Let sklearn select the best parameters over a default grid.</p>
<p><strong>Classification</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Logistic Ridge (L2 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">mod_cv</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">class_weight</span><span class="o">=</span><span class="s1">&#39;balanced&#39;</span><span class="p">,</span>
                                 <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;balanced_accuracy&#39;</span><span class="p">,</span>
                                 <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">mod_cv</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  ACC:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>== Logistic Ridge (L2 penalty) ==
Test  ACC:0.79
</pre></div>
</div>
<p><strong>Regression</strong></p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                      <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Ridge (L2 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== Lasso (L1 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LassoCV</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;== ElasticNet (L1 penalty) ==&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">ElasticNetCV</span><span class="p">(</span><span class="n">l1_ratio</span><span class="o">=</span><span class="p">[</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.9</span><span class="p">],</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">X</span><span class="o">=</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test  r2:</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>== Ridge (L2 penalty) ==
Test  r2:0.16
== Lasso (L1 penalty) ==
Test  r2:0.74
== ElasticNet (L1 penalty) ==
Test  r2:0.58
</pre></div>
</div>
</section>
</section>
</section>
<section id="random-permutations-sample-the-null-distribution">
<h1>Random Permutations: sample the null distribution<a class="headerlink" href="#random-permutations-sample-the-null-distribution" title="Link to this heading">¶</a></h1>
<p>A permutation test is a type of non-parametric randomization test in which the null distribution of a test statistic is estimated by randomly permuting the observations.</p>
<p>Permutation tests are highly attractive because they make no assumptions other than that the observations are independent and identically distributed under the null hypothesis.</p>
<ol class="arabic simple">
<li><p>Compute a observed statistic <span class="math notranslate nohighlight">\(t_{obs}\)</span> on the data.</p></li>
<li><p>Use randomization to compute the distribution of <span class="math notranslate nohighlight">\(t\)</span> under the null hypothesis: Perform <span class="math notranslate nohighlight">\(N\)</span> random permutation of the data. For each sample of permuted data, <span class="math notranslate nohighlight">\(i\)</span> the data compute the statistic <span class="math notranslate nohighlight">\(t_i\)</span>. This procedure provides the distribution of <em>t</em> under the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span>: <span class="math notranslate nohighlight">\(P(t \vert H_0)\)</span></p></li>
<li><p>Compute the p-value = <span class="math notranslate nohighlight">\(P(t&gt;t_{obs} | H_0) \left\vert\{t_i &gt; t_{obs}\}\right\vert\)</span>, where <span class="math notranslate nohighlight">\(t_i's include :math:`t_{obs}\)</span>.</p></li>
</ol>
<p>Example Ridge regression</p>
<p>Sample the distributions of r-squared and coefficients of ridge regression under the null hypothesis. Simulated dataset:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Regression dataset where first two features are predictive</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">n_features_info</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
<span class="n">beta</span><span class="p">[:</span><span class="n">n_features_info</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">Xbeta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
<span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_samples</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Xbeta</span> <span class="o">+</span> <span class="n">eps</span>


<span class="c1"># Fit model on all data (!! risk of overfit)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">RidgeCV</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients on all data:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="c1"># Random permutation loop</span>
<span class="n">nperm</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># !! Should be at least 1000 (to assess a p-value at 1%)</span>
<span class="n">scores_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;r2&quot;</span><span class="p">]</span>
<span class="n">scores_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nperm</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores_names</span><span class="p">)))</span>
<span class="n">coefs_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nperm</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">scores_perm</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="n">coefs_perm</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>

<span class="n">orig_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">perm_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">nperm</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">permutation</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">scores_perm</span><span class="p">[</span><span class="n">perm_i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">coefs_perm</span><span class="p">[</span><span class="n">perm_i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>

<span class="c1"># One-tailed empirical p-value</span>
<span class="n">pval_pred_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">scores_perm</span> <span class="o">&gt;=</span> <span class="n">scores_perm</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">scores_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">pval_coef_perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="n">coefs_perm</span> <span class="o">&gt;=</span> <span class="n">coefs_perm</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">coefs_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;R2 p-value: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">pval_pred_perm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coeficients p-values:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">pval_coef_perm</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Coefficients on all data:
[ 1.01872179  1.05713711  0.20873888 -0.01784094 -0.05265821]
R2 p-value: 0.001
Coeficients p-values: [0.001 0.001 0.098 0.573 0.627]
</pre></div>
</div>
<p>Compute p-values corrected for multiple comparisons using FWER max-T
(Westfall and Young, 1993) procedure.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">pval_coef_perm_tmax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">coefs_perm</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">coefs_perm</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>
                                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">coefs_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])])</span> <span class="o">/</span> <span class="n">coefs_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;P-values with FWER (Westfall and Young) correction&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pval_coef_perm_tmax</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>P-values with FWER (Westfall and Young) correction
[0.000999   0.000999   0.41058941 0.98001998 0.99200799]
</pre></div>
</div>
<p>Plot distribution of third coefficient under null-hypothesis
Coeffitients 0 and 1 are significantly different from 0.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">hist_pvalue</span><span class="p">(</span><span class="n">perms</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">name</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Plot statistic distribution as histogram.</span>

<span class="sd">    Paramters</span>
<span class="sd">    ---------</span>
<span class="sd">    perms: 1d array, statistics under the null hypothesis.</span>
<span class="sd">           perms[0] is the true statistic .</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Re-weight to obtain distribution</span>
    <span class="n">pval</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">perms</span> <span class="o">&gt;=</span> <span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">/</span> <span class="n">perms</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">hist</span><span class="p">([</span><span class="n">perms</span><span class="p">[</span><span class="n">perms</span> <span class="o">&gt;=</span> <span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">perms</span><span class="p">],</span> <span class="n">histtype</span><span class="o">=</span><span class="s1">&#39;stepfilled&#39;</span><span class="p">,</span>
            <span class="n">bins</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;p-val&lt;</span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">pval</span><span class="p">,</span>
            <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">weights</span><span class="p">[</span><span class="n">perms</span> <span class="o">&gt;=</span> <span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="n">weights</span><span class="p">])</span>
    <span class="c1"># , label=&quot;observed statistic&quot;)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">perms</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">ax</span>


<span class="n">n_coef</span> <span class="o">=</span> <span class="n">coefs_perm</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">n_coef</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">9</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_coef</span><span class="p">):</span>
    <span class="n">hist_pvalue</span><span class="p">(</span><span class="n">coefs_perm</span><span class="p">[:,</span> <span class="n">i</span><span class="p">],</span> <span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">axes</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Coefficient distribution under null hypothesis&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_resampling_001.png" srcset="../_images/sphx_glr_resampling_001.png" alt="resampling" class = "sphx-glr-single-img"/><p>Exercise</p>
<p>Given the logistic regression presented above and its validation given a 5 folds CV.</p>
<ol class="arabic simple">
<li><p>Compute the p-value associated with the prediction accuracy measured with 5CV using a permutation test.</p></li>
<li><p>Compute the p-value associated with the prediction accuracy using a parametric test.</p></li>
</ol>
</section>
<section id="bootstrapping">
<h1>Bootstrapping<a class="headerlink" href="#bootstrapping" title="Link to this heading">¶</a></h1>
<p>Bootstrapping is a statistical technique which consists in generating sample (called bootstrap samples) from an initial dataset of size <em>N</em> by randomly drawing with replacement <em>N</em> observations. It provides sub-samples with the same distribution than the original dataset. It aims to:</p>
<ol class="arabic simple">
<li><p>Assess the variability (standard error, <a class="reference external" href="https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html#the-bootstrap-method-and-empirical-confidence-intervals">Confidence Intervals (CI)</a> of performances scores or estimated parameters (see Efron et al. 1986).</p></li>
<li><p>Regularize model by fitting several models on bootstrap samples and averaging their predictions (see Baging and random-forest).</p></li>
</ol>
<p>A great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients.</p>
<ol class="arabic simple">
<li><p>Perform <span class="math notranslate nohighlight">\(B\)</span> sampling, with replacement, of the dataset.</p></li>
<li><p>For each sample <span class="math notranslate nohighlight">\(i\)</span> fit the model and compute the scores.</p></li>
<li><p>Assess standard errors and confidence intervals of scores using the scores obtained on the <span class="math notranslate nohighlight">\(B\)</span> resampled dataset. Or, average models predictions.</p></li>
</ol>
<p>References:</p>
<p>[Efron B, Tibshirani R. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Stat Sci 1986;1:54–75](<a class="reference external" href="https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815">https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815</a>)</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Bootstrap loop</span>
<span class="n">nboot</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># !! Should be at least 1000</span>
<span class="n">scores_names</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;r2&quot;</span><span class="p">]</span>
<span class="n">scores_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nboot</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">scores_names</span><span class="p">)))</span>
<span class="n">coefs_boot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">nboot</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">orig_all</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">boot_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nboot</span><span class="p">):</span>
    <span class="n">boot_tr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">orig_all</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">orig_all</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">boot_te</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">setdiff1d</span><span class="p">(</span><span class="n">orig_all</span><span class="p">,</span> <span class="n">boot_tr</span><span class="p">,</span> <span class="n">assume_unique</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">boot_tr</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">boot_tr</span><span class="p">]</span>
    <span class="n">Xte</span><span class="p">,</span> <span class="n">yte</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">boot_te</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">boot_te</span><span class="p">]</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">ytr</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xte</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">scores_boot</span><span class="p">[</span><span class="n">boot_i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">yte</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">coefs_boot</span><span class="p">[</span><span class="n">boot_i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">coef_</span>
</pre></div>
</div>
<p>Compute Mean, SE, CI
Coeffitients 0 and 1 are significantly different from 0.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">scores_boot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores_boot</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">scores_names</span><span class="p">)</span>
<span class="n">scores_stat</span> <span class="o">=</span> <span class="n">scores_boot</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="mf">.975</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.025</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;r-squared: Mean=</span><span class="si">%.2f</span><span class="s2">, SE=</span><span class="si">%.2f</span><span class="s2">, CI=(</span><span class="si">%.2f</span><span class="s2"> </span><span class="si">%.2f</span><span class="s2">)&quot;</span> <span class="o">%</span>
      <span class="nb">tuple</span><span class="p">(</span><span class="n">scores_stat</span><span class="o">.</span><span class="n">loc</span><span class="p">[[</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;std&quot;</span><span class="p">,</span> <span class="s2">&quot;2.5%&quot;</span><span class="p">,</span> <span class="s2">&quot;97.5%&quot;</span><span class="p">],</span> <span class="s2">&quot;r2&quot;</span><span class="p">]))</span>

<span class="n">coefs_boot</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs_boot</span><span class="p">)</span>
<span class="n">coefs_stat</span> <span class="o">=</span> <span class="n">coefs_boot</span><span class="o">.</span><span class="n">describe</span><span class="p">(</span><span class="n">percentiles</span><span class="o">=</span><span class="p">[</span><span class="mf">.975</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.025</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients distribution&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefs_stat</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>r-squared: Mean=0.59, SE=0.09, CI=(0.40 0.73)
Coefficients distribution
                0           1           2           3           4
count  100.000000  100.000000  100.000000  100.000000  100.000000
mean     1.017598    1.053832    0.212464   -0.018828   -0.045851
std      0.091508    0.105196    0.097532    0.097343    0.110555
min      0.631917    0.819190   -0.002689   -0.231580   -0.270810
2.5%     0.857418    0.883319    0.032672   -0.195018   -0.233241
50%      1.027161    1.038053    0.216531   -0.010023   -0.063331
97.5%    1.174707    1.289990    0.392701    0.150340    0.141587
max      1.204006    1.449672    0.432764    0.220711    0.290928
</pre></div>
</div>
<p>Plot coefficient distribution</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">coefs_boot</span><span class="p">)</span>
<span class="n">staked</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">var_name</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s2">&quot;Coef. distribution&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_theme</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;Variable&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;Coef. distribution&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">staked</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
</pre></div>
</div>
<img src="../_images/sphx_glr_resampling_002.png" srcset="../_images/sphx_glr_resampling_002.png" alt="resampling" class = "sphx-glr-single-img"/></section>
<section id="parallel-computation">
<h1>Parallel Computation<a class="headerlink" href="#parallel-computation" title="Link to this heading">¶</a></h1>
<p>Dataset</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span>
    <span class="n">n_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Classic sequential computation of CV:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">y_test_pred_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>  <span class="c1"># Store predictions in the original order</span>
<span class="n">coefs_seq</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span>
                                         <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_test_pred_seq</span><span class="p">[</span><span class="n">test</span><span class="p">]</span> <span class="o">=</span> <span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">coefs_seq</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="n">test_accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span>
    <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_test_pred_seq</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>

<span class="c1"># Accuracy</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accs</span><span class="p">),</span> <span class="n">test_accs</span><span class="p">)</span>

<span class="c1"># Coef</span>
<span class="n">coefs_cv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs_seq</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean of the coef&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">coefs_cv</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Std Err of the coef&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">((</span><span class="n">coefs_cv</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">coefs_cv</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0.8 [0.5, 0.5, 1.0, 1.0, 1.0]
Mean of the coef
[[-0.87  0.56  1.11 -0.08 -0.32]]
Std Err of the coef
[[0.03 0.02 0.02 0.09 0.03]]
</pre></div>
</div>
<p>Parallelization using <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html">cross_validate</a>
function</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">cv_results</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;test_score&#39;</span><span class="p">]),</span> <span class="n">cv_results</span><span class="p">[</span><span class="s1">&#39;test_score&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0.8 [0.5 0.5 1.  1.  1. ]
</pre></div>
</div>
<p>Parallel computation with <a class="reference external" href="https://joblib.readthedocs.io/en/stable/">joblib</a>:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">_split_fit_predict</span><span class="p">(</span><span class="n">estimator</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">):</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">,</span>
                                         <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
    <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">estimator</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">estimator</span><span class="o">.</span><span class="n">coef_</span><span class="p">]</span>


<span class="n">estimator</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">parallel</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">cv_ret</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span>
    <span class="n">delayed</span><span class="p">(</span><span class="n">_split_fit_predict</span><span class="p">)(</span>
        <span class="n">clone</span><span class="p">(</span><span class="n">estimator</span><span class="p">),</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="n">y_test_pred_cv</span><span class="p">,</span> <span class="n">coefs_cv</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">cv_ret</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)):</span>
    <span class="n">y_test_pred</span><span class="p">[</span><span class="n">test</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_test_pred_cv</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="n">test_accs</span> <span class="o">=</span> <span class="p">[</span><span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span>
    <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y_test_pred</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">test_accs</span><span class="p">),</span> <span class="n">test_accs</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>0.8 [0.5, 0.5, 1.0, 1.0, 1.0]
</pre></div>
</div>
<p>Test same predictions and same coefficients</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">y_test_pred</span> <span class="o">==</span> <span class="n">y_test_pred_seq</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs_cv</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">coefs_seq</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (0 minutes 7.461 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-auto-gallery-resampling-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../_downloads/5208d78a1b9fd047f16e6213cc495d86/resampling.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">resampling.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../_downloads/708fa7a89378a1b3145bb56208edb659/resampling.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">resampling.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../_downloads/1b06f6620479968f0ca0a8dca92df6d0/resampling.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">resampling.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Out-of-sample Validation for Model Selection and Evaluation</a><ul>
<li><a class="reference internal" href="#train-validation-and-test-sets">Train, validation and test sets</a></li>
<li><a class="reference internal" href="#cross-validation-cv">Cross-Validation (CV)</a><ul>
<li><a class="reference internal" href="#cv-for-regression">CV for regression</a></li>
<li><a class="reference internal" href="#cv-for-classification-stratify-for-the-target-label">CV for classification: stratify for the target label</a></li>
<li><a class="reference internal" href="#cross-validation-for-model-selection-gridsearchcv">Cross-validation for model selection (GridSearchCV)</a></li>
<li><a class="reference internal" href="#cross-validation-for-both-model-outer-evaluation-and-model-inner-selection">Cross-validation for both model (outer) evaluation and model (inner) selection</a></li>
<li><a class="reference internal" href="#models-with-built-in-cross-validation">Models with built-in cross-validation</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#random-permutations-sample-the-null-distribution">Random Permutations: sample the null distribution</a></li>
<li><a class="reference internal" href="#bootstrapping">Bootstrapping</a></li>
<li><a class="reference internal" href="#parallel-computation">Parallel Computation</a></li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/auto_gallery/resampling.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/auto_gallery/resampling.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>