{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Out-of-sample Validation for Model Selection and Evaluation\n\n[Source scikit-learn model selection and evaluation](https://scikit-learn.org/stable/model_selection.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.base import is_classifier, clone\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import datasets\nimport sklearn.linear_model as lm\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n\nimport sklearn.metrics as metrics\nX, y = datasets.make_regression(n_samples=100, n_features=100,\n                                n_informative=10, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train, validation and test sets\n\nMachine learning algorithms tend to overfit training data. Predictive performances **MUST** be evaluated on independant hold-out dataset.\nA split of into a training test and an independent test set mandatory.\nHowever to set the hyperparameters the dataset is generally splitted into three sets:\n\n1. **Training Set (Fitting the Model and Learning Parameters)**\n\n- The training set is used to fit the model by learning its parameters (e.g., weights in a neural network, coefficients in a regression model).\n- The algorithm adjusts its parameters to minimize a chosen loss function (e.g., MSE for regression, cross-entropy for classification).\n- The model learns patterns from this data, but using only the training set risks overfitting\u2014where the model memorizes data instead of generalizing.\n\n- Role: Learn the parameters of the model.\n\n2. **Validation Set (Hyperparameter Tuning and Model Selection)**\n\n- The validation set is used to fine-tune the model's hyperparameters (e.g., learning rate, number of layers, number of clusters).\n- Hyperparameters are not directly learned from data but are instead set before training.\n- The validation set helps to assess different model configurations, preventing overfitting by ensuring that the model generalizes beyond the training set.\n- If we see high performance on the training set but poor performance on the validation set, we are likely overfitting.\n- The process of choosing the best hyperparameters based on the validation set is called **model selection**.\n\n- Role: Tune hyperparameters and select the best model configuration.\n- Data Leakage Risk: If we tune hyperparameters too much on the validation set, it essentially becomes part of training, leading to potential overfitting on it.\n\n3. **Test Set (Final Independent Evaluation)**\n\n- The test set is an independent dataset used to evaluate the final model after training and hyperparameter tuning.\n- This provides an unbiased estimate of how the model will perform on completely new data.\n- The model should never be trained or tuned using the test set to ensure a fair evaluation.\n- Performance metrics (e.g., accuracy, F1-score, ROC-AUC) on the test set indicate how well the model is expected to perform in real-world scenarios.\n- Role: Evaluate the final model's performance on unseen data.\n\n.. figure:: ../images/train_val_test_cv.png\n   :alt: Train, validation and test sets.\n\n\nSummary:\n\n- Training set\n      * Fits model parameters.\n      * High risk of overfitting if the model is too complex.\n- Validation set\n      * Tunes hyperparameters and selects the best model.\n      * Risk of of overfitting if tuning too much.\n- Test set\n      * Provides a final evaluation on unseen data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split dataset in train/test sets to train and assess the the final model after training and hyperparameter tuning.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, test_size=0.25, shuffle=True, random_state=42)\n\nmod = lm.Ridge(alpha=10)\n\nmod.fit(X_train, y_train)\n\ny_pred_test = mod.predict(X_test)\nprint(\"Test R2: %.2f\" % metrics.r2_score(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cross-Validation (CV)\n\nIf sample size is limited, train/validation/test split may be impossible:\n\n- Large training+validation set (80%) small test set (20%) might provide a poor estimation of the\n  predictive performances on few test samples. The same argument stands for train vs validation samples.\n- On the contrary, large test set and small training set might produce a poorly estimated learner.\n\n**Cross Validation (CV)** ([Scikit-learn](https://scikit-learn.org/stable/modules/cross_validation.html))can be used to replace train/validation split\nand/or train+validation / test split. Main procedure:\n\n1. The dataset is divided into k equal-sized subsets (folds).\n2. The model is trained k times, each time using k-1 folds as the training set and 1 fold as the validation set.\n3. The final performance is the average of the k validation scores.\n\nFor 10-fold we can either average over 10 values (Macro measure) or\nconcatenate the 10 experiments and compute the micro measures.\n\nTwo strategies [micro vs macro estimates](https://stats.stackexchange.com/questions/34611/meanscores-vs-scoreconcatenation-in-cross-validation):\n\n- **Micro measure: average(individual scores)**: compute a score\n  $\\mathcal{S}$ for each sample and average over all samples.\n  It is similar to **average score(concatenation)**: an averaged score\n  computed over all concatenated samples.\n- **Macro measure mean(CV scores)** (the most commonly used method):\n  compute a score $\\mathcal{S}$ on each each fold *k* and average\n  across folds:\n\nThese two measures (an average of average vs. a global average) are generally\nsimilar. They may differ slightly is folds are of different sizes.\nThis validation scheme is known as the **K-Fold CV**.\nTypical choices of *K* are 5 or 10, [Kohavi 1995].\nThe extreme case where *K = N* is known as **Leave-One-Out Cross-Validation,\nLOO-CV**.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CV for regression\n\nUsually the error function $\\mathcal{L}()$ is the r-squared score.\nHowever other function (MAE, MSE) can be used.\n\n**CV with explicit loop:**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = lm.Ridge(alpha=10)\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nr2_train, r2_test = list(), list()\n\nfor train, test in cv.split(X):\n    estimator.fit(X[train, :], y[train])\n    r2_train.append(metrics.r2_score(y[train], estimator.predict(X[train, :])))\n    r2_test.append(metrics.r2_score(y[test], estimator.predict(X[test, :])))\n\nprint(\"Train r2:%.2f\" % np.mean(r2_train))\nprint(\"Test  r2:%.2f\" % np.mean(r2_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scikit-learn provides user-friendly function to perform CV\n\n[cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html): single metric\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(estimator=estimator, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\n\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(estimator=estimator, X=X, y=y, cv=cv)\nprint(\"Test  r2:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html): multi metric, + time, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = cross_validate(estimator=mod, X=X, y=y, cv=cv,\n                        scoring=['r2', 'neg_mean_absolute_error'])\n\nprint(\"Test R2:%.2f; MAE:%.2f\" % (scores['test_r2'].mean(),\n                                  -scores['test_neg_mean_absolute_error'].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CV for classification: stratify for the target label\n\nWith classification problems it is essential to sample folds where each\nset contains approximately the same percentage of samples of each target\nclass as the complete set. This is called **stratification**.\nIn this case, we will use [StratifiedKFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html)\nwith is a variation of k-fold which returns stratified folds.\nAs error function we recommend:\n\n- The [balanced accuracy](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.balanced_accuracy_score.html#sklearn.metrics.balanced_accuracy_score)\n- The [ROC-AUC](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score)\n\n**CV with explicit loop**:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = datasets.make_classification(n_samples=100, n_features=100, shuffle=True,\n                                    n_informative=10, random_state=42)\n\nmod = lm.LogisticRegression(C=1, solver='lbfgs')\n\ncv = StratifiedKFold(n_splits=5)\n\n# Lists to store scores by folds (for macro measure only)\nbacc, auc = [], []\n\nfor train, test in cv.split(X, y):\n    mod.fit(X[train, :], y[train])\n    bacc.append(metrics.roc_auc_score(\n        y[test], mod.decision_function(X[test, :])))\n    auc.append(metrics.balanced_accuracy_score(\n        y[test], mod.predict(X[test, :])))\n\nprint(\"Test AUC:%.2f; bACC:%.2f\" % (np.mean(bacc), np.mean(auc)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html): single metric\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = cross_val_score(estimator=mod, X=X, y=y, cv=5)\n\nprint(\"Test  ACC:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provide your own CV and score\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def balanced_acc(estimator, X, y, **kwargs):\n    \"\"\"Balanced acuracy scorer.\"\"\"\n    return metrics.recall_score(y, estimator.predict(X), average=None).mean()\n\n\nscores = cross_val_score(estimator=mod, X=X, y=y, cv=cv,\n                         scoring=balanced_acc)\nprint(\"Test  bACC:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html): multi metric, + time, etc.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = cross_validate(estimator=mod, X=X, y=y, cv=cv,\n                        scoring=['balanced_accuracy', 'roc_auc'])\n\nprint(\"Test AUC:%.2f; bACC:%.2f\" % (scores['test_roc_auc'].mean(),\n                                    scores['test_balanced_accuracy'].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validation for model selection (GridSearchCV)\n\nCombine CV and grid search:\n[GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\nperform hyperparameter tuning (model selection) by systematically searching the best combination of hyperparameters\nevaluating all possible combinations (over a grid of possible values) using cross-validation:\n\n1. Define the model: Choose a machine learning model (e.g., SVM, Random Forest).\n2. Specify hyperparameters: Create a dictionary of hyperparameters and their possible values.\n3. Perform exhaustive search: GridSearchCV trains the model with every possible combination of hyperparameters.\n4. Cross-validation: For each combination, it uses k-fold cross-validation (default cv=5).\n5. Select the best model: The combination with the highest validation performance is chosen.\n   By default, refit an estimator using the best found parameters on the whole dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Outer, tain/test, split:\nX_train, X_test, y_train, y_test =\\\n    train_test_split(X, y, test_size=0.25, shuffle=True, random_state=42)\n\ncv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Inner Cross-Validation (tain/validation, splits) for model selection\nlm_cv = GridSearchCV(lm.LogisticRegression(), {'C': 10. ** np.arange(-3, 3)},\n                     cv=cv_inner, n_jobs=5)\n\n# Fit, including model selection with internal CV\nlm_cv.fit(X_train, y_train)\n\n# Predict\ny_pred_test = lm_cv.predict(X_test)\nprint(\"Test bACC: %.2f\" % metrics.balanced_accuracy_score(y_test, y_pred_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validation for both model (outer) evaluation and model (inner) selection\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cv_outer = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ncv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Cross-validation for model (inner) selection\nlm_cv = GridSearchCV(lm.Ridge(), {'alpha': 10. ** np.arange(-3, 3)},\n                     cv=cv_inner, n_jobs=5)\n\n# Cross-validation for model (outer) evaluation\nscores = cross_validate(estimator=mod, X=X, y=y, cv=cv_outer,\n                        scoring=['balanced_accuracy', 'roc_auc'])\n\nprint(\"Test AUC:%.2f; bACC:%.2f, Time: %.2fs\" % (scores['test_roc_auc'].mean(),\n                                                 scores['test_balanced_accuracy'].mean(\n),\n    scores['fit_time'].sum()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Models with built-in cross-validation\n\nLet sklearn select the best parameters over a default grid.\n\n**Classification**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"== Logistic Ridge (L2 penalty) ==\")\nmod_cv = lm.LogisticRegressionCV(class_weight='balanced',\n                                 scoring='balanced_accuracy',\n                                 n_jobs=-1, cv=5)\nscores = cross_val_score(estimator=mod_cv, X=X, y=y, cv=5)\nprint(\"Test  ACC:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Regression**\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y, coef = datasets.make_regression(n_samples=50, n_features=100, noise=10,\n                                      n_informative=2, random_state=42, coef=True)\n\nprint(\"== Ridge (L2 penalty) ==\")\nmodel = lm.RidgeCV(cv=3)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\n\nprint(\"== Lasso (L1 penalty) ==\")\nmodel = lm.LassoCV(n_jobs=-1, cv=3)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())\n\nprint(\"== ElasticNet (L1 penalty) ==\")\nmodel = lm.ElasticNetCV(l1_ratio=[.1, .5, .9], n_jobs=-1, cv=3)\nscores = cross_val_score(estimator=model, X=X, y=y, cv=5)\nprint(\"Test  r2:%.2f\" % scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Permutations: sample the null distribution\n\nA permutation test is a type of non-parametric randomization test in which the null distribution of a test statistic is estimated by randomly permuting the observations.\n\nPermutation tests are highly attractive because they make no assumptions other than that the observations are independent and identically distributed under the null hypothesis.\n\n1. Compute a observed statistic $t_{obs}$ on the data.\n2. Use randomization to compute the distribution of $t$ under the null hypothesis: Perform $N$ random permutation of the data. For each sample of permuted data, $i$ the data compute the statistic $t_i$. This procedure provides the distribution of *t* under the null hypothesis $H_0$: $P(t \\vert H_0)$\n3. Compute the p-value = $P(t>t_{obs} | H_0) \\left\\vert\\{t_i > t_{obs}\\}\\right\\vert$, where $t_i's include :math:$t_{obs}`.\n\nExample Ridge regression\n\nSample the distributions of r-squared and coefficients of ridge regression under the null hypothesis. Simulated dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Regression dataset where first two features are predictive\nnp.random.seed(0)\nn_features = 5\nn_features_info = 2\nn_samples = 100\nX = np.random.randn(100, 5)\nbeta = np.zeros(n_features)\nbeta[:n_features_info] = 1\nXbeta = np.dot(X, beta)\neps = np.random.randn(n_samples)\ny = Xbeta + eps\n\n\n# Fit model on all data (!! risk of overfit)\nmodel = lm.RidgeCV()\nmodel.fit(X, y)\nprint(\"Coefficients on all data:\")\nprint(model.coef_)\n\n# Random permutation loop\nnperm = 1000  # !! Should be at least 1000 (to assess a p-value at 1%)\nscores_names = [\"r2\"]\nscores_perm = np.zeros((nperm + 1, len(scores_names)))\ncoefs_perm = np.zeros((nperm + 1, X.shape[1]))\n\nscores_perm[0, :] = metrics.r2_score(y, model.predict(X))\ncoefs_perm[0, :] = model.coef_\n\norig_all = np.arange(X.shape[0])\nfor perm_i in range(1, nperm + 1):\n    model.fit(X, np.random.permutation(y))\n    y_pred = model.predict(X).ravel()\n    scores_perm[perm_i, :] = metrics.r2_score(y, y_pred)\n    coefs_perm[perm_i, :] = model.coef_\n\n# One-tailed empirical p-value\npval_pred_perm = np.sum(scores_perm >= scores_perm[0]) / scores_perm.shape[0]\npval_coef_perm = np.sum(\n    coefs_perm >= coefs_perm[0, :], axis=0) / coefs_perm.shape[0]\n\nprint(\"R2 p-value: %.3f\" % pval_pred_perm)\nprint(\"Coeficients p-values:\", np.round(pval_coef_perm, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute p-values corrected for multiple comparisons using FWER max-T\n(Westfall and Young, 1993) procedure.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pval_coef_perm_tmax = np.array([np.sum(coefs_perm.max(axis=1) >= coefs_perm[0, j])\n                                for j in range(coefs_perm.shape[1])]) / coefs_perm.shape[0]\nprint(\"P-values with FWER (Westfall and Young) correction\")\nprint(pval_coef_perm_tmax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot distribution of third coefficient under null-hypothesis\nCoeffitients 0 and 1 are significantly different from 0.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def hist_pvalue(perms, ax, name):\n    \"\"\"Plot statistic distribution as histogram.\n\n    Paramters\n    ---------\n    perms: 1d array, statistics under the null hypothesis.\n           perms[0] is the true statistic .\n    \"\"\"\n    # Re-weight to obtain distribution\n    pval = np.sum(perms >= perms[0]) / perms.shape[0]\n    weights = np.ones(perms.shape[0]) / perms.shape[0]\n    ax.hist([perms[perms >= perms[0]], perms], histtype='stepfilled',\n            bins=100, label=\"p-val<%.3f\" % pval,\n            weights=[weights[perms >= perms[0]], weights])\n    # , label=\"observed statistic\")\n    ax.axvline(x=perms[0], color=\"k\", linewidth=2)\n    ax.set_ylabel(name)\n    ax.legend()\n    return ax\n\n\nn_coef = coefs_perm.shape[1]\nfig, axes = plt.subplots(n_coef, 1, figsize=(12, 9))\nfor i in range(n_coef):\n    hist_pvalue(coefs_perm[:, i], axes[i], str(i))\n\n_ = axes[-1].set_xlabel(\"Coefficient distribution under null hypothesis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exercise\n\nGiven the logistic regression presented above and its validation given a 5 folds CV.\n\n1. Compute the p-value associated with the prediction accuracy measured with 5CV using a permutation test.\n\n2. Compute the p-value associated with the prediction accuracy using a parametric test.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bootstrapping\n\nBootstrapping is a statistical technique which consists in generating sample (called bootstrap samples) from an initial dataset of size *N* by randomly drawing with replacement *N* observations. It provides sub-samples with the same distribution than the original dataset. It aims to:\n\n1. Assess the variability (standard error, [Confidence Intervals (CI)](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html#the-bootstrap-method-and-empirical-confidence-intervals) of performances scores or estimated parameters (see Efron et al. 1986).\n2. Regularize model by fitting several models on bootstrap samples and averaging their predictions (see Baging and random-forest).\n\nA great advantage of bootstrap is its simplicity. It is a straightforward way to derive estimates of standard errors and confidence intervals for complex estimators of complex parameters of the distribution, such as percentile points, proportions, odds ratio, and correlation coefficients.\n\n1. Perform $B$ sampling, with replacement, of the dataset.\n2. For each sample $i$ fit the model and compute the scores.\n3. Assess standard errors and confidence intervals of scores using the scores obtained on the $B$ resampled dataset. Or, average models predictions.\n\nReferences:\n\n[Efron B, Tibshirani R. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Stat Sci 1986;1:54\u201375](https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Bootstrap loop\nnboot = 100  # !! Should be at least 1000\nscores_names = [\"r2\"]\nscores_boot = np.zeros((nboot, len(scores_names)))\ncoefs_boot = np.zeros((nboot, X.shape[1]))\n\norig_all = np.arange(X.shape[0])\nfor boot_i in range(nboot):\n    boot_tr = np.random.choice(orig_all, size=len(orig_all), replace=True)\n    boot_te = np.setdiff1d(orig_all, boot_tr, assume_unique=False)\n    Xtr, ytr = X[boot_tr, :], y[boot_tr]\n    Xte, yte = X[boot_te, :], y[boot_te]\n    model.fit(Xtr, ytr)\n    y_pred = model.predict(Xte).ravel()\n    scores_boot[boot_i, :] = metrics.r2_score(yte, y_pred)\n    coefs_boot[boot_i, :] = model.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compute Mean, SE, CI\nCoeffitients 0 and 1 are significantly different from 0.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores_boot = pd.DataFrame(scores_boot, columns=scores_names)\nscores_stat = scores_boot.describe(percentiles=[.975, .5, .025])\n\nprint(\"r-squared: Mean=%.2f, SE=%.2f, CI=(%.2f %.2f)\" %\n      tuple(scores_stat.loc[[\"mean\", \"std\", \"2.5%\", \"97.5%\"], \"r2\"]))\n\ncoefs_boot = pd.DataFrame(coefs_boot)\ncoefs_stat = coefs_boot.describe(percentiles=[.975, .5, .025])\nprint(\"Coefficients distribution\")\nprint(coefs_stat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot coefficient distribution\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(coefs_boot)\nstaked = pd.melt(df, var_name=\"Variable\", value_name=\"Coef. distribution\")\nsns.set_theme(style=\"whitegrid\")\nax = sns.violinplot(x=\"Variable\", y=\"Coef. distribution\", data=staked)\n_ = ax.axhline(0, ls='--', lw=2, color=\"black\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parallel Computation\n\nDataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X, y = datasets.make_classification(\n    n_samples=20, n_features=5, n_informative=2, random_state=42)\ncv = StratifiedKFold(n_splits=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classic sequential computation of CV:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = lm.LogisticRegression(C=1, solver='lbfgs')\ny_test_pred_seq = np.zeros(len(y))  # Store predictions in the original order\ncoefs_seq = list()\nfor train, test in cv.split(X, y):\n    X_train, X_test, y_train, y_test = X[train,\n                                         :], X[test, :], y[train], y[test]\n    estimator.fit(X_train, y_train)\n    y_test_pred_seq[test] = estimator.predict(X_test)\n    coefs_seq.append(estimator.coef_)\n\ntest_accs = [metrics.accuracy_score(\n    y[test], y_test_pred_seq[test]) for train, test in cv.split(X, y)]\n\n# Accuracy\nprint(np.mean(test_accs), test_accs)\n\n# Coef\ncoefs_cv = np.array(coefs_seq)\nprint(\"Mean of the coef\")\nprint(coefs_cv.mean(axis=0).round(2))\nprint(\"Std Err of the coef\")\nprint((coefs_cv.std(axis=0) / np.sqrt(coefs_cv.shape[0])).round(2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallelization using [cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\nfunction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "estimator = lm.LogisticRegression(C=1, solver='lbfgs')\ncv_results = cross_validate(estimator, X, y, cv=cv, n_jobs=5)\nprint(np.mean(cv_results['test_score']), cv_results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parallel computation with [joblib](https://joblib.readthedocs.io/en/stable/):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _split_fit_predict(estimator, X, y, train, test):\n    X_train, X_test, y_train, y_test = X[train,\n                                         :], X[test, :], y[train], y[test]\n    estimator.fit(X_train, y_train)\n    return [estimator.predict(X_test), estimator.coef_]\n\n\nestimator = lm.LogisticRegression(C=1, solver='lbfgs')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "parallel = Parallel(n_jobs=5)\ncv_ret = parallel(\n    delayed(_split_fit_predict)(\n        clone(estimator), X, y, train, test)\n    for train, test in cv.split(X, y))\n\ny_test_pred_cv, coefs_cv = zip(*cv_ret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "y_test_pred = np.zeros(len(y))\nfor i, (train, test) in enumerate(cv.split(X, y)):\n    y_test_pred[test] = y_test_pred_cv[i]\n\ntest_accs = [metrics.accuracy_score(\n    y[test], y_test_pred[test]) for train, test in cv.split(X, y)]\nprint(np.mean(test_accs), test_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test same predictions and same coefficients\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "assert np.all(y_test_pred == y_test_pred_seq)\nassert np.allclose(np.array(coefs_cv).squeeze(), np.array(coefs_seq).squeeze())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}