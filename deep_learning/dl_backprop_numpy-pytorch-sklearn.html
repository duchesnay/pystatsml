<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Backpropagation &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Multilayer Perceptron (MLP)" href="dl_mlp_pytorch.html" />
    <link rel="prev" title="Out-of-sample Validation for Model Selection and Evaluation" href="../auto_gallery/resampling.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="backpropagation">
<h1>Backpropagation<a class="headerlink" href="#backpropagation" title="Link to this heading">¶</a></h1>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Backpropagation">Wikipedia:
Backpropagation</a>: In
machine learning, backpropagation is a gradient estimation method
commonly used for training a neural network to compute its parameter
updates. Backpropagation applies gradient descent using chain rule.</p>
<p>Sources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=1">3Blue1Brown video: But what is a neural network? | Deep learning
chapter
1</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=IHZwWFHWa-w&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=2">3Blue1Brown video: Gradient descent, how neural networks learn |
DL2</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=Ilg3gGewQ5U&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=3">3Blue1Brown video: Backpropagation, step-by-step |
DL3</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=tIeHLnjs5U8&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=4">3Blue1Brown video: Backpropagation calculus |
DL4</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Pytorch
examples</a></p></li>
</ul>
<section id="backpropagation-and-chain-rule">
<h2>Backpropagation and chain rule<a class="headerlink" href="#backpropagation-and-chain-rule" title="Link to this heading">¶</a></h2>
<p>We will set up a two layer network:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = \text{max}(\mathbf{X} \mathbf{W}^{(1)}, 0) \mathbf{W}^{(2)}\]</div>
<p>A fully-connected ReLU network with one hidden layer and no biases,
trained to predict y from x using Euclidean error.</p>
<p><strong>Forward pass</strong> with <strong>local</strong> partial derivatives of output given
inputs:</p>
<p><strong>Backward pass</strong>: compute gradient of the loss given each parameters
vectors applying the chain rule from the loss downstream to the
parameters:</p>
<p>For <span class="math notranslate nohighlight">\(w^{(2)}\)</span>:</p>
<p>For <span class="math notranslate nohighlight">\(w^{(1)}\)</span>:</p>
<section id="recap-vector-derivatives">
<h3>Recap: Vector derivatives<a class="headerlink" href="#recap-vector-derivatives" title="Link to this heading">¶</a></h3>
<p>Given a function <span class="math notranslate nohighlight">\(z = x`w\)</span> with <span class="math notranslate nohighlight">\(z\)</span> the output, <span class="math notranslate nohighlight">\(x\)</span>
the input and <span class="math notranslate nohighlight">\(w\)</span> the coefficients.</p>
<p><strong>Scalar to Scalar</strong>: <span class="math notranslate nohighlight">\(x \in \mathbb{R}, z \in \mathbb{R}\)</span>,
<span class="math notranslate nohighlight">\(w \in \mathbb{R}\)</span>. Regular derivative:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial z}{\partial w} = x \in \mathbb{R}\]</div>
<p>If <span class="math notranslate nohighlight">\(w\)</span> changes by a small amount, how much will <span class="math notranslate nohighlight">\(z\)</span> change?</p>
<p><strong>Vector to Scalar</strong>: <span class="math notranslate nohighlight">\(x \in \mathbb{R}^N, z \in \mathbb{R}\)</span>,
<span class="math notranslate nohighlight">\(w \in \mathbb{R}^N\)</span>. The derivative is the <strong>Gradient</strong> of
partial derivative:
<span class="math notranslate nohighlight">\(\frac{\partial z}{\partial w} \in \mathbb{R}^N\)</span></p>
<p>For each element <span class="math notranslate nohighlight">\(w_i\)</span> of <span class="math notranslate nohighlight">\(w\)</span>, if it changes by a small
amount then how much will y change?</p>
<p><strong>Vector to Vector</strong>: <span class="math notranslate nohighlight">\(w \in \mathbb{R}^N, z \in \mathbb{R}^M\)</span>.
The derivative is <strong>Jacobian</strong> of partial derivative:</p>
<p>TO BE COMPLETED</p>
<p><span class="math notranslate nohighlight">\(\frac{\partial z}{\partial w} \in \mathbb{R}^{N \times M}\)</span></p>
</section>
<section id="backpropagation-summary">
<h3>Backpropagation summary<a class="headerlink" href="#backpropagation-summary" title="Link to this heading">¶</a></h3>
<p>Backpropagation algorithm in a graph:</p>
<ol class="arabic simple">
<li><p><strong>Forward pass</strong>, for each node compute local partial derivatives of
output given inputs.</p></li>
<li><p><strong>Backward pass</strong>: apply chain rule from the end to each parameters.</p>
<ul class="simple">
<li><p>Update parameter with gradient descent using the current upstream
gradient and the current local gradient.</p></li>
<li><p>Compute upstream gradient for the backward nodes.</p></li>
</ul>
</li>
</ol>
<p>Think locally and remember that at each node:</p>
<ul class="simple">
<li><p>For the loss the gradient is the error</p></li>
<li><p>At each step, the upstream gradient is obtained by multiplying the
upstream gradient (an error) with the current parameters (vector of
matrix).</p></li>
<li><p>At each step, the current local gradient equal the input, therefore
the current update is the current upstream gradient time the input.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.model_selection</span>

<span class="c1"># Plot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Plot parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">)</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>
</div>
</section>
</section>
<section id="hands-on-with-numpy-and-pytorch">
<h2>Hands-on with Numpy and pytorch<a class="headerlink" href="#hands-on-with-numpy-and-pytorch" title="Link to this heading">¶</a></h2>
<section id="load-iris-data-set">
<h3>Load iris data set<a class="headerlink" href="#load-iris-data-set" title="Link to this heading">¶</a></h3>
<p>Goal: Predict Y = [petal_length, petal_width] = f(X = [sepal_length,
sepal_width])</p>
<ul class="simple">
<li><p>Plot data with seaborn</p></li>
<li><p>Remove setosa samples</p></li>
<li><p>Recode ‘versicolor’:1, ‘virginica’:2</p></li>
<li><p>Scale X and Y</p></li>
<li><p>Split data in train/test 50%/50%</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iris</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;iris&quot;</span><span class="p">)</span>
<span class="c1">#g = sns.pairplot(iris, hue=&quot;species&quot;)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">iris</span><span class="p">[</span><span class="n">iris</span><span class="o">.</span><span class="n">species</span> <span class="o">!=</span> <span class="s2">&quot;setosa&quot;</span><span class="p">]</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;species&quot;</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;species_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">species</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;versicolor&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;virginica&#39;</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>

<span class="c1"># Y = &#39;petal_length&#39;, &#39;petal_width&#39;; X = &#39;sepal_length&#39;, &#39;sepal_width&#39;)</span>
<span class="n">X_iris</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;sepal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;sepal_width&#39;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">Y_iris</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="p">[</span><span class="s1">&#39;petal_length&#39;</span><span class="p">,</span> <span class="s1">&#39;petal_width&#39;</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">label_iris</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">species_n</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

<span class="c1"># Scale</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">scalerx</span><span class="p">,</span> <span class="n">scalery</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_iris</span> <span class="o">=</span> <span class="n">scalerx</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_iris</span><span class="p">)</span>
<span class="n">Y_iris</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y_iris</span><span class="p">)</span>

<span class="c1"># Split train test</span>
<span class="n">X_iris_tr</span><span class="p">,</span> <span class="n">X_iris_val</span><span class="p">,</span> <span class="n">Y_iris_tr</span><span class="p">,</span> <span class="n">Y_iris_val</span><span class="p">,</span> <span class="n">label_iris_tr</span><span class="p">,</span> <span class="n">label_iris_val</span> <span class="o">=</span> \
    <span class="n">sklearn</span><span class="o">.</span><span class="n">model_selection</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">X_iris</span><span class="p">,</span> <span class="n">Y_iris</span><span class="p">,</span> <span class="n">label_iris</span><span class="p">,</span>
                                             <span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">label_iris</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="backpropagation-with-numpy">
<h3>Backpropagation with Numpy<a class="headerlink" href="#backpropagation-with-numpy" title="Link to this heading">¶</a></h3>
<p>This implementation uses Numpy to manually compute the forward pass,
loss, and backward pass.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># X=X_iris_tr; Y=Y_iris_tr; X_val=X_iris_val; Y_val=Y_iris_val</span>

<span class="k">def</span><span class="w"> </span><span class="nf">two_layer_regression_numpy_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">nite</span><span class="p">):</span>
    <span class="c1"># N is batch size; D_in is input dimension;</span>
    <span class="c1"># H is hidden dimension; D_out is output dimension.</span>
    <span class="c1"># N, D_in, H, D_out = 64, 1000, 100, 10</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>

    <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nite</span><span class="p">):</span>
        <span class="c1"># Forward pass: compute predicted y</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">Y_pred</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>

        <span class="c1"># Compute and print loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">Y_pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Backprop to compute gradients of w1 and w2 with respect to loss</span>
        <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">Y_pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
        <span class="n">grad_h1</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="n">grad_z1</span> <span class="o">=</span> <span class="n">grad_h1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="n">grad_z1</span><span class="p">[</span><span class="n">z1</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">grad_z1</span><span class="p">)</span>

        <span class="c1"># Update weights</span>
        <span class="n">W1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
        <span class="n">W2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>

        <span class="c1"># Forward pass for validation set: compute predicted y</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">z1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">y_pred_val</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>
        <span class="n">loss_val</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">y_pred_val</span> <span class="o">-</span> <span class="n">Y_val</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="n">losses_tr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">losses_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span>

<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> \
    <span class="n">two_layer_regression_numpy_train</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_iris_tr</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_iris_tr</span><span class="p">,</span>
                                     <span class="n">X_val</span><span class="o">=</span><span class="n">X_iris_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">=</span><span class="n">Y_iris_val</span><span class="p">,</span>
                                    <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">nite</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_tr</span><span class="p">)),</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="s2">&quot;-b&quot;</span><span class="p">,</span>
             <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_val</span><span class="p">)),</span> <span class="n">losses_val</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="backpropagation-with-pytorch-tensors">
<h3>Backpropagation with PyTorch Tensors<a class="headerlink" href="#backpropagation-with-pytorch-tensors" title="Link to this heading">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Learning PyTorch with
Examples</a></p>
<p>Numpy is a great framework, but it cannot utilize GPUs to accelerate its
numerical computations. For modern deep neural networks, GPUs often
provide speedups of 50x or greater, so unfortunately numpy won’t be
enough for modern deep learning. Here we introduce the most fundamental
PyTorch concept: the Tensor. A PyTorch Tensor is conceptually identical
to a numpy array: a Tensor is an n-dimensional array, and PyTorch
provides many functions for operating on these Tensors. Behind the
scenes, Tensors can keep track of a computational graph and gradients,
but they’re also useful as a generic tool for scientific computing. Also
unlike numpy, PyTorch Tensors can utilize GPUs to accelerate their
numeric computations. To run a PyTorch Tensor on GPU, you simply need to
cast it to a new datatype. Here we use PyTorch Tensors to fit a
two-layer network to random data. Like the numpy example above we need
to manually implement the forward and backward passes through the
network:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># X=X_iris_tr; Y=Y_iris_tr; X_val=X_iris_val; Y_val=Y_iris_val</span>

<span class="k">def</span><span class="w"> </span><span class="nf">two_layer_regression_tensor_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">nite</span><span class="p">):</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="c1"># device = torch.device(&quot;cuda:0&quot;) # Uncomment this to run on GPU</span>

    <span class="c1"># N is batch size; D_in is input dimension;</span>
    <span class="c1"># H is hidden dimension; D_out is output dimension.</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Create random input and output data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
    <span class="n">Y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y_val</span><span class="p">)</span>

    <span class="c1"># Randomly initialize weights</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

    <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nite</span><span class="p">):</span>
        <span class="c1"># Forward pass: compute predicted y</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">z1</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>

        <span class="c1"># Compute and print loss</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="c1"># Backprop to compute gradients of w1 and w2 with respect to loss</span>
        <span class="n">grad_y_pred</span> <span class="o">=</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span>
        <span class="n">grad_w2</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_y_pred</span><span class="p">)</span>
        <span class="n">grad_h1</span> <span class="o">=</span> <span class="n">grad_y_pred</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="n">grad_z1</span> <span class="o">=</span> <span class="n">grad_h1</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
        <span class="n">grad_z1</span><span class="p">[</span><span class="n">z1</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">grad_w1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">grad_z1</span><span class="p">)</span>

        <span class="c1"># Update weights using gradient descent</span>
        <span class="n">W1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w1</span>
        <span class="n">W2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_w2</span>

        <span class="c1"># Forward pass for validation set: compute predicted y</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">z1</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">y_pred_val</span> <span class="o">=</span> <span class="n">h1</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>
        <span class="n">loss_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred_val</span> <span class="o">-</span> <span class="n">Y_val</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

        <span class="n">losses_tr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">losses_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">loss_val</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span>

<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> \
    <span class="n">two_layer_regression_tensor_train</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_iris_tr</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_iris_tr</span><span class="p">,</span> <span class="n">X_val</span><span class="o">=</span><span class="n">X_iris_val</span><span class="p">,</span>
                                      <span class="n">Y_val</span><span class="o">=</span><span class="n">Y_iris_val</span><span class="p">,</span>
                                      <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">nite</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_tr</span><span class="p">)),</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="s2">&quot;-b&quot;</span><span class="p">,</span>
             <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_val</span><span class="p">)),</span> <span class="n">losses_val</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="backpropagation-with-pytorch-tensors-and-autograd">
<h3>Backpropagation with PyTorch: Tensors and autograd<a class="headerlink" href="#backpropagation-with-pytorch-tensors-and-autograd" title="Link to this heading">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Learning PyTorch with
Examples</a></p>
<p>A fully-connected ReLU network with one hidden layer and no biases,
trained to predict y from x by minimizing squared Euclidean distance.
This implementation computes the forward pass using operations on
PyTorch Tensors, and uses PyTorch autograd to compute gradients. A
PyTorch Tensor represents a node in a computational graph. If <code class="docutils literal notranslate"><span class="pre">x</span></code> is a
Tensor that has <code class="docutils literal notranslate"><span class="pre">x.requires_grad=True</span></code> then <code class="docutils literal notranslate"><span class="pre">x.grad</span></code> is another
Tensor holding the gradient of <code class="docutils literal notranslate"><span class="pre">x</span></code> with respect to some scalar value.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="k">def</span><span class="w"> </span><span class="nf">two_layer_regression_autograd_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">nite</span><span class="p">):</span>

    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="c1"># device = torch.device(&quot;cuda:0&quot;) # Uncomment this to run on GPU</span>

    <span class="c1"># N is batch size; D_in is input dimension;</span>
    <span class="c1"># H is hidden dimension; D_out is output dimension.</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># Setting requires_grad=False indicates that we do not need to compute</span>
    <span class="c1"># gradients with respect to these Tensors during the backward pass.</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
    <span class="n">Y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y_val</span><span class="p">)</span>

    <span class="c1"># Create random Tensors for weights.</span>
    <span class="c1"># Setting requires_grad=True indicates that we want to compute gradients</span>
    <span class="c1"># with respect to these Tensors during the backward pass.</span>
    <span class="n">W1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nite</span><span class="p">):</span>
        <span class="c1"># Forward pass: compute predicted y using operations on Tensors; these</span>
        <span class="c1"># are exactly the same operations we used to compute the forward pass</span>
        <span class="c1"># using Tensors, but we do not need to keep references to intermediate</span>
        <span class="c1"># values since we are not implementing the backward pass by hand.</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>

        <span class="c1"># Compute and print loss using operations on Tensors.</span>
        <span class="c1"># Now loss is a Tensor of shape (1,)</span>
        <span class="c1"># loss.item() gets the scalar value held in the loss.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="c1"># Use autograd to compute the backward pass. This call will compute the</span>
        <span class="c1"># gradient of loss with respect to all Tensors with requires_grad=True.</span>
        <span class="c1"># After this call w1.grad and w2.grad will be Tensors holding the</span>
        <span class="c1"># gradient of the loss with respect to w1 and w2 respectively.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Manually update weights using gradient descent. Wrap in torch.no_grad()</span>
        <span class="c1"># because weights have requires_grad=True, but we don&#39;t need to track this</span>
        <span class="c1"># in autograd.</span>
        <span class="c1"># An alternative way is to operate on weight.data and weight.grad.data.</span>
        <span class="c1"># Recall that tensor.data gives a tensor that shares the storage with</span>
        <span class="c1"># tensor, but doesn&#39;t track history.</span>
        <span class="c1"># You can also use torch.optim.SGD to achieve this.</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">W1</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">W1</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">W2</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">W2</span><span class="o">.</span><span class="n">grad</span>

            <span class="c1"># Manually zero the gradients after updating weights</span>
            <span class="n">W1</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>
            <span class="n">W2</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">zero_</span><span class="p">()</span>

            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">X_val</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">mm</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span>

            <span class="c1"># Compute and print loss using operations on Tensors.</span>
            <span class="c1"># Now loss is a Tensor of shape (1,)</span>
            <span class="c1"># loss.item() gets the scalar value held in the loss.</span>
            <span class="n">loss_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">Y</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">losses_tr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">losses_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span>

<span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> \
    <span class="n">two_layer_regression_autograd_train</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_iris_tr</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_iris_tr</span><span class="p">,</span>
                                        <span class="n">X_val</span><span class="o">=</span><span class="n">X_iris_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">=</span><span class="n">Y_iris_val</span><span class="p">,</span>
                                        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">nite</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_tr</span><span class="p">)),</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="s2">&quot;-b&quot;</span><span class="p">,</span>
             <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_val</span><span class="p">)),</span> <span class="n">losses_val</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="backpropagation-with-pytorch-nn">
<h3>Backpropagation with PyTorch: nn<a class="headerlink" href="#backpropagation-with-pytorch-nn" title="Link to this heading">¶</a></h3>
<p><a class="reference external" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Learning PyTorch with
Examples</a></p>
<p>This implementation uses the nn package from PyTorch to build the
network. PyTorch autograd makes it easy to define computational graphs
and take gradients, but raw autograd can be a bit too low-level for
defining complex neural networks; this is where the nn package can help.
The nn package defines a set of Modules, which you can think of as a
neural network layer that has produces output from input and may have
some trainable weights.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="k">def</span><span class="w"> </span><span class="nf">two_layer_regression_nn_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">nite</span><span class="p">):</span>

    <span class="c1"># N is batch size; D_in is input dimension;</span>
    <span class="c1"># H is hidden dimension; D_out is output dimension.</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
    <span class="n">Y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y_val</span><span class="p">)</span>

    <span class="c1"># Use the nn package to define our model as a sequence of layers.</span>
    <span class="c1"># nn.Sequential is a Module which contains other Modules, and applies</span>
    <span class="c1"># them in sequence to produce its output. Each Linear Module computes</span>
    <span class="c1"># output from input using a linear function, and holds internal Tensors</span>
    <span class="c1"># for its weight and bias.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="c1"># The nn package also contains definitions of popular loss functions; in this</span>
    <span class="c1"># case we will use Mean Squared Error (MSE) as our loss function.</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

    <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nite</span><span class="p">):</span>
        <span class="c1"># Forward pass: compute predicted y by passing x to the model. Module</span>
        <span class="c1"># objects override the __call__ operator so you can call them like</span>
        <span class="c1"># functions. When doing so you pass a Tensor of input data to the Module</span>
        <span class="c1"># and it produces a Tensor of output data.</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Compute and print loss. We pass Tensors containing the predicted and</span>
        <span class="c1"># true values of y, and the loss function returns a Tensor containing the</span>
        <span class="c1"># loss.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

        <span class="c1"># Zero the gradients before running the backward pass.</span>
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Backward pass: compute gradient of the loss with respect to all the</span>
        <span class="c1"># learnable parameters of the model. Internally, the parameters of each</span>
        <span class="c1"># Module are stored in Tensors with requires_grad=True, so this call</span>
        <span class="c1"># will compute gradients for all learnable parameters in the model.</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Update the weights using gradient descent. Each parameter is a Tensor,</span>
        <span class="c1"># so we can access its gradients like we did before.</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
                <span class="n">param</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">param</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
            <span class="n">loss_val</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">Y_val</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">losses_tr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">losses_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span>


<span class="n">model</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> \
    <span class="n">two_layer_regression_nn_train</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_iris_tr</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_iris_tr</span><span class="p">,</span>
                                  <span class="n">X_val</span><span class="o">=</span><span class="n">X_iris_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">=</span><span class="n">Y_iris_val</span><span class="p">,</span>
                                  <span class="n">lr</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">nite</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_tr</span><span class="p">)),</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="s2">&quot;-b&quot;</span><span class="p">,</span>
             <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_val</span><span class="p">)),</span> <span class="n">losses_val</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="backpropagation-with-pytorch-optim">
<h3>Backpropagation with PyTorch optim<a class="headerlink" href="#backpropagation-with-pytorch-optim" title="Link to this heading">¶</a></h3>
<p>This implementation uses the nn package from PyTorch to build the
network. Rather than manually updating the weights of the model as we
have been doing, we use the optim package to define an Optimizer that
will update the weights for us. The optim package defines many
optimization algorithms that are commonly used for deep learning,
including SGD+momentum, RMSProp, Adam, etc.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="k">def</span><span class="w"> </span><span class="nf">two_layer_regression_nn_optim_train</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">nite</span><span class="p">):</span>

    <span class="c1"># N is batch size; D_in is input dimension;</span>
    <span class="c1"># H is hidden dimension; D_out is output dimension.</span>
    <span class="n">N</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">D_out</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">100</span><span class="p">,</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">Y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>
    <span class="n">X_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
    <span class="n">Y_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">Y_val</span><span class="p">)</span>

    <span class="c1"># Use the nn package to define our model and loss function.</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="n">H</span><span class="p">),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">D_out</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">)</span>

    <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(),</span> <span class="nb">list</span><span class="p">()</span>

    <span class="c1"># Use the optim package to define an Optimizer that will update the weights of</span>
    <span class="c1"># the model for us. Here we will use Adam; the optim package contains many</span>
    <span class="c1"># other optimization algorithm. The first argument to the Adam constructor</span>
    <span class="c1"># tells the optimizer which Tensors it should update.</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nite</span><span class="p">):</span>
        <span class="c1"># Forward pass: compute predicted y by passing x to the model.</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="c1"># Compute and print loss.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>

        <span class="c1"># Before the backward pass, use the optimizer object to zero all of the</span>
        <span class="c1"># gradients for the variables it will update (which are the learnable</span>
        <span class="c1"># weights of the model). This is because by default, gradients are</span>
        <span class="c1"># accumulated in buffers( i.e, not overwritten) whenever .backward()</span>
        <span class="c1"># is called. Checkout docs of torch.autograd.backward for more details.</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

        <span class="c1"># Backward pass: compute gradient of the loss with respect to model</span>
        <span class="c1"># parameters</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

        <span class="c1"># Calling the step function on an Optimizer makes an update to its</span>
        <span class="c1"># parameters</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X_val</span><span class="p">)</span>
            <span class="n">loss_val</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">Y_val</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">t</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="n">losses_tr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
        <span class="n">losses_val</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss_val</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">model</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span>


<span class="n">model</span><span class="p">,</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="n">losses_val</span> <span class="o">=</span> \
    <span class="n">two_layer_regression_nn_optim_train</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">X_iris_tr</span><span class="p">,</span> <span class="n">Y</span><span class="o">=</span><span class="n">Y_iris_tr</span><span class="p">,</span>
                                        <span class="n">X_val</span><span class="o">=</span><span class="n">X_iris_val</span><span class="p">,</span> <span class="n">Y_val</span><span class="o">=</span><span class="n">Y_iris_val</span><span class="p">,</span>
                                        <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">nite</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_tr</span><span class="p">)),</span> <span class="n">losses_tr</span><span class="p">,</span> <span class="s2">&quot;-b&quot;</span><span class="p">,</span>
             <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">losses_val</span><span class="p">)),</span> <span class="n">losses_val</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Backpropagation</a><ul>
<li><a class="reference internal" href="#backpropagation-and-chain-rule">Backpropagation and chain rule</a><ul>
<li><a class="reference internal" href="#recap-vector-derivatives">Recap: Vector derivatives</a></li>
<li><a class="reference internal" href="#backpropagation-summary">Backpropagation summary</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hands-on-with-numpy-and-pytorch">Hands-on with Numpy and pytorch</a><ul>
<li><a class="reference internal" href="#load-iris-data-set">Load iris data set</a></li>
<li><a class="reference internal" href="#backpropagation-with-numpy">Backpropagation with Numpy</a></li>
<li><a class="reference internal" href="#backpropagation-with-pytorch-tensors">Backpropagation with PyTorch Tensors</a></li>
<li><a class="reference internal" href="#backpropagation-with-pytorch-tensors-and-autograd">Backpropagation with PyTorch: Tensors and autograd</a></li>
<li><a class="reference internal" href="#backpropagation-with-pytorch-nn">Backpropagation with PyTorch: nn</a></li>
<li><a class="reference internal" href="#backpropagation-with-pytorch-optim">Backpropagation with PyTorch optim</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/deep_learning/dl_backprop_numpy-pytorch-sklearn.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/deep_learning/dl_backprop_numpy-pytorch-sklearn.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>