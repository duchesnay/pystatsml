{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretraining and Transfer Learning\n",
    "\n",
    "Sources [Transfer Learning cs231n @ Stanford](https://cs231n.github.io/transfer-learning/):\n",
    "_In practice, very few people train an entire Convolutional Network\n",
    "from scratch (with random initialization), because it is relatively\n",
    "rare to have a dataset of sufficient size. Instead, it is common to\n",
    "pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
    "contains 1.2 million images with 1000 categories), and then use the\n",
    "ConvNet either as an initialization or a fixed feature extractor for\n",
    "the task of interest._\n",
    "\n",
    "These two major transfer learning scenarios look as follows:\n",
    "\n",
    "1. **CNN as fixed feature extractor**:\n",
    "    * Take a CNN pretrained on ImageNet\n",
    "    * Remove the last fully-connected layer (this layerâ€™s outputs are the 1000 class scores for a different task like ImageNet).\n",
    "    * Treat the rest of the CNN as a fixed feature extractor for the new dataset.\n",
    "    * This last fully connected layer is replaced with a new one with random weights and only this layer is trained:\n",
    "    * Freeze the weights for all of the network except that of the final fully connected layer. \n",
    "\n",
    "2. **Fine-tuning all the layers of the CNN**:\n",
    "    * Same procedure, but do not freeze the weights of the CNN, by continuing the backpropagation on the new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # Force CPU\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "\n",
    "See [train_val_model](https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/dl_utils.py) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml.dl_utils import train_val_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: CIFAR-10 dataset with 10 classes\n",
    "\n",
    "Load CIFAR-10 dataset [CIFAR-10 Loader](https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/datasets.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml.datasets import load_cifar10_pytorch\n",
    "\n",
    "dataloaders, _ = load_cifar10_pytorch(\n",
    "    batch_size_train=100, batch_size_test=100)\n",
    "\n",
    "# Info about the dataset\n",
    "D_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\n",
    "D_out = len(set(dataloaders[\"train\"].dataset.targets))\n",
    "print(\"Datasets shape:\", {\n",
    "      x: dataloaders[x].dataset.data.shape for x in dataloaders.keys()})\n",
    "print(\"N input features:\", D_in, \"N output:\", D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning the convnet\n",
    "\n",
    "- Load a pretrained model and reset final fully connected layer.\n",
    "- SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model_ft = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 10.\n",
    "model_ft.fc = nn.Linear(num_ftrs, D_out)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model, losses, accuracies = \\\n",
    "    train_val_model(model_ft, criterion, optimizer_ft,\n",
    "                    dataloaders, scheduler=exp_lr_scheduler, num_epochs=5,\n",
    "                    log_interval=5)\n",
    "\n",
    "epochs = np.arange(len(losses['train']))\n",
    "_ = plt.plot(epochs, losses['train'], '-b', epochs, losses['test'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# model_ft = models.resnet18(pretrained=True)\n",
    "num_ftrs = model_ft.fc.in_features\n",
    "# Here the size of each output sample is set to 10.\n",
    "model_ft.fc = nn.Linear(num_ftrs, D_out)\n",
    "\n",
    "model_ft = model_ft.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer_ft = torch.optim.Adam(model_ft.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "\n",
    "model, losses, accuracies = \\\n",
    "    train_val_model(model_ft, criterion, optimizer_ft,\n",
    "                    dataloaders, scheduler=exp_lr_scheduler, num_epochs=5,\n",
    "                    log_interval=5)\n",
    "\n",
    "epochs = np.arange(len(losses['train']))\n",
    "_ = plt.plot(epochs, losses['train'], '-b', epochs, losses['test'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet as a feature extractor\n",
    "\n",
    "Freeze all the network except the final layer: `requires_grad == False` to freeze the parameters so that the gradients are not computed in `backward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, D_out)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "model, losses, accuracies = \\\n",
    "    train_val_model(model_conv, criterion, optimizer_conv,\n",
    "                    dataloaders, scheduler=exp_lr_scheduler, num_epochs=5,\n",
    "                    log_interval=5)\n",
    "\n",
    "epochs = np.arange(len(losses['train']))\n",
    "_ = plt.plot(epochs, losses['train'], '-b', epochs, losses['test'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "# model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, D_out)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opposed to before.\n",
    "optimizer_conv = optim.Adam(model_conv.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n",
    "\n",
    "model, losses, accuracies = \\\n",
    "    train_val_model(model_conv, criterion, optimizer_conv,\n",
    "                    dataloaders, scheduler=exp_lr_scheduler, num_epochs=5,\n",
    "                    log_interval=5)\n",
    "\n",
    "epochs = np.arange(len(losses['train']))\n",
    "_ = plt.plot(epochs, losses['train'], '-b', epochs, losses['test'], '--r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
