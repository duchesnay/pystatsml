{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "![Principles of CNNs](./figures/cnn.png){width=15cm}\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [3Blue1Brown video: Convolutions in Image Processing](https://www.youtube.com/watch?v=8rrHTtUzyZA&list=PLZHQObOWTQDMp_VZelDYjka8tnXNpXhzJ)\n",
    "- [far1din video: Convolutional Neural Networks from Scratch](https://www.youtube.com/watch?v=jDe5BAsT2-Y)\n",
    "- [What is a Convolutional Neural Network?](https://poloclub.github.io/cnn-explainer/).\n",
    "- CNN [Stanford cs231n](http://cs231n.github.io/convolutional-networks/)\n",
    "- Deep learning [Stanford cs231n](http://cs231n.stanford.edu/)\n",
    "- Pytorch\n",
    "    - [WWW tutorials](https://pytorch.org/tutorials/)\n",
    "    - [github tutorials](https://github.com/pytorch/tutorials)\n",
    "    - [github examples](https://github.com/pytorch/examples)\n",
    "- MNIST and pytorch:\n",
    "    - [MNIST nextjournal.com/gkoehler/pytorch-mnist](https://nextjournal.com/gkoehler/pytorch-mnist)\n",
    "    - [MNIST github/pytorch/examples](https://github.com/pytorch/examples/tree/master/mnist)\n",
    "    - [MNIST kaggle](https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist)\n",
    "\n",
    "## Introduction to CNNs\n",
    "\n",
    "CNNs are deep learning architectures designed for processing grid-like data such as images. Inspired by the biological visual cortex, they learn hierarchical feature representations, making them effective for tasks like image classification, object detection, and segmentation.\n",
    "\n",
    "\n",
    "Key Principles of CNNs:\n",
    "\n",
    "- **Convolutional Layers** are the core building block of a CNN, which applies a convolution operation to the input, passing the result to the next layer: it perform feature extraction using learnable filters (kernels), allowing CNNs to detect local patterns such as edges and textures.\n",
    "\n",
    "- **Activation Functions** introduce non-linearity into the model, enabling the network to learn complex patterns. ReLU (Rectified Linear Unit) is the most commonly used activation function, improving training speed and mitigating vanishing gradients. Possible function are Tanh or Sigmoid and most commonly used the **ReLu(Rectified Linear Unit** function. ReLu accelerate the training because the derivative of sigmoid becomes very small in the saturating region and therefore the updates to the weights almost vanish. This is called **vanishing gradient problem**..\n",
    "\n",
    "- **Pooling Layers** reduces the spatial dimensions (height and width) of the input feature maps by downsampling the input feature maps summarizing the presence of features in patches of the feature map. Max pooling and average pooling are the most common functions.\n",
    "\n",
    "- **Fully Connected Layers** flatten extracted features and connects to a classifier, typically a softmax layer for classification tasks.\n",
    "\n",
    "- **Dropout**: reduces the over-fitting by using a Dropout layer after every FC layer. Dropout layer has a probability,(p), associated with it and is applied at every neuron of the response map separately. It randomly switches off the activation with the probability p.\n",
    "\n",
    "- **Batch Normalization**  normalizes the inputs of each layer to have a mean of zero and a variance of one, which improve network stability. This normalization is performed for each mini-batch during training.\n",
    "\n",
    "\n",
    "## CNN Architectures: Evolution from LeNet to ResNet\n",
    "\n",
    "### LeNet-5 (1998)\n",
    "\n",
    "First successful CNN for handwritten digit recognition.\n",
    "\n",
    "![LeNet](./figures/LeNet_Original_Image.jpg)\n",
    "\n",
    "### AlexNet (2012)\n",
    "\n",
    "Revolutionized deep learning by winning the ImageNet competition. Introduced ReLU activation, dropout, and GPU acceleration. Featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).\n",
    "\n",
    "![AlexNet](./figures/alexnet.png){width=7cm}\n",
    "![AlexNet architecture](./figures/alexnet_param_tab.png){width=7cm}\n",
    "\n",
    "### VGG (2014)\n",
    "\n",
    "Introduced a simple yet deep architecture with 3Ã—3 convolutions.\n",
    "\n",
    "![VGGNet](./figures/vgg.png){width=7cm}\n",
    "![VGGNet architecture](./figures/vgg_param_tab.png){width=7cm}\n",
    "\n",
    "\n",
    "### GoogLeNet (Inception) (2014)\n",
    "Introduced the **Inception module**, using multiple kernel sizes in parallel.\n",
    "\n",
    "\n",
    "### ResNet (2015)\n",
    "Introduced **skip connections**, allowing training of very deep networks.\n",
    "\n",
    "![ResNet block](./figures/resnets_modelvariants.png){width=10cm}\n",
    "\n",
    "![ResNet 18](./figures/resnet18.png){width=7cm}\n",
    "![ResNet 18 architecture](./figures/resnet_param_tab.png){width=10cm}\n",
    "\n",
    "### Architectures general guidelines\n",
    "\n",
    "- ConvNets stack CONV,POOL,FC layers\n",
    "- Trend towards smaller filters and deeper architectures: stack 3x3, instead of 5x5\n",
    "- Trend towards getting rid of POOL/FC layers (just CONV)\n",
    "- Historically architectures looked like [(CONV-RELU) x N POOL?] x M (FC-RELU) x K, SOFTMAX where N is usually up to ~5, M is large, 0 <= K <= 2.\n",
    "- But recent advances such as ResNet/GoogLeNet have challenged this paradigm\n",
    "\n",
    "### Conclusion and Further Topics\n",
    "\n",
    "- **Recent architectures:** EfficientNet, Vision Transformers (ViTs), MobileNet for edge devices.\n",
    "- **Advanced topics:** Transfer learning, object detection (YOLO, Faster R-CNN), segmentation (U-Net).\n",
    "- **Hands-on implementation:** Implement CNNs using TensorFlow/PyTorch for real-world applications.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# ML\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu' # Force CPU\n",
    "# print(device)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See [train_val_model](https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/dl_utils.py) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml.dl_utils import train_val_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN in PyTorch\n",
    "\n",
    "### LeNet-5\n",
    "\n",
    "Here we implement LeNet-5 with relu activation. Sources:\n",
    "\n",
    "- [Github: LeNet-5-PyTorch](https://github.com/bollakarthikeya/LeNet-5-PyTorch/blob/master/lenet5_cpu.py),\n",
    "- [Kaggle: lenet with pytorch](https://www.kaggle.com/usingtc/lenet-with-pytorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    layers: (nb channels in input layer, \n",
    "             nb channels in 1rst conv,\n",
    "             nb channels in 2nd conv,\n",
    "             nb neurons for 1rst FC: TO BE TUNED,\n",
    "             nb neurons for 2nd FC,\n",
    "             nb neurons for 3rd FC,\n",
    "             nb neurons output FC TO BE TUNED)\n",
    "    \"\"\"\n",
    "    def __init__(self, layers = (1, 6, 16, 1024, 120, 84, 10), debug=False):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.debug = debug\n",
    "        self.conv1 = nn.Conv2d(layers[0], layers[1], 5, padding=2) \n",
    "        self.conv2 = nn.Conv2d(layers[1], layers[2], 5)\n",
    "        self.fc1   = nn.Linear(layers[3], layers[4])\n",
    "        self.fc2   = nn.Linear(layers[4], layers[5])\n",
    "        self.fc3   = nn.Linear(layers[5], layers[6])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2) # same shape / 2\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # -4 / 2\n",
    "        if self.debug:\n",
    "            print(\"### DEBUG: Shape of last convnet=\",\n",
    "                  x.shape[1:], \". FC size=\", np.prod(x.shape[1:]))\n",
    "        x = x.view(-1, self.layers[3])            \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGGNet like: conv-relu blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network (LeNet-5)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MiniVGGNet(torch.nn.Module):\n",
    "     \n",
    "    def __init__(self, layers=(1, 16, 32, 1024, 120, 84, 10), debug=False):   \n",
    "        super(MiniVGGNet, self).__init__()\n",
    "        self.layers = layers\n",
    "        self.debug = debug\n",
    "\n",
    "        # Conv block 1\n",
    "        self.conv11 = nn.Conv2d(in_channels=layers[0],out_channels=layers[1],\n",
    "                                kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.conv12 = nn.Conv2d(in_channels=layers[1], out_channels=layers[1],\n",
    "                                kernel_size=3, stride=1, padding=0, bias=True)\n",
    "\n",
    "        # Conv block 2\n",
    "        self.conv21 = nn.Conv2d(in_channels=layers[1], out_channels=layers[2],\n",
    "                                kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.conv22 = nn.Conv2d(in_channels=layers[2], out_channels=layers[2],\n",
    "                                kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc1   = nn.Linear(layers[3], layers[4])\n",
    "        self.fc2   = nn.Linear(layers[4], layers[5])\n",
    "        self.fc3   = nn.Linear(layers[5], layers[6])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv11(x))\n",
    "        x = F.relu(self.conv12(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = F.relu(self.conv21(x))\n",
    "        x = F.relu(self.conv22(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "    \n",
    "        if self.debug:\n",
    "            print(\"### DEBUG: Shape of last convnet=\", x.shape[1:],\n",
    "                  \". FC size=\", np.prod(x.shape[1:]))\n",
    "        x = x.view(-1, self.layers[3])\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet-like Model\n",
    "\n",
    "Stack multiple resnet blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------- #\n",
    "# An implementation of https://arxiv.org/pdf/1512.03385.pdf                    #\n",
    "# See section 4.2 for the model architecture on CIFAR-10                       #\n",
    "# Some part of the code was referenced from below                              #\n",
    "# https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py   #\n",
    "# ---------------------------------------------------------------------------- #\n",
    "import torch.nn as nn\n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if (stride != 1) or (self.in_channels != out_channels):\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "        #return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet9\n",
    "\n",
    "Sources:\n",
    " \n",
    "- [DAWNBench on cifar10](https://dawn.cs.stanford.edu/benchmark/index.html#cifar10)\n",
    "- [ResNet9: train to 94% CIFAR10 accuracy in 100 seconds](https://lambdalabs.com/blog/resnet9-train-to-94-cifar10-accuracy-in-100-seconds/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: MNIST digits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST Loader](https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/datasets.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml.datasets import load_mnist_pytorch\n",
    "\n",
    "dataloaders, WD = load_mnist_pytorch(\n",
    "    batch_size_train=64, batch_size_test=10000)\n",
    "os.makedirs(os.path.join(WD, \"models\"), exist_ok=True)\n",
    "\n",
    "# Info about the dataset\n",
    "D_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\n",
    "D_out = len(dataloaders[\"train\"].dataset.targets.unique())\n",
    "print(\"Datasets shapes:\", {\n",
    "      x: dataloaders[x].dataset.data.shape for x in ['train', 'test']})\n",
    "print(\"N input features:\", D_in, \"Output classes:\", D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dry run in debug mode to get the shape of the last convnet layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((1, 6, 16, 1, 120, 84, 10), debug=True)\n",
    "batch_idx, (data_example, target_example) = next(\n",
    "    enumerate(dataloaders[\"train\"]))\n",
    "print(model)\n",
    "_ = model(data_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set First FC layer to 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((1, 6, 16, 400, 120, 84, 10)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\", np.sum([np.prod(parameter.shape) for\n",
    "                                              parameter in model.parameters()]))\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=5, log_interval=2)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGGNet(layers=(1, 16, 32, 1, 120, 84, 10), debug=True)\n",
    "\n",
    "print(model)\n",
    "_ = model(data_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set First FC layer to 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGGNet((1, 16, 32, 800, 120, 84, 10)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\",\n",
    "      np.sum([np.prod(parameter.shape)\n",
    "              for parameter in model.parameters()]))\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=5, log_interval=2)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce the size of training dataset\n",
    "\n",
    "Reduce the size of the training dataset by considering only `10` minibatche for size`16`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = dataloaders[\"train\"], dataloaders[\"test\"]\n",
    "\n",
    "train_size = 10 * 16\n",
    "\n",
    "# Stratified sub-sampling\n",
    "targets = train_loader.dataset.targets.numpy()\n",
    "nclasses = len(set(targets))\n",
    "\n",
    "indices = np.concatenate([np.random.choice(np.where(targets == lab)[0],\n",
    "                                           int(train_size / nclasses),\n",
    "                                           replace=False)\n",
    "                          for lab in set(targets)])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_loader.dataset, batch_size=16,\n",
    "                        sampler=torch.utils.data.SubsetRandomSampler(indices))\n",
    "\n",
    "# Check train subsampling\n",
    "train_labels = np.concatenate([labels.numpy()\n",
    "                              for inputs, labels in train_loader])\n",
    "print(\"Train size=\", len(train_labels), \" Train label count=\",\n",
    "      {lab: np.sum(train_labels == lab) for lab in set(train_labels)})\n",
    "print(\"Batch sizes=\", [inputs.size(0) for inputs, labels in train_loader])\n",
    "\n",
    "# Put together train and val\n",
    "dataloaders = dict(train=train_loader, val=val_loader)\n",
    "\n",
    "# Info about the dataset\n",
    "data_shape = dataloaders[\"train\"].dataset.data.shape[1:]\n",
    "D_in = np.prod(data_shape)\n",
    "D_out = len(dataloaders[\"train\"].dataset.targets.unique())\n",
    "print(\"Datasets shape\", {x: dataloaders[x].dataset.data.shape\n",
    "                         for x in ['train', 'val']})\n",
    "print(\"N input features\", D_in, \"N output\", D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((1, 6, 16, 400, 120, 84, D_out)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=100, log_interval=20)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGGNet((1, 16, 32, 800, 120, 84, 10)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=100, log_interval=20)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: CIFAR-10 dataset with 10 classes\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class.\n",
    "\n",
    "[Source Yunjey Choi Github pytorch tutorial](https://github.com/yunjey/pytorch-tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR-10 dataset [CIFAR-10 Loader](https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/datasets.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml.datasets import load_cifar10_pytorch\n",
    "\n",
    "dataloaders, _ = load_cifar10_pytorch(\n",
    "    batch_size_train=100, batch_size_test=100)\n",
    "\n",
    "# Info about the dataset\n",
    "D_in = np.prod(dataloaders[\"train\"].dataset.data.shape[1:])\n",
    "D_out = len(set(dataloaders[\"train\"].dataset.targets))\n",
    "print(\"Datasets shape:\", {\n",
    "      x: dataloaders[x].dataset.data.shape for x in dataloaders.keys()})\n",
    "print(\"N input features:\", D_in, \"N output:\", D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((3, 6, 16, 1, 120, 84, D_out), debug=True)\n",
    "batch_idx, (data_example, target_example) = next(enumerate(train_loader))\n",
    "print(model)\n",
    "_ = model(data_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set 576 neurons to the first FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD with momentum `lr=0.001, momentum=0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((3, 6, 16, 576, 120, 84, D_out)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Explore the model\n",
    "for parameter in model.parameters():\n",
    "    print(parameter.shape)\n",
    "\n",
    "print(\"Total number of parameters =\",\n",
    "      np.sum([np.prod(parameter.shape)\n",
    "              for parameter in model.parameters()]))\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=25, log_interval=5)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase learning rate and momentum `lr=0.01, momentum=0.9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((3, 6, 16, 576, 120, 84, D_out)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=25, log_interval=5)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptative learning rate: Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet5((3, 6, 16, 576, 120, 84, D_out)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=25, log_interval=5)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MiniVGGNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGGNet(layers=(3, 16, 32, 1, 120, 84, D_out), debug=True)\n",
    "print(model)\n",
    "_ = model(data_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set 1152 neurons to the first FC layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD with large momentum and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGGNet((3, 16, 32, 1152, 120, 84, D_out)).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = train_val_model(model, criterion, optimizer,\n",
    "                                            dataloaders,\n",
    "                                            num_epochs=25, log_interval=5)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniVGGNet((3, 16, 32, 1152, 120, 84, D_out)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = \\\n",
    "    train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                    num_epochs=25, log_interval=5)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet(ResidualBlock, [2, 2, 2], num_classes=D_out).to(device)\n",
    "# 195738 parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "model, losses, accuracies = \\\n",
    "    train_val_model(model, criterion, optimizer, dataloaders,\n",
    "                    num_epochs=25, log_interval=5)\n",
    "\n",
    "_ = plt.plot(losses['train'], '-b', losses['val'], '--r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation with U-Net\n",
    "\n",
    "Source [Segmentation Models](https://github.com/qubvel-org/segmentation_models.pytorch):\n",
    "\n",
    "U-Net is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "It consists of two main parts:\n",
    "\n",
    "- An encoder (downsampling path) that extracts increasingly abstract features\n",
    "- A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "The key is the use of skip connections between corresponding encoder and decoder layers. These connections allow the decoder to access fine-grained details from earlier encoder layers, which helps produce more precise segmentation masks.\n",
    "\n",
    "The skip connections work by concatenating feature maps from the encoder directly into the decoder at corresponding resolutions. This helps preserve important spatial information that would otherwise be lost during the encoding process.\n",
    "\n",
    "### Example: Image Segmentation with U-Net using PyTorch\n",
    "\n",
    "Below is an example of how to implement image segmentation using the U-Net architecture with PyTorch on a real dataset. We will use the Oxford-IIIT Pet Dataset for this example.\n",
    "\n",
    "Step1: Load the Dataset\n",
    "\n",
    "We will use the Oxford-IIIT Pet Dataset, which can be downloaded from [here](https://www.robots.ox.ac.uk/~vgg/data/pets/). For simplicity, we will assume the dataset is already downloaded and structured as follows:\n",
    "\n",
    "Step 2: Define the U-Net Model\n",
    "\n",
    "Here is the implementation of the U-Net model in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels,\n",
    "                          kernel_size=3, padding=1),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        def up_conv(in_channels, out_channels):\n",
    "            return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2,\n",
    "                                      stride=2)\n",
    "\n",
    "        self.enc1 = conv_block(in_channels, 64)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        self.upconv4 = up_conv(1024, 512)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.upconv3 = up_conv(512, 256)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.upconv2 = up_conv(256, 128)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.upconv1 = up_conv(128, 64)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.conv_final = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(self.pool(enc1))\n",
    "        enc3 = self.enc3(self.pool(enc2))\n",
    "        enc4 = self.enc4(self.pool(enc3))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool(enc4))\n",
    "\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat((dec4, enc4), dim=1)\n",
    "        dec4 = self.dec4(dec4)\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat((dec3, enc3), dim=1)\n",
    "        dec3 = self.dec3(dec3)\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.dec2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.dec1(dec1)\n",
    "\n",
    "        return self.conv_final(dec1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load and Preprocess the Dataset\n",
    "\n",
    "We use the torchvision library to load and preprocess the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Directory\n",
    "WD = \"/home/ed203246/data/pystatml/dl_Oxford-IIITPet\"\n",
    "# <Directory>/images: input images\n",
    "# <Directory>/annotations: corresponding masks\n",
    "\n",
    "\n",
    "class PetDataset(Dataset):\n",
    "    def __init__(self, image_dir, mask_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.transform = transform\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        mask_path = os.path.join(self.mask_dir,\n",
    "                                 self.image_filenames[idx].replace('.jpg', '.png'))\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = PetDataset(os.path.join(WD, 'images'),\n",
    "                     os.path.join(WD, 'annotations'), transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Train the U-Net Model\n",
    "\n",
    "Finally, we will train the U-Net model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=1)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, dataloader, criterion, optimizer, num_epochs=1):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for images, masks in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Do not executer (takes 2H)\n",
    "# train(model, dataloader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "def visualize_results(model, dataloader, num_images=9):\n",
    "    model.eval()\n",
    "    images, masks = next(iter(dataloader))\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        outputs = outputs.cpu().numpy()\n",
    "\n",
    "    images = images.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(10, 30))\n",
    "    for i in range(num_images):\n",
    "        axes[i, 0].imshow(np.transpose(images[i], (1, 2, 0)))\n",
    "        axes[i, 0].set_title('Input Image')\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        axes[i, 1].imshow(masks[i].squeeze(), cmap='gray')\n",
    "        axes[i, 1].set_title('Ground Truth Mask')\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        axes[i, 2].imshow(outputs[i].squeeze(), cmap='gray')\n",
    "        axes[i, 2].set_title('Predicted Mask')\n",
    "        axes[i, 2].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_results(model, dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U-Net: Training Image Segmentation Models in PyTorch\n",
    "\n",
    "#### [A simple pytorch implementation of U-net](https://github.com/clemkoa/u-net)\n",
    "\n",
    "- [The model](https://github.com/clemkoa/u-net/blob/master/unet/unet.py)\n",
    "- [Train with predefined dataset and dataloader](https://github.com/clemkoa/u-net/blob/master/train.py)\n",
    "\n",
    "#### [PyTorch - Lung Segmentation using pretrained U-net](https://www.kaggle.com/code/vatsalmavani/pytorch-lung-segmentation-using-pretrained-u-net)\n",
    "\n",
    "- UNet architecture with pre-trained ResNet34 from [segmentation_models.pytorch](https://github.com/qubvel/segmentation_models.pytorch) library which has many inbuilt segmentation architectures with different backbones.\n",
    "- Identify \"Pneumothorax\" or a collapsed lung from chest x-rays. \n",
    "- Data Augmentation\n",
    "- Train-val Dataset and DataLoader\n",
    "- User defined Loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
