<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Multilayer Perceptron (MLP) &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Convolutional Neural Networks (CNNs)" href="dl_cnn_cifar10_pytorch.html" />
    <link rel="prev" title="Backpropagation" href="dl_backprop_numpy-pytorch-sklearn.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="multilayer-perceptron-mlp">
<h1>Multilayer Perceptron (MLP)<a class="headerlink" href="#multilayer-perceptron-mlp" title="Link to this heading">¶</a></h1>
<p>Sources:</p>
<p>Sources:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&amp;index=1">3Blue1Brown video: But what is a neural network? | Deep learning
chapter
1</a></p></li>
<li><p><a class="reference external" href="http://cs231n.stanford.edu/">Stanford cs231n: Deep learning</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/">Pytorch: WWW tutorials</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/tutorials">Pytorch: github tutorials</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/examples">Pytorch: github examples</a></p></li>
<li><p><a class="reference external" href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html">Pytorch
examples</a></p></li>
<li><p><a class="reference external" href="https://nextjournal.com/gkoehler/pytorch-mnist">MNIST/pytorch
nextjournal.com/gkoehler/pytorch-mnist</a></p></li>
<li><p><a class="reference external" href="https://github.com/pytorch/examples/tree/master/mnist">Pytorch:
github/pytorch/examples</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/sdelecourt/cnn-with-pytorch-for-mnist">kaggle:
MNIST/pytorch</a></p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn.functional</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">F</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">optim</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">lr_scheduler</span>
<span class="c1"># import torchvision</span>
<span class="c1"># from torchvision import transforms</span>
<span class="c1"># from torchvision import datasets</span>
<span class="c1"># from torchvision import models</span>
<span class="c1">#</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="c1"># Plot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Plot parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">)</span>

<span class="c1"># Device configuration</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda:0&#39;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s1">&#39;cpu&#39;</span><span class="p">)</span>
<span class="c1"># device = &#39;cpu&#39; # Force CPU</span>
<span class="nb">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cpu</span>
</pre></div>
</div>
<section id="single-layer-softmax-classifier-multinomial-logistic-regression">
<h2>Single Layer Softmax Classifier (Multinomial Logistic Regression)<a class="headerlink" href="#single-layer-softmax-classifier-multinomial-logistic-regression" title="Link to this heading">¶</a></h2>
<p>Recall of Binary logistic regression</p>
<p><a class="reference internal" href="../_images/logistic.png"><img alt="Binary logistic regression" src="../_images/logistic.png" style="width: 7cm;" /></a> <a class="reference internal" href="../_images/logistic_multinominal.png"><img alt="Multinomial Logistic Regression" src="../_images/logistic_multinominal.png" style="width: 7cm;" /></a></p>
<p>One neuron as output layer</p>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{x}) = \sigma(\boldsymbol{x}^{T} \boldsymbol{w} + b)\]</div>
<p>Where</p>
<ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>: a vector of dimension <span class="math notranslate nohighlight">\((p)\)</span>
(layer 0).</p></li>
<li><p>Parameters: <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>: a vector of dimension <span class="math notranslate nohighlight">\((p)\)</span>
(layer 1). <span class="math notranslate nohighlight">\(b\)</span> is the scalar bias.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\(f(\boldsymbol{x})\)</span> a vector of dimension 1.</p></li>
</ul>
<p>With multinomial logistic regression we have <span class="math notranslate nohighlight">\(k\)</span> possible labels
to predict. If we consider the MNIST Handwritten Digit Recognition, the
inputs is a <span class="math notranslate nohighlight">\(28 \times 28=784\)</span> image and the output is a vector of
<span class="math notranslate nohighlight">\(k=10\)</span> labels or probabilities.</p>
<figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/logistic_multinominal_MNIST.png"><img alt="Multinomial Logistic Regression on MINST" src="../_images/logistic_multinominal_MNIST.png" style="width: 15cm;" />
</a>
<figcaption>
<p><span class="caption-text">Multinomial Logistic Regression on MINST</span><a class="headerlink" href="#id1" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="math notranslate nohighlight">
\[f(\boldsymbol{x}) = \text{softmax}(\boldsymbol{x}^{T} \boldsymbol{W} + \boldsymbol{b})\]</div>
<ul class="simple">
<li><p>Input: <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>: a vector of dimension <span class="math notranslate nohighlight">\((p=784)\)</span>
(layer 0).</p></li>
<li><p>Parameters: <span class="math notranslate nohighlight">\(\boldsymbol{W}\)</span>: the matrix of coefficients of
dimension <span class="math notranslate nohighlight">\((p \times k)\)</span> (layer 1). <span class="math notranslate nohighlight">\(b\)</span> is a
<span class="math notranslate nohighlight">\((k)\)</span>-dimentional vector of bias.</p></li>
<li><p>Output: <span class="math notranslate nohighlight">\(f(\boldsymbol{x})\)</span> a vector of dimension <span class="math notranslate nohighlight">\((k=10)\)</span>
possible labels</p></li>
</ul>
<p>The softmax function is a crucial component in many machine learning and
deep learning models, particularly in the context of classification
tasks. It is used to convert a vector of raw scores (logits) into a
probability distribution. Here’s a detailed explanation of the softmax
function: The softmax function takes a vector of real numbers as input
and outputs a vector of probabilities that sum to 1. The formula for the
softmax function is:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}\]</div>
<p>where: - <span class="math notranslate nohighlight">\(z_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>-th element of the input vector
<span class="math notranslate nohighlight">\(\mathbf{z}\)</span>. - <span class="math notranslate nohighlight">\(e\)</span> is the base of the natural logarithm. -
The sum in the denominator is over all elements of the input vector.</p>
<p>Softmax Properties</p>
<ol class="arabic simple">
<li><p><strong>Probability Distribution</strong>: The output of the softmax function is a
probability distribution, meaning that all the outputs are
non-negative and sum to 1.</p></li>
<li><p><strong>Exponential Function</strong>: The use of the exponential function ensures
that the outputs are positive and that larger input values correspond
to larger probabilities.</p></li>
<li><p><strong>Normalization</strong>: The softmax function normalizes the input values
by dividing by the sum of the exponentials of all input values,
ensuring that the outputs sum to 1</p></li>
</ol>
<p>MNIST classfification using multinomial logistic</p>
<p><a class="reference external" href="https://notebooks.azure.com/cntk/projects/edxdle/html/Lab2_LogisticRegression.ipynb">source: Logistic regression
MNIST</a></p>
<p>Here we fit a multinomial logistic regression with L2 penalty on a
subset of the MNIST digits classification task.</p>
<p><a class="reference external" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html">source:
scikit-learn.org</a></p>
<p>Hyperparameters</p>
</section>
<section id="dataset-mnist-handwritten-digit-recognition">
<h2>Dataset: MNIST Handwritten Digit Recognition<a class="headerlink" href="#dataset-mnist-handwritten-digit-recognition" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/datasets.py">MNIST
Loader</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pystatsml.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_mnist_pytorch</span>

<span class="n">dataloaders</span><span class="p">,</span> <span class="n">WD</span> <span class="o">=</span> <span class="n">load_mnist_pytorch</span><span class="p">(</span>
    <span class="n">batch_size_train</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">batch_size_test</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">WD</span><span class="p">,</span> <span class="s2">&quot;models&quot;</span><span class="p">),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Info about the dataset</span>
<span class="n">D_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">D_out</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Datasets shapes:&quot;</span><span class="p">,</span> <span class="p">{</span>
      <span class="n">x</span><span class="p">:</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">,</span> <span class="s1">&#39;test&#39;</span><span class="p">]})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;N input features:&quot;</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="s2">&quot;Output classes:&quot;</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">ed203246</span><span class="o">/</span><span class="n">data</span><span class="o">/</span><span class="n">pystatml</span><span class="o">/</span><span class="n">dl_mnist_pytorch</span>
<span class="n">Datasets</span> <span class="n">shapes</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])}</span>
<span class="n">N</span> <span class="nb">input</span> <span class="n">features</span><span class="p">:</span> <span class="mi">784</span> <span class="n">Output</span> <span class="n">classes</span><span class="p">:</span> <span class="mi">10</span>
</pre></div>
</div>
<p>Now let’s take a look at some mini-batches examples.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">example_data</span><span class="p">,</span> <span class="n">example_targets</span><span class="p">)</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
    <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train batch:&quot;</span><span class="p">,</span> <span class="n">example_data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">example_targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">example_data</span><span class="p">,</span> <span class="n">example_targets</span><span class="p">)</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
    <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Val batch:&quot;</span><span class="p">,</span> <span class="n">example_data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">example_targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Train</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">64</span><span class="p">])</span>
<span class="n">Val</span> <span class="n">batch</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10000</span><span class="p">])</span>
</pre></div>
</div>
<p>So one test data batch is a tensor of shape: . This means we have 1000
examples of 28x28 pixels in grayscale (i.e. no rgb channels, hence the
one). We can plot some of them using matplotlib.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">show_data_label_prediction</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span> <span class="k">if</span> <span class="n">y_pred</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">y_pred</span>
    <span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">)):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="o">*</span><span class="n">shape</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;True: </span><span class="si">{}</span><span class="s2"> Pred: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">y_true</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>


<span class="n">show_data_label_prediction</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">example_data</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">example_targets</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_9_0.png" src="../_images/dl_mlp_pytorch_9_0.png" />
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span> <span class="p">(</span><span class="mi">60000</span><span class="p">,)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="c1"># from sklearn.model_selection import train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">check_random_state</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Turn up tolerance for faster convergence</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">50.</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;sag&#39;</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c1"># sparsity = np.mean(clf.coef_ == 0) * 100</span>
<span class="n">score</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test score with penalty: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">score</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Test</span> <span class="n">score</span> <span class="k">with</span> <span class="n">penalty</span><span class="p">:</span> <span class="mf">0.8997</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coef</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">scale</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">l1_plot</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">l1_plot</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">coef</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">interpolation</span><span class="o">=</span><span class="s1">&#39;nearest&#39;</span><span class="p">,</span>
                   <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="n">scale</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="n">scale</span><span class="p">)</span>
    <span class="n">l1_plot</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">l1_plot</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>
    <span class="n">l1_plot</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Class </span><span class="si">%i</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">i</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Classification vector for...&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_12_0.png" src="../_images/dl_mlp_pytorch_12_0.png" />
</section>
<section id="model-two-layer-mlp">
<h2>Model: Two Layer MLP<a class="headerlink" href="#model-two-layer-mlp" title="Link to this heading">¶</a></h2>
<section id="mlp-with-scikit-learn">
<h3>MLP with Scikit-learn<a class="headerlink" href="#mlp-with-scikit-learn" title="Link to this heading">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neural_network</span><span class="w"> </span><span class="kn">import</span> <span class="n">MLPClassifier</span>

<span class="n">mlp</span> <span class="o">=</span> <span class="n">MLPClassifier</span><span class="p">(</span><span class="n">hidden_layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="p">),</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span>
                    <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;sgd&#39;</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                    <span class="n">learning_rate_init</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>

<span class="n">mlp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training set score: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Test set score: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">mlp</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef shape=&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">coefs_</span><span class="p">))</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="c1"># use global min / max to ensure all weights are shown on the same scale</span>
<span class="n">vmin</span><span class="p">,</span> <span class="n">vmax</span> <span class="o">=</span> <span class="n">mlp</span><span class="o">.</span><span class="n">coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">mlp</span><span class="o">.</span><span class="n">coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
<span class="k">for</span> <span class="n">coef</span><span class="p">,</span> <span class="n">ax</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">mlp</span><span class="o">.</span><span class="n">coefs_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">()):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">coef</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">gray</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mf">.5</span> <span class="o">*</span> <span class="n">vmin</span><span class="p">,</span>
               <span class="n">vmax</span><span class="o">=</span><span class="mf">.5</span> <span class="o">*</span> <span class="n">vmax</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">(())</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Iteration</span> <span class="mi">1</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.28828673</span>
<span class="n">Iteration</span> <span class="mi">2</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.13388073</span>
<span class="n">Iteration</span> <span class="mi">3</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.09366379</span>
<span class="n">Iteration</span> <span class="mi">4</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.07317648</span>
<span class="n">Iteration</span> <span class="mi">5</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.05340251</span>
<span class="n">Iteration</span> <span class="mi">6</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.04468092</span>
<span class="n">Iteration</span> <span class="mi">7</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.03548097</span>
<span class="n">Iteration</span> <span class="mi">8</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.02862098</span>
<span class="n">Iteration</span> <span class="mi">9</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.02449230</span>
<span class="n">Iteration</span> <span class="mi">10</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.01874513</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">ed203246</span><span class="o">/</span><span class="n">git</span><span class="o">/</span><span class="n">pystatsml</span><span class="o">/.</span><span class="n">pixi</span><span class="o">/</span><span class="n">envs</span><span class="o">/</span><span class="n">default</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python3</span><span class="mf">.12</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">sklearn</span><span class="o">/</span><span class="n">neural_network</span><span class="o">/</span><span class="n">_multilayer_perceptron</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">690</span><span class="p">:</span> <span class="n">ConvergenceWarning</span><span class="p">:</span> <span class="n">Stochastic</span> <span class="n">Optimizer</span><span class="p">:</span> <span class="n">Maximum</span> <span class="n">iterations</span> <span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="n">reached</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">optimization</span> <span class="n">hasn</span><span class="s1">&#39;t converged yet.</span>
  <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Training</span> <span class="nb">set</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.997800</span>
<span class="n">Test</span> <span class="nb">set</span> <span class="n">score</span><span class="p">:</span> <span class="mf">0.974900</span>
<span class="n">Coef</span> <span class="n">shape</span><span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_14_3.png" src="../_images/dl_mlp_pytorch_14_3.png" />
</section>
<section id="mlp-with-pytorch">
<h3>MLP with pytorch<a class="headerlink" href="#mlp-with-pytorch" title="Link to this heading">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">TwoLayerMLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_in</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_out</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TwoLayerMLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_in</span> <span class="o">=</span> <span class="n">d_in</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_in</span><span class="p">,</span> <span class="n">d_hidden</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_hidden</span><span class="p">,</span> <span class="n">d_out</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_in</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<section id="train-the-model">
<h4>Train the Model<a class="headerlink" href="#train-the-model" title="Link to this heading">¶</a></h4>
<ul class="simple">
<li><p>First we want to make sure our network is in training mode.</p></li>
<li><p>Iterate over epochs</p></li>
<li><p>Alternate train and validation dataset</p></li>
<li><p>Iterate over all training/val data once per epoch. Loading the
individual batches is handled by the DataLoader.</p></li>
<li><p>Set the gradients to zero using <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code> since
PyTorch by default accumulates gradients.</p></li>
<li><p>Forward pass:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">model(inputs)</span></code>: Produce the output of our network.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">torch.max(outputs,</span> <span class="pre">1)</span></code>: softmax predictions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">criterion(outputs,</span> <span class="pre">labels)</span></code>: loss between the output and the
ground truth label.</p></li>
</ul>
</li>
<li><p>In training mode, backward pass <code class="docutils literal notranslate"><span class="pre">backward()</span></code>: collect a new set of
gradients which we propagate back into each of the network’s
parameters using <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p></li>
<li><p>We’ll also keep track of the progress with some printouts. In order to
create a nice training curve later on we also create two lists for
saving training and testing losses. On the x-axis we want to display
the number of training examples the network has seen during training.</p></li>
<li><p>Save model state: Neural network modules as well as optimizers have
the ability to save and load their internal state using
<code class="docutils literal notranslate"><span class="pre">.state_dict()</span></code>. With this we can continue training from previously
saved state dicts if needed - we’d just need to call
<code class="docutils literal notranslate"><span class="pre">.load_state_dict(state_dict)</span></code>.</p></li>
</ul>
<p>See
<a class="reference external" href="https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/dl_utils.py">train_val_model</a>
function.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pystatsml.dl_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_val_model</span>
</pre></div>
</div>
</section>
<section id="save-and-reload-pytorch-model">
<h4>Save and reload PyTorch model<a class="headerlink" href="#save-and-reload-pytorch-model" title="Link to this heading">¶</a></h4>
<p><a class="reference external" href="https://pytorch.org/tutorials/beginner/saving_loading_models.html">PyTorch doc: Save and reload PyTorch
model</a>:<em>Note
“If you only plan to keep the best performing model (according to the
acquired validation loss), don’t forget that best_model_state =
model.state_dict() returns a reference to the state and not its copy!
You must serialize best_model_state or use best_model_state =
deepcopy(model.state_dict()) otherwise your best best_model_state will
keep getting updated by the subsequent training iterations. As a result,
the final model state will be the state of the overfitted model.”</em></p>
<p>Save/Load state_dict (Recommended) Save:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()),</span> <span class="n">PATH</span><span class="p">)</span>
</pre></div>
</div>
<p>Load:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TheModelClass</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">,</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
</pre></div>
</div>
<p>Run one epoch and save the model</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">copy</span><span class="w"> </span><span class="kn">import</span> <span class="n">deepcopy</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerMLP</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="c1"># Explore the model</span>
<span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total number of parameters =&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
    <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">parameter</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="k">for</span> <span class="n">parameter</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">()]))</span>

<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> \
    <span class="n">train_val_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">.</span><span class="n">is_cuda</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()),</span>
           <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">WD</span><span class="p">,</span> <span class="s1">&#39;models/mod-</span><span class="si">%s</span><span class="s1">.pth&#39;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kc">False</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">50</span><span class="p">,</span> <span class="mi">784</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">50</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
<span class="n">Total</span> <span class="n">number</span> <span class="n">of</span> <span class="n">parameters</span> <span class="o">=</span> <span class="mi">39760</span>
<span class="n">Epoch</span> <span class="mi">0</span><span class="o">/</span><span class="mi">0</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.4500</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">87.61</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.3059</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">91.05</span><span class="o">%</span>

<span class="n">Training</span> <span class="n">complete</span> <span class="ow">in</span> <span class="mi">0</span><span class="n">m</span> <span class="mi">5</span><span class="n">s</span>
<span class="n">Best</span> <span class="n">val</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">91.05</span><span class="o">%</span>
<span class="kc">False</span>
</pre></div>
</div>
<p>Reload model</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_</span> <span class="o">=</span> <span class="n">TwoLayerMLP</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
<span class="n">model_</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">WD</span><span class="p">,</span> <span class="s1">&#39;models/mod-</span><span class="si">%s</span><span class="s1">.pth&#39;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
<span class="n">model_</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TwoLayerMLP</span><span class="p">(</span>
  <span class="p">(</span><span class="n">linear1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
  <span class="p">(</span><span class="n">linear2</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Use the model to make new predictions. Consider the device, ie, load
data on device <code class="docutils literal notranslate"><span class="pre">example_data.to(device)</span></code> from prediction, then move
back to cpu <code class="docutils literal notranslate"><span class="pre">example_data.cpu()</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">example_data</span><span class="p">,</span> <span class="n">example_targets</span><span class="p">)</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span>
    <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">]))</span>
<span class="n">example_data</span> <span class="o">=</span> <span class="n">example_data</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">example_data</span><span class="p">)</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="n">example_data</span> <span class="o">=</span> <span class="n">example_data</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="c1"># Softmax predictions</span>
<span class="n">preds</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output shape=&quot;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s2">&quot;label shape=&quot;</span><span class="p">,</span> <span class="n">preds</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy = </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="p">(</span><span class="n">example_targets</span> <span class="o">==</span> <span class="n">preds</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">*</span> <span class="mf">100.</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">example_targets</span><span class="p">)))</span>

<span class="n">show_data_label_prediction</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="n">example_data</span><span class="p">,</span> <span class="n">y_true</span><span class="o">=</span><span class="n">example_targets</span><span class="p">,</span> <span class="n">y_pred</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Output</span> <span class="n">shape</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span> <span class="n">label</span> <span class="n">shape</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10000</span><span class="p">])</span>
<span class="n">Accuracy</span> <span class="o">=</span> <span class="mf">91.05</span><span class="o">%</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_25_1.png" src="../_images/dl_mlp_pytorch_25_1.png" />
<p>Plot missclassified samples</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">errors</span> <span class="o">=</span> <span class="n">example_targets</span> <span class="o">!=</span> <span class="n">preds</span>
<span class="c1"># print(errors, np.where(errors))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors = </span><span class="si">{}</span><span class="s2">, (Error rate = </span><span class="si">{:.2f}</span><span class="s2">%)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
    <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">errors</span><span class="p">)))</span>
<span class="n">err_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">errors</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">show_data_label_prediction</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">example_data</span><span class="p">[</span><span class="n">err_idx</span><span class="p">],</span>
                           <span class="n">y_true</span><span class="o">=</span><span class="n">example_targets</span><span class="p">[</span><span class="n">err_idx</span><span class="p">],</span>
                           <span class="n">y_pred</span><span class="o">=</span><span class="n">preds</span><span class="p">[</span><span class="n">err_idx</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span> <span class="o">=</span> <span class="mi">895</span><span class="p">,</span> <span class="p">(</span><span class="n">Error</span> <span class="n">rate</span> <span class="o">=</span> <span class="mf">8.95</span><span class="o">%</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_27_1.png" src="../_images/dl_mlp_pytorch_27_1.png" />
</section>
<section id="continue-training-from-checkpoints-reload-the-model-and-run-10-more-epochs">
<h4>Continue training from checkpoints: reload the model and run 10 more epochs<a class="headerlink" href="#continue-training-from-checkpoints-reload-the-model-and-run-10-more-epochs" title="Link to this heading">¶</a></h4>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">TwoLayerMLP</span><span class="p">(</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
    <span class="n">WD</span><span class="p">,</span> <span class="s1">&#39;models/mod-</span><span class="si">%s</span><span class="s1">.pth&#39;</span> <span class="o">%</span> <span class="n">model</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">),</span> <span class="n">weights_only</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> \
    <span class="n">train_val_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="s1">&#39;-b&#39;</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="s1">&#39;--r&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.3097</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">91.11</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2904</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">91.91</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2844</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">91.94</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2809</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.10</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2752</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.21</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2747</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.23</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">6</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2688</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.45</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2747</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.17</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">8</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2650</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.64</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.2747</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.32</span><span class="o">%</span>

<span class="n">Training</span> <span class="n">complete</span> <span class="ow">in</span> <span class="mi">0</span><span class="n">m</span> <span class="mi">40</span><span class="n">s</span>
<span class="n">Best</span> <span class="n">val</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">92.32</span><span class="o">%</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_29_1.png" src="../_images/dl_mlp_pytorch_29_1.png" />
</section>
</section>
</section>
<section id="test-several-mlp-architectures">
<h2>Test several MLP architectures<a class="headerlink" href="#test-several-mlp-architectures" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Define a <code class="docutils literal notranslate"><span class="pre">MultiLayerMLP([D_in,</span> <span class="pre">512,</span> <span class="pre">256,</span> <span class="pre">128,</span> <span class="pre">64,</span> <span class="pre">D_out])</span></code> class
that take the size of the layers as parameters of the constructor.</p></li>
<li><p>Add some non-linearity with relu acivation function</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_layer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_layer</span> <span class="o">=</span> <span class="n">d_layer</span>
        <span class="c1"># Add linear layers</span>
        <span class="n">layer_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_layer</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">d_layer</span><span class="p">[</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
                      <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">d_layer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linears</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span><span class="n">layer_list</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_layer</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="c1"># relu(Wl x) for all hidden layer</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">layer</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="c1"># softmax(Wl x) for output layer</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">X</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">([</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">D_out</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> \
    <span class="n">train_val_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="s1">&#39;-b&#39;</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="s1">&#39;--r&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.1449</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">64.01</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.3366</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">89.96</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1693</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">95.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1361</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">96.11</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0946</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">97.21</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0992</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">96.94</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">6</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0606</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">98.20</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1002</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">96.97</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">8</span><span class="o">/</span><span class="mi">9</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0395</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">98.87</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0833</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">97.42</span><span class="o">%</span>

<span class="n">Training</span> <span class="n">complete</span> <span class="ow">in</span> <span class="mi">1</span><span class="n">m</span> <span class="mi">11</span><span class="n">s</span>
<span class="n">Best</span> <span class="n">val</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">97.60</span><span class="o">%</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_32_1.png" src="../_images/dl_mlp_pytorch_32_1.png" />
</section>
<section id="reduce-the-size-of-training-dataset">
<h2>Reduce the size of training dataset<a class="headerlink" href="#reduce-the-size-of-training-dataset" title="Link to this heading">¶</a></h2>
<p>Reduce the size of the training dataset by considering only <code class="docutils literal notranslate"><span class="pre">10</span></code>
minibatche for size<code class="docutils literal notranslate"><span class="pre">16</span></code>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_size</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">16</span>

<span class="c1"># Stratified sub-sampling</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">nclasses</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">targets</span><span class="p">))</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="n">lab</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span>
                                           <span class="nb">int</span><span class="p">(</span><span class="n">train_size</span> <span class="o">/</span> <span class="n">nclasses</span><span class="p">),</span>
                                           <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                          <span class="k">for</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">targets</span><span class="p">)])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>

<span class="n">train_loader</span> <span class="o">=</span> \
    <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="p">,</span>
                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span>
                        <span class="n">sampler</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">SubsetRandomSampler</span><span class="p">(</span><span class="n">indices</span><span class="p">))</span>

<span class="c1"># Check train subsampling</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">labels</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
                              <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Train size=&quot;</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_labels</span><span class="p">),</span> <span class="s2">&quot; Train label count=&quot;</span><span class="p">,</span>
      <span class="p">{</span><span class="n">lab</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">train_labels</span> <span class="o">==</span> <span class="n">lab</span><span class="p">)</span> <span class="k">for</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">set</span><span class="p">(</span><span class="n">train_labels</span><span class="p">)})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Batch sizes=&quot;</span><span class="p">,</span> <span class="p">[</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">])</span>

<span class="c1"># Put together train and val</span>
<span class="n">dataloaders</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">train</span><span class="o">=</span><span class="n">train_loader</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;test&quot;</span><span class="p">])</span>

<span class="c1"># Info about the dataset</span>
<span class="n">D_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">D_out</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">unique</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Datasets shape&quot;</span><span class="p">,</span> <span class="p">{</span><span class="n">x</span><span class="p">:</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span>
                         <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="o">.</span><span class="n">keys</span><span class="p">()})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;N input features&quot;</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="s2">&quot;N output&quot;</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Train</span> <span class="n">size</span><span class="o">=</span> <span class="mi">160</span>  <span class="n">Train</span> <span class="n">label</span> <span class="n">count</span><span class="o">=</span> <span class="p">{</span><span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">0</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">6</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">7</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">8</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">9</span><span class="p">):</span> <span class="n">np</span><span class="o">.</span><span class="n">int64</span><span class="p">(</span><span class="mi">16</span><span class="p">)}</span>
<span class="n">Batch</span> <span class="n">sizes</span><span class="o">=</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">]</span>
<span class="n">Datasets</span> <span class="n">shape</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">60000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">]),</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">])}</span>
<span class="n">N</span> <span class="nb">input</span> <span class="n">features</span> <span class="mi">784</span> <span class="n">N</span> <span class="n">output</span> <span class="mi">10</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">([</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">D_out</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> \
    <span class="n">train_val_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="s1">&#39;-b&#39;</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="s1">&#39;--r&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.3042</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">10.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.3013</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">9.80</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">20</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.0282</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">30.63</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.0468</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">24.58</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">40</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.4945</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">88.75</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.9630</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">66.42</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">60</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0483</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.9570</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">74.40</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">80</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0145</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.0840</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">74.40</span><span class="o">%</span>

<span class="n">Training</span> <span class="n">complete</span> <span class="ow">in</span> <span class="mi">1</span><span class="n">m</span> <span class="mi">7</span><span class="n">s</span>
<span class="n">Best</span> <span class="n">val</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">75.32</span><span class="o">%</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_35_1.png" src="../_images/dl_mlp_pytorch_35_1.png" />
<p>Use an opimizer with an adaptative learning rate: Adam</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">([</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">D_out</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> \
    <span class="n">train_val_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="s1">&#39;-b&#39;</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="s1">&#39;--r&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.2763</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">15.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.1595</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">45.78</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">20</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0010</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.1346</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">77.06</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">40</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0003</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.2461</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">76.97</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">60</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0001</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3149</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">77.00</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">80</span><span class="o">/</span><span class="mi">99</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.0001</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">100.00</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.3633</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">77.07</span><span class="o">%</span>

<span class="n">Training</span> <span class="n">complete</span> <span class="ow">in</span> <span class="mi">1</span><span class="n">m</span> <span class="mi">6</span><span class="n">s</span>
<span class="n">Best</span> <span class="n">val</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">77.54</span><span class="o">%</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_37_1.png" src="../_images/dl_mlp_pytorch_37_1.png" />
</section>
<section id="run-mlp-on-cifar-10-dataset">
<h2>Run MLP on CIFAR-10 dataset<a class="headerlink" href="#run-mlp-on-cifar-10-dataset" title="Link to this heading">¶</a></h2>
<p>The CIFAR-10 dataset consists of 60000 32x32 colour images in 10
classes, with 6000 images per class. There are 50000 training images and
10000 test images.</p>
<p>The dataset is divided into five training batches and one test batch,
each with 10000 images. The test batch contains exactly 1000
randomly-selected images from each class. The training batches contain
the remaining images in random order, but some training batches may
contain more images from one class than another. Between them, the
training batches contain exactly 5000 images from each class. The ten
classes are: airplane, automobile, bird, cat, deer, dog, frog, horse,
ship, truck</p>
<p>Load CIFAR-10 dataset <a class="reference external" href="https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/datasets.py">CIFAR-10
Loader</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pystatsml.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_cifar10_pytorch</span>

<span class="n">dataloaders</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">load_cifar10_pytorch</span><span class="p">(</span>
    <span class="n">batch_size_train</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">batch_size_test</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Info about the dataset</span>
<span class="n">D_in</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">D_out</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">dataloaders</span><span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">targets</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Datasets shape:&quot;</span><span class="p">,</span> <span class="p">{</span>
      <span class="n">x</span><span class="p">:</span> <span class="n">dataloaders</span><span class="p">[</span><span class="n">x</span><span class="p">]</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">dataloaders</span><span class="o">.</span><span class="n">keys</span><span class="p">()})</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;N input features:&quot;</span><span class="p">,</span> <span class="n">D_in</span><span class="p">,</span> <span class="s2">&quot;N output:&quot;</span><span class="p">,</span> <span class="n">D_out</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Files</span> <span class="n">already</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">verified</span>
<span class="n">Files</span> <span class="n">already</span> <span class="n">downloaded</span> <span class="ow">and</span> <span class="n">verified</span>
<span class="n">Datasets</span> <span class="n">shape</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;train&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">50000</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">)}</span>
<span class="n">N</span> <span class="nb">input</span> <span class="n">features</span><span class="p">:</span> <span class="mi">3072</span> <span class="n">N</span> <span class="n">output</span><span class="p">:</span> <span class="mi">10</span>
</pre></div>
</div>
<p>Run MLP Classifier with hidden layers of sizes: 512, 256, 128, and 64:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">([</span><span class="n">D_in</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">D_out</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>

<span class="n">model</span><span class="p">,</span> <span class="n">losses</span><span class="p">,</span> <span class="n">accuracies</span> <span class="o">=</span> \
    <span class="n">train_val_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">dataloaders</span><span class="p">,</span>
                    <span class="n">num_epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">log_interval</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">losses</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> <span class="s1">&#39;-b&#39;</span><span class="p">,</span> <span class="n">losses</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> <span class="s1">&#39;--r&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Epoch</span> <span class="mi">0</span><span class="o">/</span><span class="mi">19</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.6872</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">39.50</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5318</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">45.31</span><span class="o">%</span>

<span class="n">Epoch</span> <span class="mi">10</span><span class="o">/</span><span class="mi">19</span>
<span class="o">----------</span>
<span class="n">train</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.7136</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">74.50</span><span class="o">%</span>
<span class="n">test</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">1.5536</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">54.77</span><span class="o">%</span>

<span class="n">Training</span> <span class="n">complete</span> <span class="ow">in</span> <span class="mi">4</span><span class="n">m</span> <span class="mi">3</span><span class="n">s</span>
<span class="n">Best</span> <span class="n">val</span> <span class="n">Acc</span><span class="p">:</span> <span class="mf">54.84</span><span class="o">%</span>
</pre></div>
</div>
<img alt="../_images/dl_mlp_pytorch_42_1.png" src="../_images/dl_mlp_pytorch_42_1.png" />
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Multilayer Perceptron (MLP)</a><ul>
<li><a class="reference internal" href="#single-layer-softmax-classifier-multinomial-logistic-regression">Single Layer Softmax Classifier (Multinomial Logistic Regression)</a></li>
<li><a class="reference internal" href="#dataset-mnist-handwritten-digit-recognition">Dataset: MNIST Handwritten Digit Recognition</a></li>
<li><a class="reference internal" href="#model-two-layer-mlp">Model: Two Layer MLP</a><ul>
<li><a class="reference internal" href="#mlp-with-scikit-learn">MLP with Scikit-learn</a></li>
<li><a class="reference internal" href="#mlp-with-pytorch">MLP with pytorch</a><ul>
<li><a class="reference internal" href="#train-the-model">Train the Model</a></li>
<li><a class="reference internal" href="#save-and-reload-pytorch-model">Save and reload PyTorch model</a></li>
<li><a class="reference internal" href="#continue-training-from-checkpoints-reload-the-model-and-run-10-more-epochs">Continue training from checkpoints: reload the model and run 10 more epochs</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#test-several-mlp-architectures">Test several MLP architectures</a></li>
<li><a class="reference internal" href="#reduce-the-size-of-training-dataset">Reduce the size of training dataset</a></li>
<li><a class="reference internal" href="#run-mlp-on-cifar-10-dataset">Run MLP on CIFAR-10 dataset</a></li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/deep_learning/dl_mlp_pytorch.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/deep_learning/dl_mlp_pytorch.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>