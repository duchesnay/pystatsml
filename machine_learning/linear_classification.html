
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Linear models for classification problems &#8212; Statistics and Machine Learning in Python 0.5 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-binder.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-dataframe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery-rendered-html.css" />
    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.5 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Non-linear models" href="../auto_gallery/ml_supervized_nonlinear.html" />
    <link rel="prev" title="Linear models for regression problems" href="linear_regression.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="linear-models-for-classification-problems">
<h1>Linear models for classification problems<a class="headerlink" href="#linear-models-for-classification-problems" title="Permalink to this headline">¶</a></h1>
<figure class="align-default" id="id3">
<img alt="Linear (logistic) classification" src="../_images/linear_logistic.png" />
<figcaption>
<p><span class="caption-text">Linear (logistic) classification</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Given a training set of <span class="math notranslate nohighlight">\(N\)</span> samples,
<span class="math notranslate nohighlight">\(D = \{(\boldsymbol{x_1} , y_1 ), \ldots , (\boldsymbol{x_N} , y_N )\}\)</span>
, where <span class="math notranslate nohighlight">\(\boldsymbol{x_i}\)</span> is a multidimensional input vector with
dimension <span class="math notranslate nohighlight">\(P\)</span> and class label (target or response).</p>
<p>Multiclass Classification problems can be seen as several binary
classification problems <span class="math notranslate nohighlight">\(y_i \in \{0, 1\}\)</span> where the classifier
aims to discriminate the sample of the current class (label 1) versus
the samples of other classes (label 0).</p>
<p>Therfore, for each class the classifier seek for a vector of parameters
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> that performs a linear combination of the input
variables, <span class="math notranslate nohighlight">\(\boldsymbol{x}^T \boldsymbol{w}\)</span>. This step performs a
<strong>projection</strong> or a <strong>rotation</strong> of input sample into a good
discriminative one-dimensional sub-space, that best discriminate sample
of current class vs sample of other classes.</p>
<p>This score (a.k.a decision function) is tranformed, using the nonlinear
activation funtion <span class="math notranslate nohighlight">\(f(.)\)</span>, to a “posterior probabilities” of class
1: <span class="math notranslate nohighlight">\(p(y=1|\boldsymbol{x}) = f(\boldsymbol{x}^T \boldsymbol{w})\)</span>,
where, <span class="math notranslate nohighlight">\(p(y=1|\boldsymbol{x}) = 1 - p(y=0|\boldsymbol{x})\)</span>.</p>
<p>The decision surfaces (orthogonal hyperplan to <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>)
correspond to <span class="math notranslate nohighlight">\(f(x)=\text{constant}\)</span>, so that
<span class="math notranslate nohighlight">\(\boldsymbol{x}^T \boldsymbol{w}=\text{constant}\)</span> and hence the
decision surfaces are linear functions of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, even
if the function <span class="math notranslate nohighlight">\(f(.)\)</span> is nonlinear.</p>
<p>A thresholding of the activation (shifted by the bias or intercept)
provides the predicted class label.</p>
<p>The vector of parameters, that defines the discriminative axis,
minimizes an <strong>objective function</strong> <span class="math notranslate nohighlight">\(J(\boldsymbol{w})\)</span> that is a
sum of of <strong>loss function</strong> <span class="math notranslate nohighlight">\(L(\boldsymbol{w})\)</span> and some penalties
on the weights vector <span class="math notranslate nohighlight">\(\Omega(\boldsymbol{w})\)</span>.</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{w}}~J = \sum_i L(y_i, f(\boldsymbol{x_i}^T\boldsymbol{w})) + \Omega(\boldsymbol{w}),\]</div>
<section id="fishers-linear-discriminant-with-equal-class-covariance">
<h2>Fisher’s linear discriminant with equal class covariance<a class="headerlink" href="#fishers-linear-discriminant-with-equal-class-covariance" title="Permalink to this headline">¶</a></h2>
<p>This geometric method does not make any probabilistic assumptions,
instead it relies on distances. It looks for the <strong>linear projection</strong>
of the data points onto a vector, <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, that maximizes
the between/within variance ratio, denoted <span class="math notranslate nohighlight">\(F(\boldsymbol{w})\)</span>.
Under a few assumptions, it will provide the same results as linear
discriminant analysis (LDA), explained below.</p>
<p>Suppose two classes of observations, <span class="math notranslate nohighlight">\(C_0\)</span> and <span class="math notranslate nohighlight">\(C_1\)</span>, have
means <span class="math notranslate nohighlight">\(\boldsymbol{\mu_0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mu_1}\)</span> and the
same total within-class scatter (“covariance”) matrix,</p>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{X_c}\)</span> is the <span class="math notranslate nohighlight">\((N \times P)\)</span> matrix of
data centered on their respective means:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{X_c} = \begin{bmatrix}
          \boldsymbol{X_0} -  \boldsymbol{\mu_0} \\
          \boldsymbol{X_1} -  \boldsymbol{\mu_1}
      \end{bmatrix},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{X_0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{X_1}\)</span> are the
<span class="math notranslate nohighlight">\((N_0 \times P)\)</span> and <span class="math notranslate nohighlight">\((N_1 \times P)\)</span> matrices of samples of
classes <span class="math notranslate nohighlight">\(C_0\)</span> and <span class="math notranslate nohighlight">\(C_1\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{S_B}\)</span> being the scatter “between-class” matrix,
given by</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{S_B} = (\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )(\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )^T.\]</div>
<p>The linear combination of features <span class="math notranslate nohighlight">\(\boldsymbol{w}^T x\)</span> have means
<span class="math notranslate nohighlight">\(\boldsymbol{w}^T \mu_i\)</span> for <span class="math notranslate nohighlight">\(i=0,1\)</span>, and variance
<span class="math notranslate nohighlight">\(\boldsymbol{w}^T \boldsymbol{X^T_c} \boldsymbol{X_c} \boldsymbol{w}\)</span>.
Fisher defined the separation between these two distributions to be the
ratio of the variance between the classes to the variance within the
classes:</p>
<section id="the-fisher-most-discriminant-projection">
<h3>The Fisher most discriminant projection<a class="headerlink" href="#the-fisher-most-discriminant-projection" title="Permalink to this headline">¶</a></h3>
<p>In the two-class case, the maximum separation occurs by a projection on
the <span class="math notranslate nohighlight">\((\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\)</span> using the
Mahalanobis metric <span class="math notranslate nohighlight">\(\boldsymbol{S_W}^{-1}\)</span>, so that</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w} \propto \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0}).\]</div>
<section id="demonstration">
<h4>Demonstration<a class="headerlink" href="#demonstration" title="Permalink to this headline">¶</a></h4>
<p>Differentiating <span class="math notranslate nohighlight">\(F_{\text{Fisher}}(w)\)</span> with respect to <span class="math notranslate nohighlight">\(w\)</span>
gives</p>
<p>Since we do not care about the magnitude of <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, only
its direction, we replaced the scalar factor
<span class="math notranslate nohighlight">\((\boldsymbol{w}^T \boldsymbol{S_B} \boldsymbol{w}) / (\boldsymbol{w}^T \boldsymbol{S_W} \boldsymbol{w})\)</span>
by <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>In the multiple-class case, the solutions <span class="math notranslate nohighlight">\(w\)</span> are determined by
the eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{S_W}^{-1}{\boldsymbol{S_B}}\)</span> that
correspond to the <span class="math notranslate nohighlight">\(K-1\)</span> largest eigenvalues.</p>
<p>However, in the two-class case (in which
<span class="math notranslate nohighlight">\(\boldsymbol{S_B} = (\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )(\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )^T\)</span>)
it is easy to show that
<span class="math notranslate nohighlight">\(\boldsymbol{w} = \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\)</span>
is the unique eigenvector of
<span class="math notranslate nohighlight">\(\boldsymbol{S_W}^{-1}{\boldsymbol{S_B}}\)</span>:</p>
<p>where here
<span class="math notranslate nohighlight">\(\lambda = (\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )^T \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\)</span>.
Which leads to the result</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w} \propto \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0}).\]</div>
</section>
</section>
<section id="the-separating-hyperplane">
<h3>The separating hyperplane<a class="headerlink" href="#the-separating-hyperplane" title="Permalink to this headline">¶</a></h3>
<p>The separating hyperplane is a <span class="math notranslate nohighlight">\(P-1\)</span>-dimensional hyper surface,
orthogonal to the projection vector, <span class="math notranslate nohighlight">\(w\)</span>. There is no single best
way to find the origin of the plane along <span class="math notranslate nohighlight">\(w\)</span>, or equivalently the
classification threshold that determines whether a point should be
classified as belonging to <span class="math notranslate nohighlight">\(C_0\)</span> or to <span class="math notranslate nohighlight">\(C_1\)</span>. However, if
the projected points have roughly the same distribution, then the
threshold can be chosen as the hyperplane exactly between the
projections of the two means, i.e. as</p>
<div class="math notranslate nohighlight">
\[T = \boldsymbol{w} \cdot \frac{1}{2}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0}).\]</div>
<figure class="align-default" id="id4">
<img alt="The Fisher most discriminant projection" src="../_images/fisher_linear_disc.png" />
<figcaption>
<p><span class="caption-text">The Fisher most discriminant projection</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">matplotlib</span> inline

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">sklearn.linear_model</span> <span class="k">as</span> <span class="nn">lm</span>
<span class="kn">import</span> <span class="nn">sklearn.metrics</span> <span class="k">as</span> <span class="nn">metrics</span>

<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="linear-discriminant-analysis-lda">
<h2>Linear discriminant analysis (LDA)<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Permalink to this headline">¶</a></h2>
<p>Linear discriminant analysis (LDA) is a probabilistic generalization of
Fisher’s linear discriminant. It uses Bayes’ rule to fix the threshold
based on prior probabilities of classes.</p>
<ol class="arabic simple">
<li><p>First compute the class-<strong>conditional distributions</strong> of
<span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> given class <span class="math notranslate nohighlight">\(C_k\)</span>:
<span class="math notranslate nohighlight">\(p(x|C_k) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{S_W})\)</span>.
Where
<span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{S_W})\)</span>
is the multivariate Gaussian distribution defined over a
P-dimensional vector <span class="math notranslate nohighlight">\(x\)</span> of continuous variables, which is
given by</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{S_W}) = \frac{1}{(2\pi)^{P/2}|\boldsymbol{S_W}|^{1/2}}\exp\{-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu_k})^T \boldsymbol{S_W}^{-1}(x - \boldsymbol{\mu_k})\}\]</div>
<ol class="arabic simple" start="2">
<li><p>Estimate the <strong>prior probabilities</strong> of class <span class="math notranslate nohighlight">\(k\)</span>,
<span class="math notranslate nohighlight">\(p(C_k) = N_k/N\)</span>.</p></li>
<li><p>Compute <strong>posterior probabilities</strong> (ie. the probability of a each
class given a sample) combining conditional with priors using Bayes’
rule:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[p(C_k|\boldsymbol{x}) = \frac{p(C_k) p(\boldsymbol{x}|C_k)}{p(\boldsymbol{x})}\]</div>
<p>Where <span class="math notranslate nohighlight">\(p(x)\)</span> is the marginal distribution obtained by suming of
classes: As usual, the denominator in Bayes’ theorem can be found in
terms of the quantities appearing in the numerator, because</p>
<div class="math notranslate nohighlight">
\[p(x) = \sum_k p(\boldsymbol{x}|C_k)p(C_k)\]</div>
<ol class="arabic simple" start="4">
<li><p>Classify <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> using the Maximum-a-Posteriori
probability: <span class="math notranslate nohighlight">\(C_k= \arg \max_{C_k} p(C_k|\boldsymbol{x})\)</span></p></li>
</ol>
<p>LDA is a <strong>generative model</strong> since the class-conditional distributions
cal be used to generate samples of each classes.</p>
<p>LDA is useful to deal with imbalanced group sizes (eg.:
<span class="math notranslate nohighlight">\(N_1 \gg N_0\)</span>) since priors probabilities can be used to
explicitly re-balance the classification by setting
<span class="math notranslate nohighlight">\(p(C_0) = p(C_1) = 1/2\)</span> or whatever seems relevant.</p>
<p>LDA can be generalised to the multiclass case with <span class="math notranslate nohighlight">\(K&gt;2\)</span>.</p>
<p>With <span class="math notranslate nohighlight">\(N_1 = N_0\)</span>, LDA lead to the same solution than Fisher’s
linear discriminant.</p>
<section id="exercise">
<h3>Exercise<a class="headerlink" href="#exercise" title="Permalink to this headline">¶</a></h3>
<p>How many parameters are required to estimate to perform a LDA ?</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>

<span class="c1"># Dataset 2 two multivariate normal</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">mean0</span><span class="p">,</span> <span class="n">mean1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">.8</span><span class="p">],[</span><span class="mf">.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean0</span><span class="p">,</span> <span class="n">Cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">Cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="c1"># LDA with scikit-learn</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_pred_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_lda</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_lda</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.05</span>
</pre></div>
</div>
</section>
</section>
<section id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Permalink to this headline">¶</a></h2>
<p>Logistic regression is called a generalized linear models. ie.: it is a
linear model with a link function that maps the output of linear
multiple regression to the posterior probability of class <span class="math notranslate nohighlight">\(1\)</span>
<span class="math notranslate nohighlight">\(p(1|x)\)</span> using the logistic sigmoid function:</p>
<div class="math notranslate nohighlight">
\[p(1|\boldsymbol{w, x_i}) = \frac{1}{1 + \exp(-\boldsymbol{w} \cdot \boldsymbol{x_i})}\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Logistic (sigmoid)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Logistic (sigmoid)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_6_1.png" src="../_images/linear_classification_6_1.png" />
<p>Logistic regression is a <strong>discriminative model</strong> since it focuses only
on the posterior probability of each class <span class="math notranslate nohighlight">\(p(C_k|x)\)</span>. It only
requires to estimate the <span class="math notranslate nohighlight">\(P\)</span> weights of the <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
vector. Thus it should be favoured over LDA with many input features. In
small dimension and balanced situations it would provide similar
predictions than LDA.</p>
<p>However imbalanced group sizes cannot be explicitly controlled. It can
be managed using a reweighting of the input samples.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logreg</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># This class implements regularized logistic regression.</span>
<span class="c1"># C is the Inverse of regularization strength.</span>
<span class="c1"># Large value =&gt; no regularization.</span>

<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_logreg</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_logreg</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_logreg</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.05</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">5.15</span>  <span class="mf">5.57</span><span class="p">]]</span>
</pre></div>
</div>
<section id="id1">
<h3>Exercise<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Explore the <code class="docutils literal notranslate"><span class="pre">Logistic</span> <span class="pre">Regression</span></code> parameters and proposes a solution
in cases of highly imbalanced training dataset <span class="math notranslate nohighlight">\(N_1 \gg N_0\)</span> when
we know that in reality both classes have the same probability
<span class="math notranslate nohighlight">\(p(C_1) = p(C_0)\)</span>.</p>
</section>
</section>
<section id="losses">
<h2>Losses<a class="headerlink" href="#losses" title="Permalink to this headline">¶</a></h2>
<section id="negative-log-likelihood-or-cross-entropy">
<h3>Negative log likelihood or cross-entropy<a class="headerlink" href="#negative-log-likelihood-or-cross-entropy" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Loss function</strong> for sample <span class="math notranslate nohighlight">\(i\)</span> is the negative log of the
probability:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L(\boldsymbol{w, x_i}, y_i) = \begin{cases}
        -\log(p(1|w, \boldsymbol{x_i}))  &amp; \text{if } y_i = 1
        \\
        -\log(1 - p(1|w, \boldsymbol{x_i})  &amp; \text{if } y_i = 0
        \end{cases}\end{split}\]</div>
<p>For the whole dataset
<span class="math notranslate nohighlight">\(\boldsymbol{X}, \boldsymbol{y} = \{\boldsymbol{x_i}, y_i\}\)</span> the
loss function to minimize <span class="math notranslate nohighlight">\(L(\boldsymbol{w, X, y})\)</span> is the
negative negative log likelihood (nll) that can be simplied using a 0/1
coding of the label in the case of binary classification:</p>
<p>This is known as the <strong>cross-entropy</strong> between the true label <span class="math notranslate nohighlight">\(y\)</span>
and the predicted probability <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>For the logistic regression case, we have:</p>
<div class="math notranslate nohighlight">
\[L(\boldsymbol{w, X, y}) = \sum_i\{y_i \boldsymbol{w \cdot x_i} - \log(1 + \exp(\boldsymbol{w \cdot x_i}))\}\]</div>
<p>This is solved by numerical method using the gradient of the loss:</p>
<div class="math notranslate nohighlight">
\[\partial\frac{L(\boldsymbol{w, X, y})}{\partial\boldsymbol{w}} = \sum_i \boldsymbol{x_i} (y_i - p(1|\boldsymbol{w}, \boldsymbol{x_i}))\]</div>
<p>See also <a class="reference external" href="https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation">Scikit learn
doc</a></p>
</section>
<section id="hinge-loss-or-ell-1-loss">
<h3>Hinge loss or <span class="math notranslate nohighlight">\(\ell_1\)</span> loss<a class="headerlink" href="#hinge-loss-or-ell-1-loss" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</section>
</section>
<section id="overfitting">
<h2>Overfitting<a class="headerlink" href="#overfitting" title="Permalink to this headline">¶</a></h2>
<p>VC dimension (for Vapnik–Chervonenkis dimension) is a measure of the
<strong>capacity</strong> (complexity, expressive power, richness, or flexibility) of
a statistical classification algorithm, defined as the cardinality of
the largest set of points that the algorithm can shatter.</p>
<p>Theorem: Linear classifier in <span class="math notranslate nohighlight">\(R^P\)</span> have VC dimension of
<span class="math notranslate nohighlight">\(P+1\)</span>. Hence in dimension two (<span class="math notranslate nohighlight">\(P=2\)</span>) any random partition
of 3 points can be learned.</p>
<figure class="align-default" id="id5">
<img alt="In 2D we can shatter any three non-collinear points" src="../_images/vc_dimension_linear_2d.png" />
<figcaption>
<p><span class="caption-text">In 2D we can shatter any three non-collinear points</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="regularization-using-penalization-of-coefficients">
<h2>Regularization using penalization of coefficients<a class="headerlink" href="#regularization-using-penalization-of-coefficients" title="Permalink to this headline">¶</a></h2>
<p>The penalties use in regression are also used in classification. The
only difference is the loss function generally the negative log
likelihood (cross-entropy) or the hinge loss. We will explore:</p>
<ul class="simple">
<li><p>Ridge (also called <span class="math notranslate nohighlight">\(\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_2^2\)</span>. It shrinks coefficients toward 0.</p></li>
<li><p>Lasso (also called <span class="math notranslate nohighlight">\(\ell_1\)</span>) penalty: <span class="math notranslate nohighlight">\(\|\mathbf{w}\|_1\)</span>.
It performs feature selection by setting some coefficients to 0.</p></li>
<li><p>ElasticNet (also called <span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\alpha \left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right)\)</span>.
It performs selection of group of correlated features by setting some
coefficients to 0.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset with some correlation</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                          <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                          <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>

<span class="c1"># use solver &#39;saga&#39; to handle L1 penalty</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>

<span class="n">l1l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;elasticnet&#39;</span><span class="p">,</span>  <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>


<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span>
             <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l1l2&#39;</span><span class="p">])</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
      <th>9</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>lr</th>
      <td>4.37e-02</td>
      <td>1.14</td>
      <td>-0.28</td>
      <td>0.57</td>
      <td>0.55</td>
      <td>-0.03</td>
      <td>0.17</td>
      <td>0.37</td>
      <td>-0.42</td>
      <td>0.39</td>
    </tr>
    <tr>
      <th>l2</th>
      <td>-4.87e-02</td>
      <td>0.52</td>
      <td>-0.21</td>
      <td>0.34</td>
      <td>0.26</td>
      <td>-0.05</td>
      <td>0.14</td>
      <td>0.27</td>
      <td>-0.25</td>
      <td>0.21</td>
    </tr>
    <tr>
      <th>l1</th>
      <td>0.00e+00</td>
      <td>0.31</td>
      <td>0.00</td>
      <td>0.10</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.26</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>l1l2</th>
      <td>-7.08e-03</td>
      <td>0.41</td>
      <td>-0.15</td>
      <td>0.29</td>
      <td>0.12</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.20</td>
      <td>-0.10</td>
      <td>0.06</td>
    </tr>
  </tbody>
</table>
</div></section>
<section id="ridge-fishers-linear-classification-ell-2-regularization">
<h2>Ridge Fisher’s linear classification (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)<a class="headerlink" href="#ridge-fishers-linear-classification-ell-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>When the matrix <span class="math notranslate nohighlight">\(\boldsymbol{S_W}\)</span> is not full rank or
<span class="math notranslate nohighlight">\(P \gg N\)</span>, the The Fisher most discriminant projection estimate of
the is not unique. This can be solved using a biased version of
<span class="math notranslate nohighlight">\(\boldsymbol{S_W}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{S_W}^{Ridge} = \boldsymbol{S_W} + \lambda \boldsymbol{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the <span class="math notranslate nohighlight">\(P \times P\)</span> identity matrix. This leads to
the regularized (ridge) estimator of the Fisher’s linear discriminant
analysis:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w}^{Ridge} \propto (\boldsymbol{S_W} + \lambda \boldsymbol{I})^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\]</div>
<figure class="align-default" id="id6">
<img alt="The Ridge Fisher most discriminant projection" src="../_images/ridge_fisher_linear_disc.png" />
<figcaption>
<p><span class="caption-text">The Ridge Fisher most discriminant projection</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> will:</p>
<ul class="simple">
<li><p>Shrinks the coefficients toward zero.</p></li>
<li><p>The covariance will converge toward the diagonal matrix, reducing the
contribution of the pairwise covariances.</p></li>
</ul>
</section>
<section id="ridge-logistic-regression-ell-2-regularization">
<h2>Ridge logistic regression (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)<a class="headerlink" href="#ridge-logistic-regression-ell-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss (negative log likelyhood)
<span class="math notranslate nohighlight">\(-\log \mathcal{L}(\boldsymbol{w})\)</span> with a penalty of the L2 norm
of the weights vector. In the two-class case, using the 0/1 coding we
obtain:</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{w}}~\text{Logistic ridge}(\boldsymbol{w}) = -\log \mathcal{L}(\boldsymbol{w, X, y}) + \lambda~\|\boldsymbol{w}\|^2\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="n">lrl2</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="c1"># This class implements regularized logistic regression. C is the Inverse of regularization strength.</span>
<span class="c1"># Large value =&gt; no regularization.</span>

<span class="n">lrl2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_l2</span> <span class="o">=</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">prob_pred_l2</span> <span class="o">=</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probas of 5 first samples for class 0 and class 1:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prob_pred_l2</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:])</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lrl2</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>

<span class="c1"># Retrieve proba from coef vector</span>
<span class="n">probas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Diff&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prob_pred_l2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">probas</span><span class="p">)))</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_l2</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Probas of 5 first samples for class 0 and class 1:
[[0.89 0.11]
 [0.72 0.28]
 [0.73 0.27]
 [0.75 0.25]
 [0.48 0.52]]
Coef vector:
[[-0.05  0.52 -0.21  0.34  0.26 -0.05  0.14  0.27 -0.25  0.21]]
Diff 0.0
Nb errors=24, error rate=0.24
</pre></div>
</div>
</section>
<section id="lasso-logistic-regression-ell-1-regularization">
<h2>Lasso logistic regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-logistic-regression-ell-1-regularization" title="Permalink to this headline">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss <span class="math notranslate nohighlight">\(-\log \mathcal{L}(\boldsymbol{w})\)</span> with a penalty
of the L1 norm of the weights vector. In the two-class case, using the
0/1 coding we obtain:</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{w}}~\text{Logistic Lasso}(w) = -\log \mathcal{L}(\boldsymbol{w, X, y}) + \lambda~\|\boldsymbol{w}\|_1\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">linear_model</span>
<span class="n">lrl1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span> <span class="c1"># lambda = 1 / C!</span>

<span class="c1"># This class implements regularized logistic regression. C is the Inverse of regularization strength.</span>
<span class="c1"># Large value =&gt; no regularization.</span>

<span class="n">lrl1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_lrl1</span> <span class="o">=</span> <span class="n">lrl1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_lrl1</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_lrl1</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lrl1</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.27</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="mf">0.</span>   <span class="mf">0.31</span> <span class="mf">0.</span>   <span class="mf">0.1</span>  <span class="mf">0.</span>   <span class="mf">0.</span>   <span class="mf">0.</span>   <span class="mf">0.26</span> <span class="mf">0.</span>   <span class="mf">0.</span>  <span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="ridge-linear-support-vector-machine-ell-2-regularization">
<h2>Ridge linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)<a class="headerlink" href="#ridge-linear-support-vector-machine-ell-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>Support Vector Machine seek for separating hyperplane with maximum
margin to enforce robustness against noise. Like logistic regression it
is a <strong>discriminative method</strong> that only focuses of predictions.</p>
<p>Here we present the non separable case of Maximum Margin Classifiers
with <span class="math notranslate nohighlight">\(\pm 1\)</span> coding (ie.: <span class="math notranslate nohighlight">\(y_i \ \{-1, +1\}\)</span>). In the next
figure the legend aply to samples of “dot” class.</p>
<figure class="align-default" id="id7">
<img alt="Linear lar margin classifiers" src="../_images/svm.png" />
<figcaption>
<p><span class="caption-text">Linear lar margin classifiers</span><a class="headerlink" href="#id7" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Linear SVM for classification (also called SVM-C or SVC) minimizes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lll}
\text{min}   &amp; \text{Linear SVM}(\boldsymbol{w}) &amp;= \text{penalty}(w) +  C~\text{Hinge loss}(w)\\
             &amp; &amp; = \|w\|_2^2 + C~\sum_i^N\xi_i\\
\text{with}  &amp; \forall i &amp; y_i (w \cdot \boldsymbol{x_i}) \geq 1 - \xi_i
\end{array}\end{split}\]</div>
<p>Here we introduced the slack variables: <span class="math notranslate nohighlight">\(\xi_i\)</span>, with
<span class="math notranslate nohighlight">\(\xi_i = 0\)</span> for points that are on or inside the correct margin
boundary and <span class="math notranslate nohighlight">\(\xi_i = |y_i - (w \ cdot \cdot \boldsymbol{x_i})|\)</span>
for other points. Thus:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(y_i (w \cdot \boldsymbol{x_i}) \geq 1\)</span> then the point lies
outside the margin but on the correct side of the decision boundary.
In this case <span class="math notranslate nohighlight">\(\xi_i=0\)</span>. The constraint is thus not active for
this point. It does not contribute to the prediction.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(1 &gt; y_i (w \cdot \boldsymbol{x_i}) \geq 0\)</span> then the point
lies inside the margin and on the correct side of the decision
boundary. In this case <span class="math notranslate nohighlight">\(0&lt;\xi_i \leq 1\)</span>. The constraint is
active for this point. It does contribute to the prediction as a
support vector.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(0 &lt; y_i (w \cdot \boldsymbol{x_i})\)</span>) then the point is on
the wrong side of the decision boundary (missclassification). In this
case <span class="math notranslate nohighlight">\(0&lt;\xi_i &gt; 1\)</span>. The constraint is active for this point. It
does contribute to the prediction as a support vector.</p></li>
</ol>
<p>This loss is called the hinge loss, defined as:</p>
<div class="math notranslate nohighlight">
\[\max(0, 1 - y_i~ (w \cdot \boldsymbol{x_i}))\]</div>
<p>So linear SVM is closed to Ridge logistic regression, using the hinge
loss instead of the logistic loss. Both will provide very similar
predictions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="n">svmlin</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="c1"># Remark: by default LinearSVC uses squared_hinge as loss</span>
<span class="n">svmlin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_svmlin</span> <span class="o">=</span> <span class="n">svmlin</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_svmlin</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_svmlin</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svmlin</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.20</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.</span>    <span class="mf">0.32</span> <span class="o">-</span><span class="mf">0.09</span>  <span class="mf">0.17</span>  <span class="mf">0.16</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.06</span>  <span class="mf">0.13</span> <span class="o">-</span><span class="mf">0.16</span>  <span class="mf">0.13</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="lasso-linear-support-vector-machine-ell-1-regularization">
<h2>Lasso linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-linear-support-vector-machine-ell-1-regularization" title="Permalink to this headline">¶</a></h2>
<p>Linear SVM for classification (also called SVM-C or SVC) with
l1-regularization</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lll}
\text{min}   &amp; F_{\text{Lasso linear SVM}}(w) &amp;= ||w||_1 + C~\sum_i^N\xi_i\\
\text{with}  &amp; \forall i &amp; y_i (w \cdot \boldsymbol{x_i}) \geq 1 - \xi_i
\end{array}\end{split}\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="n">svmlinl1</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Remark: by default LinearSVC uses squared_hinge as loss</span>

<span class="n">svmlinl1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_svmlinl1</span> <span class="o">=</span> <span class="n">svmlinl1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_svmlinl1</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_svmlinl1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svmlinl1</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.20</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.37</span> <span class="o">-</span><span class="mf">0.12</span>  <span class="mf">0.24</span>  <span class="mf">0.17</span>  <span class="mf">0.</span>    <span class="mf">0.</span>    <span class="mf">0.1</span>  <span class="o">-</span><span class="mf">0.16</span>  <span class="mf">0.13</span><span class="p">]]</span>
</pre></div>
</div>
<p>## Exercise</p>
<p>Compare predictions of Logistic regression (LR) and their SVM
counterparts, ie.: L2 LR vs L2 SVM and L1 LR vs L1 SVM</p>
<ul class="simple">
<li><p>Compute the correlation between pairs of weights vectors.</p></li>
<li><p>Compare the predictions of two classifiers using their decision
function:</p>
<ul>
<li><p>Give the equation of the decision function for a linear
classifier, assuming that their is no intercept.</p></li>
<li><p>Compute the correlation decision function.</p></li>
<li><p>Plot the pairwise decision function of the classifiers.</p></li>
</ul>
</li>
<li><p>Conclude on the differences between Linear SVM and logistic
regression.</p></li>
</ul>
</section>
<section id="elastic-net-classification-ell-1-ell-2-regularization">
<h2>Elastic-net classification (<span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>-regularization)<a class="headerlink" href="#elastic-net-classification-ell-1-ell-2-regularization" title="Permalink to this headline">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss <span class="math notranslate nohighlight">\(\log L(\boldsymbol{w})\)</span> or the hinge loss with
combination of L1 and L2 penalties. In the two-class case, using the 0/1
coding we obtain:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use SGD solver</span>
<span class="n">enetlog</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">enetlog</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Or saga solver:</span>
<span class="c1"># enetloglike = lm.LogisticRegression(penalty=&#39;elasticnet&#39;,</span>
<span class="c1">#                                    C=.1, l1_ratio=0.5, solver=&#39;saga&#39;)</span>

<span class="n">enethinge</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">enethinge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hinge loss and logistic loss provide almost the same predictions.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion matrix&quot;</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">enetlog</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">enethinge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decision_function log x hinge losses:&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">enetlog</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
             <span class="n">enethinge</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Hinge</span> <span class="n">loss</span> <span class="ow">and</span> <span class="n">logistic</span> <span class="n">loss</span> <span class="n">provide</span> <span class="n">almost</span> <span class="n">the</span> <span class="n">same</span> <span class="n">predictions</span><span class="o">.</span>
<span class="n">Confusion</span> <span class="n">matrix</span>
<span class="n">Decision_function</span> <span class="n">log</span> <span class="n">x</span> <span class="n">hinge</span> <span class="n">losses</span><span class="p">:</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_25_1.png" src="../_images/linear_classification_25_1.png" />
</section>
<section id="classification-performance-evaluation-metrics">
<h2>Classification performance evaluation metrics<a class="headerlink" href="#classification-performance-evaluation-metrics" title="Permalink to this headline">¶</a></h2>
<p>source: <a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></p>
<p>Imagine a study evaluating a new test that screens people for a disease.
Each person taking the test either has or does not have the disease. The
test outcome can be positive (classifying the person as having the
disease) or negative (classifying the person as not having the disease).
The test results for each subject may or may not match the subject’s
actual status. In that setting:</p>
<ul>
<li><p>True positive (TP): Sick people correctly identified as sick</p></li>
<li><p>False positive (FP): Healthy people incorrectly identified as sick</p></li>
<li><p>True negative (TN): Healthy people correctly identified as healthy</p></li>
<li><p>False negative (FN): Sick people incorrectly identified as healthy</p></li>
<li><p><strong>Accuracy</strong> (ACC):</p>
<p>ACC = (TP + TN) / (TP + FP + FN + TN)</p>
</li>
<li><p><strong>Sensitivity</strong> (SEN) or <strong>recall</strong> of the positive class or true
positive rate (TPR) or hit rate:</p>
<p>SEN = TP / P = TP / (TP+FN)</p>
</li>
<li><p><strong>Specificity</strong> (SPC) or <strong>recall</strong> of the negative class or true
negative rate:</p>
<p>SPC = TN / N = TN / (TN+FP)</p>
</li>
<li><p><strong>Precision</strong> or positive predictive value (PPV):</p>
<p>PPV = TP / (TP + FP)</p>
</li>
<li><p><strong>Balanced accuracy</strong> (bACC):is a useful performance measure is the
balanced accuracy which avoids inflated performance estimates on
imbalanced datasets (Brodersen, et al. (2010). “The balanced accuracy
and its posterior distribution”). It is defined as the arithmetic
mean of sensitivity and specificity, or the average accuracy obtained
on either class:</p>
<p>bACC = 1/2 * (SEN + SPC)</p>
</li>
<li><p>F1 Score (or F-score) which is a weighted average of precision and
recall are usefull to deal with imballaced datasets</p></li>
</ul>
<p>The four outcomes can be formulated in a 2×2 contingency table or
confusion matrix
<a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></p>
<p>For more precision see:
<a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html">http://scikit-learn.org/stable/modules/model_evaluation.html</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># The overall precision an recall</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Recalls on individual classes: SEN &amp; SPC</span>
<span class="n">recalls</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">recalls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># is the recall of class 0: specificity</span>
<span class="n">recalls</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># is the recall of class 1: sensitivity</span>

<span class="c1"># Balanced accuracy</span>
<span class="n">b_acc</span> <span class="o">=</span> <span class="n">recalls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># The overall precision an recall on each individual class</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<section id="significance-of-classification-rate">
<h3>Significance of classification rate<a class="headerlink" href="#significance-of-classification-rate" title="Permalink to this headline">¶</a></h3>
<p>P-value associated to classification rate. Compared the number of
correct classifications (=accuracy <span class="math notranslate nohighlight">\(\times N\)</span>) to the null
hypothesis of Binomial distribution of parameters <span class="math notranslate nohighlight">\(p\)</span> (typically
50% of chance level) and <span class="math notranslate nohighlight">\(N\)</span> (Number of observations).</p>
<p>Is 65% of accuracy a significant prediction rate among 70 observations?</p>
<p>Since this is an exact, <strong>two-sided</strong> test of the null hypothesis, the
p-value can be divided by 2 since we test that the accuracy is superior
to the chance level.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.stats</span>

<span class="n">acc</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="mf">0.65</span><span class="p">,</span> <span class="mi">70</span>
<span class="n">pval</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">binom_test</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="n">acc</span> <span class="o">*</span> <span class="n">N</span><span class="p">),</span> <span class="n">n</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
<span class="nb">print</span><span class="p">(</span><span class="n">pval</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.01123144774625465</span>
</pre></div>
</div>
</section>
<section id="area-under-curve-auc-of-receiver-operating-characteristic-roc">
<h3>Area Under Curve (AUC) of Receiver operating characteristic (ROC)<a class="headerlink" href="#area-under-curve-auc-of-receiver-operating-characteristic-roc" title="Permalink to this headline">¶</a></h3>
<p>Some classifier may have found a good discriminative projection
<span class="math notranslate nohighlight">\(w\)</span>. However if the threshold to decide the final predicted class
is poorly adjusted, the performances will highlight an high specificity
and a low sensitivity or the contrary.</p>
<p>In this case it is recommended to use the AUC of a ROC analysis which
basically provide a measure of overlap of the two classes when points
are projected on the discriminative axis. For more detail on ROC and AUC
see:https://en.wikipedia.org/wiki/Receiver_operating_characteristic.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.1</span> <span class="p">,</span><span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.6</span><span class="p">,</span> <span class="mf">.7</span><span class="p">,</span> <span class="mf">.8</span><span class="p">])</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">thres</span> <span class="o">=</span> <span class="mf">.9</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">score_pred</span> <span class="o">&gt;</span> <span class="n">thres</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With a threshold of </span><span class="si">%.2f</span><span class="s2">, the rule always predict 0. Predictions:&quot;</span> <span class="o">%</span> <span class="n">thres</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># The overall precision an recall on each individual class</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recalls on individual classes are:&quot;</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="s2">&quot;ie, 100</span><span class="si">% o</span><span class="s2">f specificity, 0</span><span class="si">% o</span><span class="s2">f sensitivity&quot;</span><span class="p">)</span>

<span class="c1"># However AUC=1 indicating a perfect separation of the two classes</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">score_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;But the AUC of </span><span class="si">%.2f</span><span class="s2"> demonstrate a good classes separation.&quot;</span> <span class="o">%</span> <span class="n">auc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">With</span> <span class="n">a</span> <span class="n">threshold</span> <span class="n">of</span> <span class="mf">0.90</span><span class="p">,</span> <span class="n">the</span> <span class="n">rule</span> <span class="n">always</span> <span class="n">predict</span> <span class="mf">0.</span> <span class="n">Predictions</span><span class="p">:</span>
<span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">Recalls</span> <span class="n">on</span> <span class="n">individual</span> <span class="n">classes</span> <span class="n">are</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">0.</span><span class="p">]</span> <span class="n">ie</span><span class="p">,</span> <span class="mi">100</span><span class="o">%</span> <span class="n">of</span> <span class="n">specificity</span><span class="p">,</span> <span class="mi">0</span><span class="o">%</span> <span class="n">of</span> <span class="n">sensitivity</span>
<span class="n">But</span> <span class="n">the</span> <span class="n">AUC</span> <span class="n">of</span> <span class="mf">1.00</span> <span class="n">demonstrate</span> <span class="n">a</span> <span class="n">good</span> <span class="n">classes</span> <span class="n">separation</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>
<section id="imbalanced-classes">
<h2>Imbalanced classes<a class="headerlink" href="#imbalanced-classes" title="Permalink to this headline">¶</a></h2>
<p>Learning with discriminative (logistic regression, SVM) methods is
generally based on minimizing the misclassification of training samples,
which may be unsuitable for imbalanced datasets where the recognition
might be biased in favor of the most numerous class. This problem can be
addressed with a generative approach, which typically requires more
parameters to be determined leading to reduced performances in high
dimension.</p>
<p>Dealing with imbalanced class may be addressed by three main ways (see
Japkowicz and Stephen (2002) for a review), resampling, reweighting and
one class learning.</p>
<p>In <strong>sampling strategies</strong>, either the minority class is oversampled or
majority class is undersampled or some combination of the two is
deployed. Undersampling (Zhang and Mani, 2003) the majority class would
lead to a poor usage of the left-out samples. Sometime one cannot afford
such strategy since we are also facing a small sample size problem even
for the majority class. Informed oversampling, which goes beyond a
trivial duplication of minority class samples, requires the estimation
of class conditional distributions in order to generate synthetic
samples. Here generative models are required. An alternative, proposed
in (Chawla et al., 2002) generate samples along the line segments
joining any/all of the k minority class nearest neighbors. Such
procedure blindly generalizes the minority area without regard to the
majority class, which may be particularly problematic with
high-dimensional and potentially skewed class distribution.</p>
<p><strong>Reweighting</strong>, also called cost-sensitive learning, works at an
algorithmic level by adjusting the costs of the various classes to
counter the class imbalance. Such reweighting can be implemented within
SVM (Chang and Lin, 2001) or logistic regression (Friedman et al., 2010)
classifiers. Most classifiers of Scikit learn offer such reweighting
possibilities.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter can be positioned into the <code class="docutils literal notranslate"><span class="pre">&quot;balanced&quot;</span></code>
mode which uses the values of <span class="math notranslate nohighlight">\(y\)</span> to automatically adjust weights
inversely proportional to class frequencies in the input data as
<span class="math notranslate nohighlight">\(N / (2 N_k)\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="s2">&quot;#samples of class </span><span class="si">%i</span><span class="s2"> = </span><span class="si">%i</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">lev</span><span class="p">))</span> <span class="k">for</span> <span class="n">lev</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# No Reweighting balanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_inter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">lr_inter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; The predictions are balanced in sensitivity and specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Create imbalanced dataset, by subsampling sample of class 0: keep only 10% of</span>
<span class="c1"># class 0&#39;s samples and all class 1&#39;s samples.</span>
<span class="n">n0</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">subsample_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">][:</span><span class="n">n0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">Ximb</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">subsample_idx</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">yimb</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subsample_idx</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="s2">&quot;#samples of class </span><span class="si">%i</span><span class="s2"> = </span><span class="si">%i</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yimb</span> <span class="o">==</span> <span class="n">lev</span><span class="p">))</span> <span class="k">for</span> <span class="n">lev</span> <span class="ow">in</span>
        <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">yimb</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# No Reweighting on imbalanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_inter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Ximb</span><span class="p">,</span> <span class="n">yimb</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">yimb</span><span class="p">,</span> <span class="n">lr_inter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Ximb</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; Sensitivity &gt;&gt; specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# Reweighting on imbalanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter_reweight</span> <span class="o">=</span> <span class="n">linear_model</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s2">&quot;balanced&quot;</span><span class="p">)</span>
<span class="n">lr_inter_reweight</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Ximb</span><span class="p">,</span> <span class="n">yimb</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">yimb</span><span class="p">,</span>
                                                     <span class="n">lr_inter_reweight</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Ximb</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; The predictions are balanced in sensitivity and specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#samples of class 0 = 250; #samples of class 1 = 250;</span>
<span class="c1"># No Reweighting balanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">0.940</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.928</span>
<span class="c1"># =&gt; The predictions are balanced in sensitivity and specificity</span>

<span class="c1">#samples of class 0 = 12; #samples of class 1 = 250;</span>
<span class="c1"># No Reweighting on imbalanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">0.750</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.996</span>
<span class="c1"># =&gt; Sensitivity &gt;&gt; specificity</span>

<span class="c1"># Reweighting on imbalanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">1.000</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.980</span>
<span class="c1"># =&gt; The predictions are balanced in sensitivity and specificity</span>
</pre></div>
</div>
</section>
<section id="confidence-interval-cross-validation">
<h2>Confidence interval cross-validation<a class="headerlink" href="#confidence-interval-cross-validation" title="Permalink to this headline">¶</a></h2>
<p>Confidence interval CI classification accuracy measured by
cross-validation: <img alt="CI classification" src="../_images/classif_accuracy_95ci_sizes.png" /></p>
</section>
<section id="id2">
<h2>Exercise<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<section id="fisher-linear-discriminant-rule">
<h3>Fisher linear discriminant rule<a class="headerlink" href="#fisher-linear-discriminant-rule" title="Permalink to this headline">¶</a></h3>
<p>Write a class <code class="docutils literal notranslate"><span class="pre">FisherLinearDiscriminant</span></code> that implements the Fisher’s
linear discriminant analysis. This class must be compliant with the
scikit-learn API by providing two methods: - <code class="docutils literal notranslate"><span class="pre">fit(X,</span> <span class="pre">y)</span></code> which fits
the model and returns the object itself; - <code class="docutils literal notranslate"><span class="pre">predict(X)</span></code> which returns
a vector of the predicted values. Apply the object on the dataset
presented for the LDA.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Statistics and Machine Learning in Python</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction/python_ecosystem.html">Python ecosystem for data-science</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../introduction/machine_learning.html#data-analysis-methodology">Data analysis methodology</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html">Import libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#basic-operations">Basic operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#data-types">Data types</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#execution-control-statements">Execution control statements</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#list-comprehensions-iterators-etc">List comprehensions, iterators, etc.</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#functions">Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#regular-expression">Regular expression</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#system-programming">System programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#scripts-and-argument-parsing">Scripts and argument parsing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#networking">Networking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#modules-and-packages">Modules and packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#object-oriented-programming-oop">Object Oriented Programming (OOP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#style-guide-for-python-programming">Style guide for Python programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#documenting">Documenting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/python_lang.html#exercises">Exercises</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/scipy_numpy.html">Numpy: arrays and matrices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/scipy_pandas.html">Pandas: data manipulation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../scientific_python/scipy_matplotlib.html">Data visualization: matplotlib &amp; seaborn</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_univ.html">Univariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/stat_univ_lab_brain-volume.html">Lab: Brain volumes study</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/lmm/lmm.html">Linear Mixed Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/stat_multiv.html">Multivariate statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../statistics/time_series.html">Time series in python</a></li>
</ul>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="decomposition.html">Linear dimension reduction and feature extraction</a></li>
<li class="toctree-l1"><a class="reference internal" href="manifold.html">Manifold learning: non-linear dimension reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="linear_regression.html">Linear models for regression problems</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Linear models for classification problems</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#fishers-linear-discriminant-with-equal-class-covariance">Fisher’s linear discriminant with equal class covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#linear-discriminant-analysis-lda">Linear discriminant analysis (LDA)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#logistic-regression">Logistic regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="#losses">Losses</a></li>
<li class="toctree-l2"><a class="reference internal" href="#overfitting">Overfitting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#regularization-using-penalization-of-coefficients">Regularization using penalization of coefficients</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ridge-fishers-linear-classification-ell-2-regularization">Ridge Fisher’s linear classification (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ridge-logistic-regression-ell-2-regularization">Ridge logistic regression (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lasso-logistic-regression-ell-1-regularization">Lasso logistic regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#ridge-linear-support-vector-machine-ell-2-regularization">Ridge linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#lasso-linear-support-vector-machine-ell-1-regularization">Lasso linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#elastic-net-classification-ell-1-ell-2-regularization">Elastic-net classification (<span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>-regularization)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#classification-performance-evaluation-metrics">Classification performance evaluation metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#imbalanced-classes">Imbalanced classes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#confidence-interval-cross-validation">Confidence interval cross-validation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id2">Exercise</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_supervized_nonlinear.html">Non-linear models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_resampling.html">Resampling methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="ensemble_learning.html">Ensemble learning: bagging, boosting and stacking</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimization/optim_gradient_descent.html">Gradient descent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../auto_gallery/ml_lab_face_recognition.html">Lab: Faces recognition using various learning models</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_backprop_numpy-pytorch-sklearn.html">Backpropagation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_mlp_mnist_pytorch.html">Multilayer Perceptron (MLP)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_cnn_cifar10_pytorch.html">Convolutional neural network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deep_learning/dl_transfer-learning_cifar10-ants-bees_pytorch.html">Transfer Learning Tutorial</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="linear_regression.html" title="previous chapter">Linear models for regression problems</a></li>
      <li>Next: <a href="../auto_gallery/ml_supervized_nonlinear.html" title="next chapter">Non-linear models</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.0.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/machine_learning/linear_classification.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>