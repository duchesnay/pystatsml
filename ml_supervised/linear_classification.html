<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Models for Classification &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Non-Linear Kernel Methods and Support Vector Machines (SVM)" href="../auto_gallery/kernel_svm.html" />
    <link rel="prev" title="Linear Models for Regression" href="linear_regression.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="linear-models-for-classification">
<h1>Linear Models for Classification<a class="headerlink" href="#linear-models-for-classification" title="Link to this heading">¶</a></h1>
<p>Linear vs. non-linear classifier: See <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py">Scikit-learn Classifier
comparison</a>.</p>
<p><img alt="Linear (logistic) classification" src="../_images/linear_logistic.png" />[width=15cm]</p>
<p>Given a training set of <span class="math notranslate nohighlight">\(N\)</span> samples,
<span class="math notranslate nohighlight">\(D = \{(\boldsymbol{x_1} , y_1 ), \ldots , (\boldsymbol{x_N} , y_N )\}\)</span>
, where <span class="math notranslate nohighlight">\(\boldsymbol{x_i}\)</span> is a multidimensional input vector with
dimension <span class="math notranslate nohighlight">\(P\)</span> and class label (target or response).</p>
<p>Multiclass Classification problems can be seen as several binary
classification problems <span class="math notranslate nohighlight">\(y_i \in \{0, 1\}\)</span> where the classifier
aims to discriminate the sample of the current class (label 1) versus
the samples of other classes (label 0).</p>
<p>Therfore, for each class the classifier seek for a vector of parameters
<span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span> that performs a linear combination of the input
variables, <span class="math notranslate nohighlight">\(\boldsymbol{x}^T \boldsymbol{w}\)</span>. This step performs a
<strong>projection</strong> or a <strong>rotation</strong> of input sample into a good
discriminative one-dimensional sub-space, that best discriminate sample
of current class vs sample of other classes.</p>
<p>This score (a.k.a decision function) is tranformed, using the nonlinear
activation funtion <span class="math notranslate nohighlight">\(f(.)\)</span>, to a “posterior probabilities” of class
1: <span class="math notranslate nohighlight">\(p(y=1|\boldsymbol{x}) = f(\boldsymbol{x}^T \boldsymbol{w})\)</span>,
where, <span class="math notranslate nohighlight">\(p(y=1|\boldsymbol{x}) = 1 - p(y=0|\boldsymbol{x})\)</span>.</p>
<p>The decision surfaces (orthogonal hyperplan to <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>)
correspond to <span class="math notranslate nohighlight">\(f(x)=\text{constant}\)</span>, so that
<span class="math notranslate nohighlight">\(\boldsymbol{x}^T \boldsymbol{w}=\text{constant}\)</span> and hence the
decision surfaces are linear functions of <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, even
if the function <span class="math notranslate nohighlight">\(f(.)\)</span> is nonlinear.</p>
<p>A thresholding of the activation (shifted by the bias or intercept)
provides the predicted class label.</p>
<p>The vector of parameters, that defines the discriminative axis,
minimizes an <strong>objective function</strong> <span class="math notranslate nohighlight">\(J(\boldsymbol{w})\)</span> that is a
sum of of <strong>loss function</strong> <span class="math notranslate nohighlight">\(L(\boldsymbol{w})\)</span> and some penalties
on the weights vector <span class="math notranslate nohighlight">\(\Omega(\boldsymbol{w})\)</span>.</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{w}}~J = \sum_i L(y_i, f(\boldsymbol{x_i}^T\boldsymbol{w})) + \Omega(\boldsymbol{w}),\]</div>
<section id="fishers-linear-discriminant-with-equal-class-covariance">
<h2>Fisher’s linear discriminant with equal class covariance<a class="headerlink" href="#fishers-linear-discriminant-with-equal-class-covariance" title="Link to this heading">¶</a></h2>
<p>This geometric method does not make any probabilistic assumptions,
instead it relies on distances. It looks for the <strong>linear projection</strong>
of the data points onto a vector, <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, that maximizes
the between/within variance ratio, denoted <span class="math notranslate nohighlight">\(F(\boldsymbol{w})\)</span>.
Under a few assumptions, it will provide the same results as linear
discriminant analysis (LDA), explained below.</p>
<p>Suppose two classes of observations, <span class="math notranslate nohighlight">\(C_0\)</span> and <span class="math notranslate nohighlight">\(C_1\)</span>, have
means <span class="math notranslate nohighlight">\(\boldsymbol{\mu_0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{\mu_1}\)</span> and the
same total within-class scatter (“covariance”) matrix,</p>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{X_c}\)</span> is the <span class="math notranslate nohighlight">\((N \times P)\)</span> matrix of
data centered on their respective means:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\boldsymbol{X_c} = \begin{bmatrix}
          \boldsymbol{X_0} -  \boldsymbol{\mu_0} \\
          \boldsymbol{X_1} -  \boldsymbol{\mu_1}
      \end{bmatrix},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{X_0}\)</span> and <span class="math notranslate nohighlight">\(\boldsymbol{X_1}\)</span> are the
<span class="math notranslate nohighlight">\((N_0 \times P)\)</span> and <span class="math notranslate nohighlight">\((N_1 \times P)\)</span> matrices of samples of
classes <span class="math notranslate nohighlight">\(C_0\)</span> and <span class="math notranslate nohighlight">\(C_1\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\boldsymbol{S_B}\)</span> being the scatter “between-class” matrix,
given by</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{S_B} = (\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )(\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )^T.\]</div>
<p>The linear combination of features <span class="math notranslate nohighlight">\(\boldsymbol{w}^T x\)</span> have means
<span class="math notranslate nohighlight">\(\boldsymbol{w}^T \mu_i\)</span> for <span class="math notranslate nohighlight">\(i=0,1\)</span>, and variance
<span class="math notranslate nohighlight">\(\boldsymbol{w}^T
\boldsymbol{X^T_c} \boldsymbol{X_c} \boldsymbol{w}\)</span>. Fisher defined the
separation between these two distributions to be the ratio of the
variance between the classes to the variance within the classes:</p>
<section id="the-fisher-most-discriminant-projection">
<h3>The Fisher most discriminant projection<a class="headerlink" href="#the-fisher-most-discriminant-projection" title="Link to this heading">¶</a></h3>
<p>In the two-class case, the maximum separation occurs by a projection on
the <span class="math notranslate nohighlight">\((\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\)</span> using the
Mahalanobis metric <span class="math notranslate nohighlight">\(\boldsymbol{S_W}^{-1}\)</span>, so that</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w} \propto \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0}).\]</div>
<section id="demonstration">
<h4>Demonstration<a class="headerlink" href="#demonstration" title="Link to this heading">¶</a></h4>
<p>Differentiating <span class="math notranslate nohighlight">\(F_{\text{Fisher}}(w)\)</span> with respect to <span class="math notranslate nohighlight">\(w\)</span>
gives</p>
<p>Since we do not care about the magnitude of <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>, only
its direction, we replaced the scalar factor
<span class="math notranslate nohighlight">\((\boldsymbol{w}^T \boldsymbol{S_B} \boldsymbol{w}) / (\boldsymbol{w}^T \boldsymbol{S_W} \boldsymbol{w})\)</span>
by <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>In the multiple-class case, the solutions <span class="math notranslate nohighlight">\(w\)</span> are determined by
the eigenvectors of <span class="math notranslate nohighlight">\(\boldsymbol{S_W}^{-1}{\boldsymbol{S_B}}\)</span> that
correspond to the <span class="math notranslate nohighlight">\(K-1\)</span> largest eigenvalues.</p>
<p>However, in the two-class case (in which
<span class="math notranslate nohighlight">\(\boldsymbol{S_B} = (\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )(\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )^T\)</span>)
it is easy to show that
<span class="math notranslate nohighlight">\(\boldsymbol{w} = \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\)</span>
is the unique eigenvector of
<span class="math notranslate nohighlight">\(\boldsymbol{S_W}^{-1}{\boldsymbol{S_B}}\)</span>:</p>
<p>where here
<span class="math notranslate nohighlight">\(\lambda = (\boldsymbol{\mu_1} - \boldsymbol{\mu_0} )^T \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\)</span>.
Which leads to the result</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w} \propto \boldsymbol{S_W}^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0}).\]</div>
</section>
</section>
<section id="the-separating-hyperplane">
<h3>The separating hyperplane<a class="headerlink" href="#the-separating-hyperplane" title="Link to this heading">¶</a></h3>
<p>The separating hyperplane is a <span class="math notranslate nohighlight">\(P-1\)</span>-dimensional hyper surface,
orthogonal to the projection vector, <span class="math notranslate nohighlight">\(w\)</span>. There is no single best
way to find the origin of the plane along <span class="math notranslate nohighlight">\(w\)</span>, or equivalently the
classification threshold that determines whether a point should be
classified as belonging to <span class="math notranslate nohighlight">\(C_0\)</span> or to <span class="math notranslate nohighlight">\(C_1\)</span>. However, if
the projected points have roughly the same distribution, then the
threshold can be chosen as the hyperplane exactly between the
projections of the two means, i.e. as</p>
<div class="math notranslate nohighlight">
\[T = \boldsymbol{w} \cdot \frac{1}{2}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0}).\]</div>
<figure class="align-default" id="id3">
<img alt="The Fisher most discriminant projection" src="../_images/fisher_linear_disc.png" />
<figcaption>
<p><span class="caption-text">The Fisher most discriminant projection</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">metrics</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Plot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Plot parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="linear-discriminant-analysis-lda">
<h2>Linear discriminant analysis (LDA)<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Link to this heading">¶</a></h2>
<p>Linear discriminant analysis (LDA) is a probabilistic generalization of
Fisher’s linear discriminant. It uses Bayes’ rule to fix the threshold
based on prior probabilities of classes.</p>
<ol class="arabic simple">
<li><p>First compute the class-<strong>conditional distributions</strong> of
<span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> given class <span class="math notranslate nohighlight">\(C_k\)</span>:
<span class="math notranslate nohighlight">\(p(x|C_k) = \mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{S_W})\)</span>.
Where
<span class="math notranslate nohighlight">\(\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{S_W})\)</span>
is the multivariate Gaussian distribution defined over a
P-dimensional vector <span class="math notranslate nohighlight">\(x\)</span> of continuous variables, which is
given by</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal{N}(\boldsymbol{x}|\boldsymbol{\mu_k}, \boldsymbol{S_W}) = \frac{1}{(2\pi)^{P/2}|\boldsymbol{S_W}|^{1/2}}\exp\{-\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu_k})^T \boldsymbol{S_W}^{-1}(x - \boldsymbol{\mu_k})\}\]</div>
<ol class="arabic simple" start="2">
<li><p>Estimate the <strong>prior probabilities</strong> of class <span class="math notranslate nohighlight">\(k\)</span>,
<span class="math notranslate nohighlight">\(p(C_k) = N_k/N\)</span>.</p></li>
<li><p>Compute <strong>posterior probabilities</strong> (ie. the probability of a each
class given a sample) combining conditional with priors using Bayes’
rule:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[p(C_k|\boldsymbol{x}) = \frac{p(C_k) p(\boldsymbol{x}|C_k)}{p(\boldsymbol{x})}\]</div>
<p>Where <span class="math notranslate nohighlight">\(p(x)\)</span> is the marginal distribution obtained by summing of
classes: As usual, the denominator in Bayes’ theorem can be found in
terms of the quantities appearing in the numerator, because</p>
<div class="math notranslate nohighlight">
\[p(x) = \sum_k p(\boldsymbol{x}|C_k)p(C_k)\]</div>
<ol class="arabic simple" start="4">
<li><p>Classify <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> using the Maximum-a-Posteriori
probability: <span class="math notranslate nohighlight">\(C_k= \arg \max_{C_k} p(C_k|\boldsymbol{x})\)</span></p></li>
</ol>
<p>LDA is a <strong>generative model</strong> since the class-conditional distributions
cal be used to generate samples of each classes.</p>
<p>LDA is useful to deal with imbalanced group sizes (eg.:
<span class="math notranslate nohighlight">\(N_1 \gg N_0\)</span>) since priors probabilities can be used to
explicitly re-balance the classification by setting
<span class="math notranslate nohighlight">\(p(C_0) = p(C_1) = 1/2\)</span> or whatever seems relevant.</p>
<p>LDA can be generalized to the multiclass case with <span class="math notranslate nohighlight">\(K&gt;2\)</span>.</p>
<p>With <span class="math notranslate nohighlight">\(N_1 = N_0\)</span>, LDA lead to the same solution than Fisher’s
linear discriminant.</p>
<p><strong>Question:</strong> How many parameters are required to estimate to perform a
LDA?</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.discriminant_analysis</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>

<span class="c1"># Dataset 2 two multivariate normal</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">mean0</span><span class="p">,</span> <span class="n">mean1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">.8</span><span class="p">],[</span><span class="mf">.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean0</span><span class="p">,</span> <span class="n">Cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">Cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>


<span class="c1"># LDA with scikit-learn</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="n">proj</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">y_pred_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_lda</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_lda</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.05</span>
</pre></div>
</div>
</section>
<section id="logistic-regression">
<h2>Logistic regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">¶</a></h2>
<p>Logistic regression is called a generalized linear models. ie.: it is a
linear model with a link function that maps the output of linear
multiple regression to the posterior probability of class <span class="math notranslate nohighlight">\(1\)</span>
<span class="math notranslate nohighlight">\(p(1|x)\)</span> using the logistic sigmoid function:</p>
<div class="math notranslate nohighlight">
\[p(1|\boldsymbol{w, x_i}) = \frac{1}{1 + \exp(-\boldsymbol{w} \cdot \boldsymbol{x_i})}\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">logistic</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Logistic (sigmoid)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Logistic (sigmoid)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_6_1.png" src="../_images/linear_classification_6_1.png" />
<p>Logistic regression is a <strong>discriminative model</strong> since it focuses only
on the posterior probability of each class <span class="math notranslate nohighlight">\(p(C_k|x)\)</span>. It only
requires to estimate the <span class="math notranslate nohighlight">\(P\)</span> weights of the <span class="math notranslate nohighlight">\(\boldsymbol{w}\)</span>
vector. Thus it should be favoured over LDA with many input features. In
small dimension and balanced situations it would provide similar
predictions than LDA.</p>
<p>However imbalanced group sizes cannot be explicitly controlled. It can
be managed using a reweighting of the input samples.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1"># This class implements regularized logistic regression.</span>
<span class="c1"># C is the Inverse of regularization strength.</span>
<span class="c1"># Large value =&gt; no regularization.</span>

<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_logreg</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_logreg</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_logreg</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.05</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">5.15</span>  <span class="mf">5.57</span><span class="p">]]</span>
</pre></div>
</div>
<section id="exercise">
<h3>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">¶</a></h3>
<p>Explore the <code class="docutils literal notranslate"><span class="pre">Logistic</span> <span class="pre">Regression</span></code> parameters and proposes a solution
in cases of highly imbalanced training dataset <span class="math notranslate nohighlight">\(N_1 \gg N_0\)</span> when
we know that in reality both classes have the same probability
<span class="math notranslate nohighlight">\(p(C_1) = p(C_0)\)</span>.</p>
</section>
</section>
<section id="losses">
<h2>Losses<a class="headerlink" href="#losses" title="Link to this heading">¶</a></h2>
<section id="negative-log-likelihood-or-cross-entropy">
<h3>Negative log likelihood or cross-entropy<a class="headerlink" href="#negative-log-likelihood-or-cross-entropy" title="Link to this heading">¶</a></h3>
<p>The <strong>Loss function</strong> for sample <span class="math notranslate nohighlight">\(i\)</span> is the negative log of the
probability:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L(\boldsymbol{w, x_i}, y_i) = \begin{cases}
        -\log(p(1|w, \boldsymbol{x_i}))  &amp; \text{if } y_i = 1
        \\
        -\log(1 - p(1|w, \boldsymbol{x_i})  &amp; \text{if } y_i = 0
        \end{cases}\end{split}\]</div>
<p>For the whole dataset
<span class="math notranslate nohighlight">\(\boldsymbol{X}, \boldsymbol{y} = \{\boldsymbol{x_i}, y_i\}\)</span> the
loss function to minimize <span class="math notranslate nohighlight">\(L(\boldsymbol{w, X, y})\)</span> is the
negative negative log likelihood (nll) that can be simplied using a 0/1
coding of the label in the case of binary classification:</p>
<p>This is known as the <strong>cross-entropy</strong> between the true label <span class="math notranslate nohighlight">\(y\)</span>
and the predicted probability <span class="math notranslate nohighlight">\(p\)</span>.</p>
<p>For the logistic regression case, we have:</p>
<div class="math notranslate nohighlight">
\[L(\boldsymbol{w, X, y}) = \sum_i\{y_i \boldsymbol{w \cdot x_i} - \log(1 + \exp(\boldsymbol{w \cdot x_i}))\}\]</div>
<p>This is solved by numerical method using the gradient of the loss:</p>
<div class="math notranslate nohighlight">
\[\partial\frac{L(\boldsymbol{w, X, y})}{\partial\boldsymbol{w}} = \sum_i \boldsymbol{x_i} (y_i - p(1|\boldsymbol{w}, \boldsymbol{x_i}))\]</div>
<p>See also <a class="reference external" href="https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation">Scikit learn
doc</a></p>
</section>
<section id="hinge-loss-or-ell-1-loss">
<h3>Hinge loss or <span class="math notranslate nohighlight">\(\ell_1\)</span> loss<a class="headerlink" href="#hinge-loss-or-ell-1-loss" title="Link to this heading">¶</a></h3>
<p>TODO</p>
</section>
</section>
<section id="regularization-using-penalization-of-coefficients">
<h2>Regularization using penalization of coefficients<a class="headerlink" href="#regularization-using-penalization-of-coefficients" title="Link to this heading">¶</a></h2>
<p>The penalties use in regression are also used in classification. The
only difference is the loss function generally the negative log
likelihood (cross-entropy) or the hinge loss. We will explore:</p>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Ridge (also called <span class="math notranslate nohighlight">\(\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_2^2\)</span>. It shrinks coefficients toward 0.</p></li>
<li><p>Lasso (also called <span class="math notranslate nohighlight">\(\ell_1\)</span>) penalty: <span class="math notranslate nohighlight">\(\|\mathbf{w}\|_1\)</span>.
It performs feature selection by setting some coefficients to 0.</p></li>
<li><p>ElasticNet (also called <span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\alpha \left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right)\)</span>.
It performs selection of group of correlated features by setting some
coefficients to 0.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset with some correlation</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Logistic Regression unpenalized</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Logistic Regression with L2 penalty</span>
<span class="n">l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>

<span class="c1"># Logistic Regression with L1 penalty</span>
<span class="c1"># use solver &#39;saga&#39; to handle L1 penalty. lambda = 1 / C</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Logistic Regression with L1/L2 penalties</span>
<span class="n">l1l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;elasticnet&#39;</span><span class="p">,</span>  <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                             <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>


<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span>
             <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l1l2&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>         <span class="mi">0</span>     <span class="mi">1</span>     <span class="mi">2</span>     <span class="mi">3</span>     <span class="mi">4</span>     <span class="mi">5</span>     <span class="mi">6</span>     <span class="mi">7</span>     <span class="mi">8</span>     <span class="mi">9</span>
<span class="n">lr</span>    <span class="mf">0.04</span>  <span class="mf">1.14</span> <span class="o">-</span><span class="mf">0.28</span>  <span class="mf">0.57</span>  <span class="mf">0.55</span> <span class="o">-</span><span class="mf">0.03</span>  <span class="mf">0.17</span>  <span class="mf">0.37</span> <span class="o">-</span><span class="mf">0.42</span>  <span class="mf">0.39</span>
<span class="n">l2</span>   <span class="o">-</span><span class="mf">0.05</span>  <span class="mf">0.52</span> <span class="o">-</span><span class="mf">0.21</span>  <span class="mf">0.34</span>  <span class="mf">0.26</span> <span class="o">-</span><span class="mf">0.05</span>  <span class="mf">0.14</span>  <span class="mf">0.27</span> <span class="o">-</span><span class="mf">0.25</span>  <span class="mf">0.21</span>
<span class="n">l1</span>    <span class="mf">0.00</span>  <span class="mf">0.31</span>  <span class="mf">0.00</span>  <span class="mf">0.10</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.26</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>
<span class="n">l1l2</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.41</span> <span class="o">-</span><span class="mf">0.15</span>  <span class="mf">0.29</span>  <span class="mf">0.12</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.20</span> <span class="o">-</span><span class="mf">0.10</span>  <span class="mf">0.06</span>
</pre></div>
</div>
</section>
<section id="understanding-the-effect-of-penalty-using-ell-2-regularization-fishers-linear-classification">
<h3>Understanding the effect of penalty using <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization Fisher’s linear classification<a class="headerlink" href="#understanding-the-effect-of-penalty-using-ell-2-regularization-fishers-linear-classification" title="Link to this heading">¶</a></h3>
<p>When the matrix <span class="math notranslate nohighlight">\(\boldsymbol{S_W}\)</span> is not full rank or
<span class="math notranslate nohighlight">\(P \gg N\)</span>, the The Fisher most discriminant projection estimate of
the is not unique. This can be solved using a biased version of
<span class="math notranslate nohighlight">\(\boldsymbol{S_W}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{S_W}^{Ridge} = \boldsymbol{S_W} + \lambda \boldsymbol{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the <span class="math notranslate nohighlight">\(P \times P\)</span> identity matrix. This leads to
the regularized (ridge) estimator of the Fisher’s linear discriminant
analysis:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{w}^{Ridge} \propto (\boldsymbol{S_W} + \lambda \boldsymbol{I})^{-1}(\boldsymbol{\mu_1} - \boldsymbol{\mu_0})\]</div>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/ridge_fisher_linear_disc.png"><img alt="The Ridge Fisher most discriminant projection" src="../_images/ridge_fisher_linear_disc.png" style="width: 15cm;" />
</a>
<figcaption>
<p><span class="caption-text">The Ridge Fisher most discriminant projection</span><a class="headerlink" href="#id4" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> will:</p>
<ul class="simple">
<li><p>Shrinks the coefficients toward zero.</p></li>
<li><p>The covariance will converge toward the diagonal matrix, reducing the
contribution of the pairwise covariances.</p></li>
</ul>
</section>
</section>
<section id="ell-2-regularized-logistic-regression">
<h2><span class="math notranslate nohighlight">\(\ell_2\)</span>-regularized logistic regression<a class="headerlink" href="#ell-2-regularized-logistic-regression" title="Link to this heading">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss (negative log likelyhood)
<span class="math notranslate nohighlight">\(-\log \mathcal{L}(\boldsymbol{w})\)</span> with a penalty of the L2 norm
of the weights vector. In the two-class case, using the 0/1 coding we
obtain:</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{w}}~\text{Logistic ridge}(\boldsymbol{w}) = -\log \mathcal{L}(\boldsymbol{w, X, y}) + \lambda~\|\boldsymbol{w}\|^2\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="n">lrl2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="c1"># This class implements regularized logistic regression.</span>
<span class="c1"># C is the Inverse of regularization strength. Large value =&gt; no regularization.</span>

<span class="n">lrl2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_l2</span> <span class="o">=</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">prob_pred_l2</span> <span class="o">=</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probas of 5 first samples for class 0 and class 1:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prob_pred_l2</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lrl2</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Retrieve proba from coef vector</span>
<span class="n">probas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Diff&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prob_pred_l2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">probas</span><span class="p">)))</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_l2</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Probas of 5 first samples for class 0 and class 1:
[[0.89 0.11]
 [0.72 0.28]
 [0.73 0.27]
 [0.75 0.25]
 [0.48 0.52]]
Coef vector:
[[-0.05  0.52 -0.21  0.34  0.26 -0.05  0.14  0.27 -0.25  0.21]]
Diff 0.0
Nb errors=24, error rate=0.24
</pre></div>
</div>
</section>
<section id="lasso-logistic-regression-ell-1-regularization">
<h2>Lasso logistic regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-logistic-regression-ell-1-regularization" title="Link to this heading">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss <span class="math notranslate nohighlight">\(-\log \mathcal{L}(\boldsymbol{w})\)</span> with a penalty
of the L1 norm of the weights vector. In the two-class case, using the
0/1 coding we obtain:</p>
<div class="math notranslate nohighlight">
\[\min_{\boldsymbol{w}}~\text{Logistic Lasso}(w) = -\log \mathcal{L}(\boldsymbol{w, X, y}) + \lambda~\|\boldsymbol{w}\|_1\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="n">lrl1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span> <span class="c1"># lambda = 1 / C!</span>

<span class="c1"># This class implements regularized logistic regression. C is the Inverse of regularization strength.</span>
<span class="c1"># Large value =&gt; no regularization.</span>

<span class="n">lrl1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_lrl1</span> <span class="o">=</span> <span class="n">lrl1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_lrl1</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_lrl1</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lrl1</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.27</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="mf">0.</span>   <span class="mf">0.31</span> <span class="mf">0.</span>   <span class="mf">0.1</span>  <span class="mf">0.</span>   <span class="mf">0.</span>   <span class="mf">0.</span>   <span class="mf">0.26</span> <span class="mf">0.</span>   <span class="mf">0.</span>  <span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="linear-support-vector-machine-ell-2-regularization-with-hinge-loss">
<h2>Linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization with Hinge loss)<a class="headerlink" href="#linear-support-vector-machine-ell-2-regularization-with-hinge-loss" title="Link to this heading">¶</a></h2>
<p>Support Vector Machine seek for separating hyperplane with maximum
margin to enforce robustness against noise. Like logistic regression it
is a <strong>discriminative method</strong> that only focuses of predictions.</p>
<p>Here we present the non separable case of Maximum Margin Classifiers
with <span class="math notranslate nohighlight">\(\pm 1\)</span> coding (ie.: <span class="math notranslate nohighlight">\(y_i \ \{-1, +1\}\)</span>). In the next
figure the legend aply to samples of “dot” class.</p>
<figure class="align-default" id="id5">
<img alt="Linear lar margin classifiers" src="../_images/svm.png" />
<figcaption>
<p><span class="caption-text">Linear lar margin classifiers</span><a class="headerlink" href="#id5" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Linear SVM for classification (also called SVM-C or SVC) minimizes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lll}
\text{min}   &amp; \text{Linear SVM}(\boldsymbol{w}) &amp;= \text{penalty}(w) +  C~\text{Hinge loss}(w)\\
             &amp; &amp; = \|w\|_2^2 + C~\sum_i^N\xi_i\\
\text{with}  &amp; \forall i &amp; y_i (w \cdot \boldsymbol{x_i}) \geq 1 - \xi_i
\end{array}\end{split}\]</div>
<p>Here we introduced the slack variables: <span class="math notranslate nohighlight">\(\xi_i\)</span>, with
<span class="math notranslate nohighlight">\(\xi_i = 0\)</span> for points that are on or inside the correct margin
boundary and <span class="math notranslate nohighlight">\(\xi_i = |y_i - (w \ cdot  \cdot \boldsymbol{x_i})|\)</span>
for other points. Thus:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(y_i (w \cdot \boldsymbol{x_i}) \geq 1\)</span> then the point lies
outside the margin but on the correct side of the decision boundary.
In this case <span class="math notranslate nohighlight">\(\xi_i=0\)</span>. The constraint is thus not active for
this point. It does not contribute to the prediction.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(1 &gt; y_i (w \cdot \boldsymbol{x_i}) \geq 0\)</span> then the point
lies inside the margin and on the correct side of the decision
boundary. In this case <span class="math notranslate nohighlight">\(0&lt;\xi_i \leq 1\)</span>. The constraint is
active for this point. It does contribute to the prediction as a
support vector.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(0 &lt;  y_i (w \cdot \boldsymbol{x_i})\)</span>) then the point is on
the wrong side of the decision boundary (missclassification). In this
case <span class="math notranslate nohighlight">\(0&lt;\xi_i &gt; 1\)</span>. The constraint is active for this point. It
does contribute to the prediction as a support vector.</p></li>
</ol>
<p>This loss is called the hinge loss, defined as:</p>
<div class="math notranslate nohighlight">
\[\max(0, 1 - y_i~ (w \cdot \boldsymbol{x_i}))\]</div>
<p>So linear SVM is closed to Ridge logistic regression, using the hinge
loss instead of the logistic loss. Both will provide very similar
predictions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">svm</span>

<span class="n">svmlin</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="c1"># Remark: by default LinearSVC uses squared_hinge as loss</span>
<span class="n">svmlin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_svmlin</span> <span class="o">=</span> <span class="n">svmlin</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_svmlin</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_svmlin</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svmlin</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.20</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.</span>    <span class="mf">0.32</span> <span class="o">-</span><span class="mf">0.09</span>  <span class="mf">0.17</span>  <span class="mf">0.16</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.06</span>  <span class="mf">0.13</span> <span class="o">-</span><span class="mf">0.16</span>  <span class="mf">0.13</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="lasso-linear-support-vector-machine-ell-1-regularization">
<h2>Lasso linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-linear-support-vector-machine-ell-1-regularization" title="Link to this heading">¶</a></h2>
<p>Linear SVM for classification (also called SVM-C or SVC) with
l1-regularization</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{lll}
\text{min}   &amp; F_{\text{Lasso linear SVM}}(w) &amp;= ||w||_1 + C~\sum_i^N\xi_i\\
\text{with}  &amp; \forall i &amp; y_i (w \cdot \boldsymbol{x_i}) \geq 1 - \xi_i
\end{array}\end{split}\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">svm</span>

<span class="n">svmlinl1</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Remark: by default LinearSVC uses squared_hinge as loss</span>

<span class="n">svmlinl1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_svmlinl1</span> <span class="o">=</span> <span class="n">svmlinl1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_svmlinl1</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_svmlinl1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svmlinl1</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.20</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.37</span> <span class="o">-</span><span class="mf">0.12</span>  <span class="mf">0.24</span>  <span class="mf">0.17</span>  <span class="mf">0.</span>    <span class="mf">0.</span>    <span class="mf">0.1</span>  <span class="o">-</span><span class="mf">0.16</span>  <span class="mf">0.13</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="id1">
<h2>Exercise<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h2>
<p>Compare predictions of Logistic regression (LR) and their SVM
counterparts, ie.: L2 LR vs L2 SVM and L1 LR vs L1 SVM</p>
<ul class="simple">
<li><p>Compute the correlation between pairs of weights vectors.</p></li>
<li><p>Compare the predictions of two classifiers using their decision
function:</p>
<ul>
<li><p>Give the equation of the decision function for a linear classifier,
assuming that their is no intercept.</p></li>
<li><p>Compute the correlation decision function.</p></li>
<li><p>Plot the pairwise decision function of the classifiers.</p></li>
</ul>
</li>
<li><p>Conclude on the differences between Linear SVM and logistic
regression.</p></li>
</ul>
</section>
<section id="elastic-net-classification-ell-1-ell-2-regularization">
<h2>Elastic-net classification (<span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>-regularization)<a class="headerlink" href="#elastic-net-classification-ell-1-ell-2-regularization" title="Link to this heading">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss <span class="math notranslate nohighlight">\(\log L(\boldsymbol{w})\)</span> or the hinge loss with
combination of L1 and L2 penalties. In the two-class case, using the 0/1
coding we obtain:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use SGD solver</span>
<span class="n">enetlog</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;log_loss&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">enetlog</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Or saga solver:</span>
<span class="c1"># enetloglike = lm.LogisticRegression(penalty=&#39;elasticnet&#39;,</span>
<span class="c1">#                                    C=.1, l1_ratio=0.5, solver=&#39;saga&#39;)</span>

<span class="n">enethinge</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">enethinge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hinge loss and logistic loss provide almost the same predictions.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion matrix&quot;</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">enetlog</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">enethinge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decision_function log x hinge losses:&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">enetlog</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
             <span class="n">enethinge</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Hinge</span> <span class="n">loss</span> <span class="ow">and</span> <span class="n">logistic</span> <span class="n">loss</span> <span class="n">provide</span> <span class="n">almost</span> <span class="n">the</span> <span class="n">same</span> <span class="n">predictions</span><span class="o">.</span>
<span class="n">Confusion</span> <span class="n">matrix</span>
<span class="n">Decision_function</span> <span class="n">log</span> <span class="n">x</span> <span class="n">hinge</span> <span class="n">losses</span><span class="p">:</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_24_1.png" src="../_images/linear_classification_24_1.png" />
</section>
<section id="classification-performance-evaluation-metrics">
<h2>Classification performance evaluation metrics<a class="headerlink" href="#classification-performance-evaluation-metrics" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Wikipedia Sensitivity and
specificity</a></p>
<p>Imagine a study evaluating a new test that screens people for a disease.
Each person taking the test either has or does not have the disease. The
test outcome can be positive (classifying the person as having the
disease) or negative (classifying the person as not having the disease).
The test results for each subject may or may not match the subject’s
actual status. In that setting:</p>
<ul>
<li><p>True positive (TP): Sick people correctly identified as sick</p></li>
<li><p>False positive (FP): Healthy people incorrectly identified as sick</p></li>
<li><p>True negative (TN): Healthy people correctly identified as healthy</p></li>
<li><p>False negative (FN): Sick people incorrectly identified as healthy</p></li>
<li><p><strong>Accuracy</strong> (ACC):</p>
<p>ACC = (TP + TN) / (TP + FP + FN + TN)</p>
</li>
<li><p><strong>Sensitivity</strong> (SEN) or <strong>recall</strong> of the positive class or true
positive rate (TPR) or hit rate:</p>
<p>SEN = TP / P = TP / (TP+FN)</p>
</li>
<li><p><strong>Specificity</strong> (SPC) or <strong>recall</strong> of the negative class or true
negative rate:</p>
<p>SPC = TN / N = TN / (TN+FP)</p>
</li>
<li><p><strong>Precision</strong> or positive predictive value (PPV):</p>
<p>PPV = TP / (TP + FP)</p>
</li>
<li><p><strong>Balanced accuracy</strong> (bACC):is a useful performance measure is the
balanced accuracy which avoids inflated performance estimates on
imbalanced datasets (Brodersen, et al. (2010). “The balanced accuracy
and its posterior distribution”). It is defined as the arithmetic mean
of sensitivity and specificity, or the average accuracy obtained on
either class:</p>
<p>bACC = 1/2 * (SEN + SPC)</p>
</li>
<li><p>F1 Score (or F-score) which is a weighted average of precision and
recall are usefull to deal with imballaced datasets</p></li>
</ul>
<p>The four outcomes can be formulated in a 2×2 contingency table or
confusion matrix
<a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">https://en.wikipedia.org/wiki/Sensitivity_and_specificity</a></p>
<p>For more precision see:
<a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html">http://scikit-learn.org/stable/modules/model_evaluation.html</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># The overall precision an recall</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Recalls on individual classes: SEN &amp; SPC</span>
<span class="n">recalls</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">recalls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># is the recall of class 0: specificity</span>
<span class="n">recalls</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># is the recall of class 1: sensitivity</span>

<span class="c1"># Balanced accuracy</span>
<span class="n">b_acc</span> <span class="o">=</span> <span class="n">recalls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># The overall precision an recall on each individual class</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<section id="area-under-curve-auc-of-receiver-operating-characteristic-roc">
<h3>Area Under Curve (AUC) of Receiver operating characteristic (ROC)<a class="headerlink" href="#area-under-curve-auc-of-receiver-operating-characteristic-roc" title="Link to this heading">¶</a></h3>
<p>Some classifier may have found a good discriminative projection
<span class="math notranslate nohighlight">\(w\)</span>. However if the threshold to decide the final predicted class
is poorly adjusted, the performances will highlight an high specificity
and a low sensitivity or the contrary.</p>
<p>In this case it is recommended to use the AUC of a ROC analysis which
basically provide a measure of overlap of the two classes when points
are projected on the discriminative axis. For more detail on ROC and AUC
see:https://en.wikipedia.org/wiki/Receiver_operating_characteristic.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.6</span><span class="p">,</span> <span class="mf">.7</span><span class="p">,</span> <span class="mf">.8</span><span class="p">])</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">thres</span> <span class="o">=</span> <span class="mf">.9</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">score_pred</span> <span class="o">&gt;</span> <span class="n">thres</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With a threshold of </span><span class="si">%.2f</span><span class="s2">, the rule always predict 0. Predictions:&quot;</span> <span class="o">%</span> <span class="n">thres</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># The overall precision an recall on each individual class</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recalls on individual classes are:&quot;</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span>
      <span class="s2">&quot;ie, 100</span><span class="si">% o</span><span class="s2">f specificity, 0</span><span class="si">% o</span><span class="s2">f sensitivity&quot;</span><span class="p">)</span>

<span class="c1"># However AUC=1 indicating a perfect separation of the two classes</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">score_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;But the AUC of </span><span class="si">%.2f</span><span class="s2"> demonstrate a good classes separation.&quot;</span> <span class="o">%</span> <span class="n">auc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">With</span> <span class="n">a</span> <span class="n">threshold</span> <span class="n">of</span> <span class="mf">0.90</span><span class="p">,</span> <span class="n">the</span> <span class="n">rule</span> <span class="n">always</span> <span class="n">predict</span> <span class="mf">0.</span> <span class="n">Predictions</span><span class="p">:</span>
<span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">Recalls</span> <span class="n">on</span> <span class="n">individual</span> <span class="n">classes</span> <span class="n">are</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">0.</span><span class="p">]</span> <span class="n">ie</span><span class="p">,</span> <span class="mi">100</span><span class="o">%</span> <span class="n">of</span> <span class="n">specificity</span><span class="p">,</span> <span class="mi">0</span><span class="o">%</span> <span class="n">of</span> <span class="n">sensitivity</span>
<span class="n">But</span> <span class="n">the</span> <span class="n">AUC</span> <span class="n">of</span> <span class="mf">1.00</span> <span class="n">demonstrate</span> <span class="n">a</span> <span class="n">good</span> <span class="n">classes</span> <span class="n">separation</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>
<section id="imbalanced-classes">
<h2>Imbalanced classes<a class="headerlink" href="#imbalanced-classes" title="Link to this heading">¶</a></h2>
<p>Learning with discriminative (logistic regression, SVM) methods is
generally based on minimizing the misclassification of training samples,
which may be unsuitable for imbalanced datasets where the recognition
might be biased in favor of the most numerous class. This problem can be
addressed with a generative approach, which typically requires more
parameters to be determined leading to reduced performances in high
dimension.</p>
<p>Dealing with imbalanced class may be addressed by three main ways (see
Japkowicz and Stephen (2002) for a review), resampling, reweighting and
one class learning.</p>
<p>In <strong>sampling strategies</strong>, either the minority class is oversampled or
majority class is undersampled or some combination of the two is
deployed. Undersampling (Zhang and Mani, 2003) the majority class would
lead to a poor usage of the left-out samples. Sometime one cannot afford
such strategy since we are also facing a small sample size problem even
for the majority class. Informed oversampling, which goes beyond a
trivial duplication of minority class samples, requires the estimation
of class conditional distributions in order to generate synthetic
samples. Here generative models are required. An alternative, proposed
in (Chawla et al., 2002) generate samples along the line segments
joining any/all of the k minority class nearest neighbors. Such
procedure blindly generalizes the minority area without regard to the
majority class, which may be particularly problematic with
high-dimensional and potentially skewed class distribution.</p>
<p><strong>Reweighting</strong>, also called cost-sensitive learning, works at an
algorithmic level by adjusting the costs of the various classes to
counter the class imbalance. Such reweighting can be implemented within
SVM (Chang and Lin, 2001) or logistic regression (Friedman et al., 2010)
classifiers. Most classifiers of Scikit learn offer such reweighting
possibilities.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter can be positioned into the <code class="docutils literal notranslate"><span class="pre">&quot;balanced&quot;</span></code>
mode which uses the values of <span class="math notranslate nohighlight">\(y\)</span> to automatically adjust weights
inversely proportional to class frequencies in the input data as
<span class="math notranslate nohighlight">\(N / (2 N_k)\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="s2">&quot;#samples of class </span><span class="si">%i</span><span class="s2"> = </span><span class="si">%i</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">lev</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">lev</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# No Reweighting balanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_inter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">lr_inter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; The predictions are balanced in sensitivity and specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Create imbalanced dataset, by subsampling sample of class 0: keep only 10% of</span>
<span class="c1">#  class 0&#39;s samples and all class 1&#39;s samples.</span>
<span class="n">n0</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">subsample_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">][:</span><span class="n">n0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">Ximb</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">subsample_idx</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">yimb</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subsample_idx</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="s2">&quot;#samples of class </span><span class="si">%i</span><span class="s2"> = </span><span class="si">%i</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yimb</span> <span class="o">==</span> <span class="n">lev</span><span class="p">))</span> <span class="k">for</span> <span class="n">lev</span> <span class="ow">in</span>
        <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">yimb</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# No Reweighting on imbalanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_inter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Ximb</span><span class="p">,</span> <span class="n">yimb</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span>
    <span class="n">yimb</span><span class="p">,</span> <span class="n">lr_inter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Ximb</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; Sensitivity &gt;&gt; specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# Reweighting on imbalanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter_reweight</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s2">&quot;balanced&quot;</span><span class="p">)</span>
<span class="n">lr_inter_reweight</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Ximb</span><span class="p">,</span> <span class="n">yimb</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">yimb</span><span class="p">,</span>
                                                     <span class="n">lr_inter_reweight</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Ximb</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; The predictions are balanced in sensitivity and specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#samples of class 0 = 250; #samples of class 1 = 250;</span>
<span class="c1"># No Reweighting balanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">0.940</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.928</span>
<span class="c1"># =&gt; The predictions are balanced in sensitivity and specificity</span>

<span class="c1">#samples of class 0 = 12; #samples of class 1 = 250;</span>
<span class="c1"># No Reweighting on imbalanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">0.750</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.996</span>
<span class="c1"># =&gt; Sensitivity &gt;&gt; specificity</span>

<span class="c1"># Reweighting on imbalanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">1.000</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.980</span>
<span class="c1"># =&gt; The predictions are balanced in sensitivity and specificity</span>
</pre></div>
</div>
</section>
<section id="confidence-interval-cross-validation">
<h2>Confidence interval cross-validation<a class="headerlink" href="#confidence-interval-cross-validation" title="Link to this heading">¶</a></h2>
<p>Confidence interval CI classification accuracy measured by
cross-validation: <img alt="CI classification" src="../_images/classif_accuracy_95ci_sizes.png" /></p>
</section>
<section id="significance-of-classification-metrics">
<h2>Significance of classification metrics<a class="headerlink" href="#significance-of-classification-metrics" title="Link to this heading">¶</a></h2>
<p><strong>P-value of classification accuracy:</strong> Compare the number of correct
classifications (=accuracy <span class="math notranslate nohighlight">\(\times N\)</span>) to the null hypothesis of
Binomial distribution of parameters <span class="math notranslate nohighlight">\(p\)</span> (typically 50% of chance
level) and <span class="math notranslate nohighlight">\(N\)</span> (Number of observations). Is 60% accuracy a
significant prediction rate among 50 observations? Since this is an
exact, <strong>two-sided</strong> test of the null hypothesis, the p-value can be
divided by two since we test that the accuracy is superior to the chance
level.</p>
<p><strong>P-value of ROC-AUC:</strong> ROC-AUC measures the ranking of the two classes.
Therefore non-parametric test can be used to asses the significance of
the classes’s separation. Mason and Graham (RMetS, 2002) show that the
ROC area is equivalent to the Mann–Whitney U-statistic.</p>
<p>Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon, Wilcoxon
rank-sum test or Wilcoxon–Mann–Whitney test) is a nonparametric</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="n">N_test</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                        <span class="n">test_size</span><span class="o">=</span><span class="n">N_test</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">proba_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">bacc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">proba_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ACC=</span><span class="si">%.2f</span><span class="s2">, bACC=</span><span class="si">%.2f</span><span class="s2">, AUC=</span><span class="si">%.2f</span><span class="s2">,&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">bacc</span><span class="p">,</span> <span class="n">auc</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ACC</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span> <span class="n">bACC</span><span class="o">=</span><span class="mf">0.61</span><span class="p">,</span> <span class="n">AUC</span><span class="o">=</span><span class="mf">0.72</span><span class="p">,</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># acc, N = 0.65, 70</span>
<span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">acc</span> <span class="o">*</span> <span class="n">N_test</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binomtest</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">N_test</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;greater&#39;</span><span class="p">)</span>
<span class="n">auc_pval</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">mannwhitneyu</span><span class="p">(</span>
    <span class="n">proba_pred</span><span class="p">[</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">proba_pred</span><span class="p">[</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">pvalue</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_significant</span><span class="p">(</span><span class="n">pval</span><span class="p">):</span> <span class="k">return</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">pval</span> <span class="o">&lt;</span> <span class="mf">0.05</span> <span class="k">else</span> <span class="kc">False</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ACC=</span><span class="si">%.2f</span><span class="s2"> (pval=</span><span class="si">%.3f</span><span class="s2">, significance=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">acc_test</span><span class="o">.</span><span class="n">pvalue</span><span class="p">,</span> <span class="n">is_significant</span><span class="p">(</span><span class="n">acc_test</span><span class="o">.</span><span class="n">pvalue</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC=</span><span class="si">%.2f</span><span class="s2"> (pval=</span><span class="si">%.3f</span><span class="s2">, significance=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">auc</span><span class="p">,</span> <span class="n">auc_pval</span><span class="p">,</span> <span class="n">is_significant</span><span class="p">(</span><span class="n">auc_pval</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ACC</span><span class="o">=</span><span class="mf">0.60</span> <span class="p">(</span><span class="n">pval</span><span class="o">=</span><span class="mf">0.101</span><span class="p">,</span> <span class="n">significance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">AUC</span><span class="o">=</span><span class="mf">0.72</span> <span class="p">(</span><span class="n">pval</span><span class="o">=</span><span class="mf">0.009</span><span class="p">,</span> <span class="n">significance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id2">
<h2>Exercise<a class="headerlink" href="#id2" title="Link to this heading">¶</a></h2>
<section id="fisher-linear-discriminant-rule">
<h3>Fisher linear discriminant rule<a class="headerlink" href="#fisher-linear-discriminant-rule" title="Link to this heading">¶</a></h3>
<p>Write a class <code class="docutils literal notranslate"><span class="pre">FisherLinearDiscriminant</span></code> that implements the Fisher’s
linear discriminant analysis. This class must be compliant with the
scikit-learn API by providing two methods: - <code class="docutils literal notranslate"><span class="pre">fit(X,</span> <span class="pre">y)</span></code> which fits
the model and returns the object itself; - <code class="docutils literal notranslate"><span class="pre">predict(X)</span></code> which returns
a vector of the predicted values. Apply the object on the dataset
presented for the LDA.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Linear Models for Classification</a><ul>
<li><a class="reference internal" href="#fishers-linear-discriminant-with-equal-class-covariance">Fisher’s linear discriminant with equal class covariance</a><ul>
<li><a class="reference internal" href="#the-fisher-most-discriminant-projection">The Fisher most discriminant projection</a><ul>
<li><a class="reference internal" href="#demonstration">Demonstration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#the-separating-hyperplane">The separating hyperplane</a></li>
</ul>
</li>
<li><a class="reference internal" href="#linear-discriminant-analysis-lda">Linear discriminant analysis (LDA)</a></li>
<li><a class="reference internal" href="#logistic-regression">Logistic regression</a><ul>
<li><a class="reference internal" href="#exercise">Exercise</a></li>
</ul>
</li>
<li><a class="reference internal" href="#losses">Losses</a><ul>
<li><a class="reference internal" href="#negative-log-likelihood-or-cross-entropy">Negative log likelihood or cross-entropy</a></li>
<li><a class="reference internal" href="#hinge-loss-or-ell-1-loss">Hinge loss or <span class="math notranslate nohighlight">\(\ell_1\)</span> loss</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regularization-using-penalization-of-coefficients">Regularization using penalization of coefficients</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#understanding-the-effect-of-penalty-using-ell-2-regularization-fishers-linear-classification">Understanding the effect of penalty using <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization Fisher’s linear classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ell-2-regularized-logistic-regression"><span class="math notranslate nohighlight">\(\ell_2\)</span>-regularized logistic regression</a></li>
<li><a class="reference internal" href="#lasso-logistic-regression-ell-1-regularization">Lasso logistic regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li><a class="reference internal" href="#linear-support-vector-machine-ell-2-regularization-with-hinge-loss">Linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization with Hinge loss)</a></li>
<li><a class="reference internal" href="#lasso-linear-support-vector-machine-ell-1-regularization">Lasso linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li><a class="reference internal" href="#id1">Exercise</a></li>
<li><a class="reference internal" href="#elastic-net-classification-ell-1-ell-2-regularization">Elastic-net classification (<span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>-regularization)</a></li>
<li><a class="reference internal" href="#classification-performance-evaluation-metrics">Classification performance evaluation metrics</a><ul>
<li><a class="reference internal" href="#area-under-curve-auc-of-receiver-operating-characteristic-roc">Area Under Curve (AUC) of Receiver operating characteristic (ROC)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#imbalanced-classes">Imbalanced classes</a></li>
<li><a class="reference internal" href="#confidence-interval-cross-validation">Confidence interval cross-validation</a></li>
<li><a class="reference internal" href="#significance-of-classification-metrics">Significance of classification metrics</a></li>
<li><a class="reference internal" href="#id2">Exercise</a><ul>
<li><a class="reference internal" href="#fisher-linear-discriminant-rule">Fisher linear discriminant rule</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml_supervised/linear_classification.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/ml_supervised/linear_classification.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>