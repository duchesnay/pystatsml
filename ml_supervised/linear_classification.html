<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Models for Classification &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Non-Linear Kernel Methods and Support Vector Machines (SVM)" href="../auto_gallery/kernel_svm.html" />
    <link rel="prev" title="Linear Models for Regression" href="linear_regression.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="linear-models-for-classification">
<h1>Linear Models for Classification<a class="headerlink" href="#linear-models-for-classification" title="Link to this heading">¶</a></h1>
<p>Linear vs. non-linear classifier: See <a class="reference external" href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py">Scikit-learn Classifier
comparison</a>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">metrics</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_classification</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Plot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Plot parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">)</span>
</pre></div>
</div>
<section id="geometric-method-naive-method">
<h2>Geometric Method: Naive Method<a class="headerlink" href="#geometric-method-naive-method" title="Link to this heading">¶</a></h2>
<p>Principles:</p>
<ul class="simple">
<li><p>Compute classes means <span class="math notranslate nohighlight">\(\mathbf{\mu}_1\)</span>,
<span class="math notranslate nohighlight">\(\mathbf{\mu}_2, \mathbf{\mu}_k, \ldots\)</span></p></li>
<li><p>Classify new point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> to the closest mean, i.e.: class
<span class="math notranslate nohighlight">\(\arg \min_k \|\mathbf{x} - \mathbf{\mu}_k\|_2\)</span></p></li>
</ul>
<p>For binary classification, this is equivalent to compute the most
discriminative direction as the vector between class mean:</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathbf{w}_{\text{naive}} = \mathbf{\mu}_1 - \mathbf{\mu}_2}\]</div>
<p>And projecting a new point <span class="math notranslate nohighlight">\(\mathbf{x_i}\)</span> along this direction to
obtain a score <span class="math notranslate nohighlight">\(z_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[z_i = \mathbf{x_i}^\top \mathbf{w}_{\text{naive}}\]</div>
<p>Finally the Classify the point to the closest projected class mean.</p>
<p>Illustration:</p>
<figure class="align-default" id="id1">
<img alt="Most discriminant projections, Naive and Fisher methods" src="../_images/fisher_linear_disc.png" />
<figcaption>
<p><span class="caption-text">Most discriminant projections, Naive and Fisher methods</span><a class="headerlink" href="#id1" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="geometric-method-fishers-linear-discriminant">
<h2>Geometric Method: Fisher’s Linear Discriminant<a class="headerlink" href="#geometric-method-fishers-linear-discriminant" title="Link to this heading">¶</a></h2>
<p>Principles:</p>
<ul class="simple">
<li><p>Dimensionality reduction before later classification.</p></li>
<li><p>Find the most discriminant axis.</p></li>
<li><p>Taking account the distribution, assuming same normal distribution for
all classes.</p></li>
</ul>
<p>Simply compute the <strong>within class covariance</strong> <span class="math notranslate nohighlight">\(\mathbf{S_W}\)</span> to
rotate the projection direction according to the point (elliptic)
distribution:</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathbf{w}_{\text{Fisher}} = \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0})}.\]</div>
<p>This geometric method does not make any probabilistic assumptions,
instead it relies on distances. It looks for the <strong>linear projection</strong>
of the data points onto a vector, <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, that maximizes the
between/within variance ratio, denoted <span class="math notranslate nohighlight">\(F(\mathbf{w})\)</span>. Under a
few assumptions, it will provide the same results as linear discriminant
analysis (LDA), explained below.</p>
<p>Suppose two classes of observations, <span class="math notranslate nohighlight">\(C_0\)</span> and <span class="math notranslate nohighlight">\(C_1\)</span>, have
means <span class="math notranslate nohighlight">\(\mathbf{\mu_0}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\mu_1}\)</span> and the same
total within-class scatter (“covariance”) matrix,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{S_W} &amp;= \sum_{i\in C_0} (\mathbf{x_i} - \mathbf{\mu_0})(\mathbf{x_i} - \mathbf{\mu_0})^T + \sum_{j\in C_1} (\mathbf{x_j} - \mathbf{\mu_1})(\mathbf{x_j} -\mathbf{\mu_1})^T\\
    &amp;= \mathbf{X_c}^T \mathbf{X_c},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X_c}\)</span> is the <span class="math notranslate nohighlight">\((N \times P)\)</span> matrix of data
centered on their respective means:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{X_c} = \begin{bmatrix}
          \mathbf{X_0} -  \mathbf{\mu_0} \\
          \mathbf{X_1} -  \mathbf{\mu_1}
      \end{bmatrix},\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X_0}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{X_1}\)</span> are the
<span class="math notranslate nohighlight">\((N_0 \times P)\)</span> and <span class="math notranslate nohighlight">\((N_1 \times P)\)</span> matrices of samples of
classes <span class="math notranslate nohighlight">\(C_0\)</span> and <span class="math notranslate nohighlight">\(C_1\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(\mathbf{S_B}\)</span> being the scatter “between-class” matrix, given
by</p>
<div class="math notranslate nohighlight">
\[\mathbf{S_B} = (\mathbf{\mu_1} - \mathbf{\mu_0} )(\mathbf{\mu_1} - \mathbf{\mu_0} )^T.\]</div>
<p>The linear combination of features <span class="math notranslate nohighlight">\(\mathbf{w}^T x\)</span> have means
<span class="math notranslate nohighlight">\(\mathbf{w}^T \mu_i\)</span> for <span class="math notranslate nohighlight">\(i=0,1\)</span>, and variance
<span class="math notranslate nohighlight">\(\mathbf{w}^T
\mathbf{X^T_c} \mathbf{X_c} \mathbf{w}\)</span>. Fisher defined the separation
between these two distributions to be the ratio of the variance between
the classes to the variance within the classes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}F_{\text{Fisher}}(\mathbf{w}) &amp;= \frac{\sigma_{\text{between}}^2}{\sigma_{\text{within}}^2}\\
                     &amp;= \frac{(\mathbf{w}^T \mathbf{\mu_1} - \mathbf{w}^T \mathbf{\mu_0})^2}{\mathbf{w}^T  X^T_c \mathbf{X_c} \mathbf{w}}\\
                     &amp;= \frac{(\mathbf{w}^T (\mathbf{\mu_1} - \mathbf{\mu_0}))^2}{\mathbf{w}^T  X^T_c \mathbf{X_c} \mathbf{w}}\\
                     &amp;= \frac{\mathbf{w}^T (\mathbf{\mu_1} - \mathbf{\mu_0}) (\mathbf{\mu_1} - \mathbf{\mu_0})^T w}{\mathbf{w}^T X^T_c \mathbf{X_c} \mathbf{w}}\\
                     &amp;= \frac{\mathbf{w}^T \mathbf{S_B} w}{\mathbf{w}^T \mathbf{S_W} \mathbf{w}}.\end{split}\]</div>
<p>In the two-class case, the maximum separation occurs by a projection on
the <span class="math notranslate nohighlight">\((\mathbf{\mu_1} - \mathbf{\mu_0})\)</span> using the Mahalanobis
metric <span class="math notranslate nohighlight">\(\mathbf{S_W}^{-1}\)</span>, so that</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathbf{w} \propto \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0})}.\]</div>
<p><strong>Demonstration</strong></p>
<p>Differentiating <span class="math notranslate nohighlight">\(F_{\text{Fisher}}(w)\)</span> with respect to <span class="math notranslate nohighlight">\(w\)</span>
gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\mathbf{w}}F_{\text{Fisher}}(\mathbf{w}) &amp;= 0\\
\nabla_{\mathbf{w}}\left(\frac{\mathbf{w}^T \mathbf{S_B} w}{\mathbf{w}^T \mathbf{S_W} \mathbf{w}}\right) &amp;= 0\\
(\mathbf{w}^T \mathbf{S_W} \mathbf{w})(2 \mathbf{S_B} \mathbf{w}) - (\mathbf{w}^T \mathbf{S_B} \mathbf{w})(2 \mathbf{S_W} \mathbf{w}) &amp;= 0\\
(\mathbf{w}^T \mathbf{S_W} \mathbf{w})(\mathbf{S_B} \mathbf{w}) &amp;= (\mathbf{w}^T \mathbf{S_B} \mathbf{w})(\mathbf{S_W} \mathbf{w})\\
\mathbf{S_B} \mathbf{w} &amp;= \frac{\mathbf{w}^T \mathbf{S_B} \mathbf{w}}{\mathbf{w}^T \mathbf{S_W} \mathbf{w}}(\mathbf{S_W} \mathbf{w})\\
\mathbf{S_B} \mathbf{w} &amp;= \lambda (\mathbf{S_W} \mathbf{w})\\
\mathbf{S_W}^{-1}{\mathbf{S_B}} \mathbf{w} &amp;= \lambda  \mathbf{w}.\end{split}\]</div>
<p>Since we do not care about the magnitude of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>, only its
direction, we replaced the scalar factor
<span class="math notranslate nohighlight">\((\mathbf{w}^T \mathbf{S_B} \mathbf{w}) / (\mathbf{w}^T \mathbf{S_W} \mathbf{w})\)</span>
by <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>In the multiple-class case, the solutions <span class="math notranslate nohighlight">\(w\)</span> are determined by
the eigenvectors of <span class="math notranslate nohighlight">\(\mathbf{S_W}^{-1}{\mathbf{S_B}}\)</span> that
correspond to the <span class="math notranslate nohighlight">\(K-1\)</span> largest eigenvalues.</p>
<p>However, in the two-class case (in which
<span class="math notranslate nohighlight">\(\mathbf{S_B} = (\mathbf{\mu_1} - \mathbf{\mu_0} )(\mathbf{\mu_1} - \mathbf{\mu_0} )^T\)</span>)
it is easy to show that
<span class="math notranslate nohighlight">\(\mathbf{w} = \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0})\)</span>
is the unique eigenvector of <span class="math notranslate nohighlight">\(\mathbf{S_W}^{-1}{\mathbf{S_B}}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0} )(\mathbf{\mu_1} - \mathbf{\mu_0} )^T \mathbf{w} &amp;= \lambda  \mathbf{w}\\
\mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0} )(\mathbf{\mu_1} - \mathbf{\mu_0} )^T \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0}) &amp;= \lambda  \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0}),\end{split}\]</div>
<p>where here
<span class="math notranslate nohighlight">\(\lambda = (\mathbf{\mu_1} - \mathbf{\mu_0} )^T \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0})\)</span>.
Which leads to the result</p>
<div class="math notranslate nohighlight">
\[\mathbf{w} \propto \mathbf{S_W}^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0}).\]</div>
<p><strong>The separating hyperplane</strong></p>
<p>The separating hyperplane is a <span class="math notranslate nohighlight">\(P-1\)</span>-dimensional hyper surface,
orthogonal to the projection vector, <span class="math notranslate nohighlight">\(w\)</span>. There is no single best
way to find the origin of the plane along <span class="math notranslate nohighlight">\(w\)</span>, or equivalently the
classification threshold that determines whether a point should be
classified as belonging to <span class="math notranslate nohighlight">\(C_0\)</span> or to <span class="math notranslate nohighlight">\(C_1\)</span>. However, if
the projected points have roughly the same distribution, then the
threshold can be chosen as the hyperplane exactly between the
projections of the two means, i.e. as</p>
<div class="math notranslate nohighlight">
\[T = \mathbf{w} \cdot \frac{1}{2}(\mathbf{\mu_1} - \mathbf{\mu_0}).\]</div>
</section>
<section id="generative-model-linear-discriminant-analysis-lda">
<h2>Generative Model: Linear Discriminant Analysis (LDA)<a class="headerlink" href="#generative-model-linear-discriminant-analysis-lda" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p>Probabilistic generalization of Fisher’s linear discriminant.</p></li>
<li><p>Generative model of the <strong>conditional distribution</strong> of the input data
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given the label <span class="math notranslate nohighlight">\(k\)</span>:
<span class="math notranslate nohighlight">\(p(\mathbf{x}|y=k)\)</span>.</p></li>
<li><p>Uses Bayes’ rule to provide the <strong>posterior distribution</strong> of the
label <span class="math notranslate nohighlight">\(k\)</span> given the input data <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:
<span class="math notranslate nohighlight">\(p(y=k|\mathbf{x})\)</span>.</p></li>
<li><p>Uses Bayes’ rule to fix the threshold based on prior probabilities of
classes.</p></li>
</ul>
<ol class="arabic simple">
<li><p>First compute the class-<strong>conditional distributions</strong> of
<span class="math notranslate nohighlight">\(\mathbf{x}\)</span> given class <span class="math notranslate nohighlight">\(C_k\)</span>:
<span class="math notranslate nohighlight">\(p(x|C_k) = \mathcal{N}(\mathbf{x}|\mathbf{\mu_k}, \mathbf{S_W})\)</span>.
Where <span class="math notranslate nohighlight">\(\mathcal{N}(\mathbf{x}|\mathbf{\mu_k}, \mathbf{S_W})\)</span> is
the multivariate Gaussian distribution defined over a P-dimensional
vector <span class="math notranslate nohighlight">\(x\)</span> of continuous variables, which is given by</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal{N}(\mathbf{x}|\mathbf{\mu_k}, \mathbf{S_W}) = \frac{1}{(2\pi)^{P/2}|\mathbf{S_W}|^{1/2}}\exp\{-\frac{1}{2} (\mathbf{x} - \mathbf{\mu_k})^T \mathbf{S_W}^{-1}(x - \mathbf{\mu_k})\}\]</div>
<ol class="arabic simple" start="2">
<li><p>Estimate the <strong>prior probabilities</strong> of class <span class="math notranslate nohighlight">\(k\)</span>,
<span class="math notranslate nohighlight">\(p(C_k) = N_k/N\)</span>.</p></li>
<li><p>Compute <strong>posterior probabilities</strong> (ie. the probability of a each
class given a sample) combining conditional with priors using Bayes’
rule:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[p(C_k|\mathbf{x}) = \frac{p(C_k) p(\mathbf{x}|C_k)}{p(\mathbf{x})}\]</div>
<p>Where <span class="math notranslate nohighlight">\(p(x)\)</span> is the marginal distribution obtained by summing of
classes: As usual, the denominator in Bayes’ theorem can be found in
terms of the quantities appearing in the numerator, because</p>
<div class="math notranslate nohighlight">
\[p(x) = \sum_k p(\mathbf{x}|C_k)p(C_k)\]</div>
<ol class="arabic simple" start="4">
<li><p>Classify <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> using the Maximum-a-Posteriori
probability: <span class="math notranslate nohighlight">\(C_k= \arg \max_{C_k} p(C_k|\mathbf{x})\)</span></p></li>
</ol>
<p>LDA is a <strong>generative model</strong> since the class-conditional distributions
cal be used to generate samples of each classes.</p>
<p>LDA is useful to deal with imbalanced group sizes (eg.:
<span class="math notranslate nohighlight">\(N_1 \gg N_0\)</span>) since priors probabilities can be used to
explicitly re-balance the classification by setting
<span class="math notranslate nohighlight">\(p(C_0) = p(C_1) = 1/2\)</span> or whatever seems relevant.</p>
<p>LDA can be generalized to the multiclass case with <span class="math notranslate nohighlight">\(K&gt;2\)</span>.</p>
<p>With <span class="math notranslate nohighlight">\(N_1 = N_0\)</span>, LDA lead to the same solution than Fisher’s
linear discriminant.</p>
<p><strong>Question:</strong> How many parameters are required to estimate to perform a
LDA?</p>
<p>Application with scikit-learn</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.discriminant_analysis</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> <span class="k">as</span> <span class="n">LDA</span>

<span class="c1"># Dataset 2 two multivariate normal</span>
<span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span>
<span class="n">mean0</span><span class="p">,</span> <span class="n">mean1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">Cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">.8</span><span class="p">],[</span><span class="mf">.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">X0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean0</span><span class="p">,</span> <span class="n">Cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">X1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean1</span><span class="p">,</span> <span class="n">Cov</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X0</span><span class="p">,</span> <span class="n">X1</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">X0</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">X1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Naive rule</span>
<span class="n">w_naive</span> <span class="o">=</span> <span class="n">mean1</span> <span class="o">-</span> <span class="n">mean0</span>
<span class="n">w_naive</span> <span class="o">=</span> <span class="n">w_naive</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_naive</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">score_naive</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w_naive</span><span class="p">)</span>

<span class="c1"># Fischer rule</span>
<span class="n">Xc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">X0</span> <span class="o">-</span> <span class="n">mean0</span><span class="p">,</span> <span class="n">X1</span> <span class="o">-</span> <span class="n">mean1</span><span class="p">])</span>
<span class="n">Sw</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Xc</span><span class="o">.</span><span class="n">T</span> <span class="p">,</span> <span class="n">Xc</span><span class="p">)</span>
<span class="n">w_fisher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Sw</span><span class="p">),</span> <span class="n">mean1</span> <span class="o">-</span> <span class="n">mean0</span><span class="p">)</span>
<span class="n">w_fisher</span> <span class="o">=</span> <span class="n">w_fisher</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_fisher</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">score_fisher</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w_fisher</span><span class="p">)</span>


<span class="c1"># LDA with scikit-learn</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LDA</span><span class="p">()</span>
<span class="n">score_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">w_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">w_lda</span> <span class="o">=</span> <span class="n">w_lda</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">w_lda</span><span class="p">,</span> <span class="nb">ord</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="mi">2</span>
<span class="n">y_pred_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_lda</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_lda</span><span class="p">)))</span>

<span class="c1"># Plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">fig_w</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">fig_w</span> <span class="o">*</span> <span class="mf">0.5</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)]),</span> <span class="n">columns</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="s2">&quot;x2&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">))</span>
<span class="n">ax_</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;x1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;x2&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;y&quot;</span><span class="p">)</span>
<span class="n">ax_</span><span class="o">.</span><span class="n">quiver</span><span class="p">([</span><span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="p">[</span><span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span>
           <span class="p">[</span><span class="n">w_naive</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w_fisher</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">w_lda</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span>
           <span class="p">[</span><span class="n">w_naive</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_fisher</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">w_lda</span><span class="p">[</span><span class="mi">1</span><span class="p">]],</span>
           <span class="n">units</span><span class="o">=</span><span class="s1">&#39;xy&#39;</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">scores</span> <span class="o">=</span> <span class="p">[(</span><span class="s2">&quot;Naive&quot;</span><span class="p">,</span> <span class="n">score_naive</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;Fisher&quot;</span><span class="p">,</span> <span class="n">score_fisher</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;LDA&quot;</span><span class="p">,</span> <span class="n">score_lda</span><span class="p">)]</span>
<span class="n">colors</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;axes.prop_cycle&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">by_key</span><span class="p">()[</span><span class="s1">&#39;color&#39;</span><span class="p">]</span>
<span class="c1">#colors = [colors[i] for i in [0, 2]]</span>

<span class="c1">#Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="n">fig_w</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="p">(</span><span class="n">title</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="p">,</span> <span class="n">scores</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">lab</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">):</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">histplot</span><span class="p">(</span><span class="n">score</span><span class="p">[</span><span class="n">y</span> <span class="o">==</span> <span class="n">lab</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span>  <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;Label </span><span class="si">{</span><span class="n">lab</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">kde</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">lab</span><span class="p">])</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">title</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Projection on discriminative directions&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.05</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_7_1.png" src="../_images/linear_classification_7_1.png" />
<img alt="../_images/linear_classification_7_2.png" src="../_images/linear_classification_7_2.png" />
</section>
<section id="logistic-regression">
<h2>Logistic Regression<a class="headerlink" href="#logistic-regression" title="Link to this heading">¶</a></h2>
<p><img alt="Linear (logistic) classification" src="../_images/linear_logistic.png" />[width=15cm]</p>
<p>Principles:</p>
<ol class="arabic simple">
<li><p>Map the output of a linear model: <span class="math notranslate nohighlight">\(\mathbf{w}^\top \mathbf{x}\)</span>
into a score <span class="math notranslate nohighlight">\(z\)</span>. This step performs a <strong>projection</strong> or a
<strong>rotation</strong> of input sample into a good discriminative
one-dimensional sub-space, that best discriminate sample of current
class vs sample of other classes.</p></li>
<li><p>Using an activation funtion <span class="math notranslate nohighlight">\(\sigma(.)\)</span>, this score (a.k.a
decision function) is transformed, to a “posterior probabilities” of
a class <span class="math notranslate nohighlight">\(k\)</span>:
<span class="math notranslate nohighlight">\(P(y=k| \mathbf{x}) = \sigma(\mathbf{w}^\top \mathbf{x})\)</span>.</p></li>
</ol>
<p>Properties:</p>
<ul class="simple">
<li><p>Consider only the posterior probability of a class <span class="math notranslate nohighlight">\(k\)</span>:
<span class="math notranslate nohighlight">\(P(y=k| \mathbf{x})\)</span></p></li>
<li><p>Multiclass Classification problems can be seen as several binary
classification problems <span class="math notranslate nohighlight">\(y_i \in \{0, 1\}\)</span> where the classifier
aims to discriminate the sample of the current class (label 1) versus
the samples of other classes (label 0).</p></li>
<li><p>The decision surfaces (orthogonal hyperplan to <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>)
correspond to <span class="math notranslate nohighlight">\(\sigma(z)=\text{constant}\)</span>, so that
<span class="math notranslate nohighlight">\(\mathbf{x}^T \mathbf{w}=\text{constant}\)</span> and hence the decision
surfaces are linear functions of <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>, even if the
function <span class="math notranslate nohighlight">\(\sigma(.)\)</span> is nonlinear.</p></li>
<li><p>A thresholding of the activation (shifted by the bias or intercept)
provides the predicted class label.</p></li>
</ul>
<p><strong>Linear Discriminant Analysis (LDA) vs. Logistic Regression</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p><strong>Linear Discriminant
Analysis (LDA)</strong></p></th>
<th class="head"><p><strong>Logistic Regression</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Model
Type</strong></p></td>
<td><p>Generative model</p></td>
<td><p>Discriminative model</p></td>
</tr>
<tr class="row-odd"><td><p><strong>What It
Models</strong></p></td>
<td><p>Joint probability:
:math:
<cite>P(x, y) = P(x mid y) P(y)</cite></p></td>
<td><p>Posterior probability:
<span class="math notranslate nohighlight">\(P(y \mid x)\)</span></p></td>
</tr>
<tr class="row-even"><td><p><strong>Ass
umptions</strong></p></td>
<td><p>Gaussian class-conditional
distributions with equal
covariance</p></td>
<td><p>No distributional
assumption on features</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Decision
Boundary</strong></p></td>
<td><p>Linear (under equal
covariance assumption)</p></td>
<td><p>Linear</p></td>
</tr>
<tr class="row-even"><td><p><strong>P
robability
Es
timation</strong></p></td>
<td><p>Uses Bayes’ rule + Gaussian
likelihood</p></td>
<td><p>Uses sigmoid of linear
function</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Interpre
tability</strong></p></td>
<td><p>Less intuitive, based on
data distribution</p></td>
<td><p>Coefficients directly
reflect impact on class
log-odds</p></td>
</tr>
<tr class="row-even"><td><p><strong>Per
formance</strong></p></td>
<td><p>Can outperform logistic
regression when assumptions
hold</p></td>
<td><p>More robust when
assumptions (e.g.,
normality) are violated</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Use in
Mu
lticlass</strong></p></td>
<td><p>Naturally extends to
multiclass</p></td>
<td><p>Extends via one-vs-rest
or softmax (multinomial
logistic)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Regula
rization</strong></p></td>
<td><p>Not built-in (requires
extensions like shrinkage
LDA)</p></td>
<td><p>Easily regularized
(e.g., with L1/L2
penalties)</p></td>
</tr>
</tbody>
</table>
<p><strong>Key Insights</strong></p>
<ul class="simple">
<li><p><strong>LDA</strong> is <strong>generative</strong>: it models how the data was generated
(distribution of features given class), then uses Bayes’ theorem to
classify.</p></li>
<li><p><strong>Logistic regression</strong> is <strong>discriminative</strong>: it models the boundary
between classes directly.</p></li>
</ul>
<section id="activation-functions-for-classification-sigmoid-and-softmax">
<h3>Activation Functions for Classification (Sigmoid and Softmax)<a class="headerlink" href="#activation-functions-for-classification-sigmoid-and-softmax" title="Link to this heading">¶</a></h3>
<p>The <strong>sigmoid</strong> and <strong>softmax</strong> functions are closely related—they both
transform real-valued inputs (<em>logits</em>) into probabilities, but they are
used in different settings:</p>
<p><strong>The sigmoid function</strong> maps real-valued inputs (called <em>logits</em>) into
the interval <span class="math notranslate nohighlight">\([0, 1]\)</span>, making them interpretable as probabilities
(for scalar <span class="math notranslate nohighlight">\(z\)</span>):</p>
<div class="math notranslate nohighlight">
\[\sigma(z) = \frac{1}{1 + e^{-z}}\]</div>
<p>It has the following properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma(z) \to 1\)</span> as <span class="math notranslate nohighlight">\(z \to +\infty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(z) \to 0\)</span> as <span class="math notranslate nohighlight">\(z \to -\infty\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(0) = 0.5\)</span></p></li>
</ul>
<p>In binary classification, we use
<span class="math notranslate nohighlight">\(\sigma(\mathbf{w}^\top \mathbf{x})\)</span> to estimate the probability
of the positive class. This function is differentiable, which is
essential for optimization via gradient descent.</p>
<p><strong>The softmax function</strong> converts raw scores <span class="math notranslate nohighlight">\(z_k\)</span> into
probabilities:</p>
<div class="math notranslate nohighlight">
\[\hat{p}_k = \frac{e^{z_k}}{\sum_{j=1}^K e^{z_j}}\]</div>
<p>It ensures that output probabilities are positive and sum to 1, making
it suitable for use with cross-entropy.</p>
<p><strong>Softmax function</strong> (for vector <span class="math notranslate nohighlight">\(\mathbf{z} \in \mathbb{R}^K\)</span>):</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, \dots, K\]</div>
<p>The <strong>sigmoid function is a special case of the softmax</strong> function when
you have <strong>two classes (binary classification)</strong>. Given two logits
<span class="math notranslate nohighlight">\(z_0\)</span> and <span class="math notranslate nohighlight">\(z_1\)</span>, the softmax for class 1 is:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(z_1) = \frac{e^{z_1}}{e^{z_0} + e^{z_1}}\]</div>
<p>If we define <span class="math notranslate nohighlight">\(z = z_1 - z_0\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(z_1) = \frac{1}{1 + e^{-(z_1 - z_0)}} = \sigma(z)\]</div>
<p>So: <strong>Sigmoid = Softmax over 2 logits, if one logit is fixed as 0
(reference class)</strong></p>
<p>Intuition</p>
<ul class="simple">
<li><p><strong>Sigmoid</strong> gives the probability of one class (positive class)
vs. its complement.</p></li>
<li><p><strong>Softmax</strong> generalizes this to <span class="math notranslate nohighlight">\(K &gt; 2\)</span> classes, ensuring the
sum of probabilities is 1.</p></li>
</ul>
<p><strong>Summary</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>F
unction</p></th>
<th class="head"><p>Use Case</p></th>
<th class="head"><p>Output</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Sigmoid</p></td>
<td><p>Binary classification</p></td>
<td><p>Scalar in <span class="math notranslate nohighlight">\((0, 1)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Softmax</p></td>
<td><p>Multiclass classification</p></td>
<td><p>Vector of probabilities
summing to 1 over classes</p></td>
</tr>
</tbody>
</table>
<ul class="simple">
<li><p>Sigmoid and Softmax maps logits to probabilities over classes</p></li>
<li><p>The <strong>sigmoid function is equivalent to a 2-class softmax</strong>.</p></li>
<li><p>Both map logits to probabilities but are used in different
classification settings.</p></li>
<li><p>Softmax ensures a <strong>normalized probability distribution</strong> over
multiple classes.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span> <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Sigmoid (Logistic)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s1">&#39;Sigmoid (Logistic)&#39;</span><span class="p">)</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_10_1.png" src="../_images/linear_classification_10_1.png" />
</section>
<section id="loss-functions-for-classification">
<h3>Loss Functions for Classification<a class="headerlink" href="#loss-functions-for-classification" title="Link to this heading">¶</a></h3>
<section id="negative-log-likelihood-nll">
<span id="test-nll"></span><h4>Negative Log-Likelihood (NLL)<a class="headerlink" href="#negative-log-likelihood-nll" title="Link to this heading">¶</a></h4>
<p>The Negative Log-Likelihood (NLL) for a dataset of observations
<span class="math notranslate nohighlight">\(\{(\mathbf{x}_i, y_i)\}_{i=1}^n\)</span>, where each <span class="math notranslate nohighlight">\(x_i\)</span> is an
input and <span class="math notranslate nohighlight">\(y_i\)</span> is the corresponding label.</p>
<p><strong>General Formulation of NLL</strong></p>
<p>Given a <strong>probabilistic model</strong> that predicts
<span class="math notranslate nohighlight">\(P(y_i \mid x_i; \theta)\)</span>, where <span class="math notranslate nohighlight">\(\theta\)</span> are the model
parameters (e.g., weights in a neural network or logistic regression),
the <strong>Negative Log-Likelihood</strong> over the dataset is:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{NLL}}(\theta) = - \sum_{i=1}^n \log P(y_i \mid x_i; \theta)\]</div>
<p>This expression encourages the model to assign <strong>high probability</strong> to
the <strong>correct labels</strong>.</p>
<p><strong>Binary Classification (Sigmoid Output) a.k.a. Logistic or Binary
Cross-Entropy Loss</strong></p>
<p>For binary labels <span class="math notranslate nohighlight">\(y_i \in \{0, 1\}\)</span>, and using:</p>
<div class="math notranslate nohighlight">
\[P(y_i = 1 \mid x_i; \theta) = \hat{p}_i = \sigma(z_i)\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{p}\)</span> is the predicted probability of the positive class
(obtained via a sigmoid activation, <span class="math notranslate nohighlight">\(\sigma(z_i)\)</span>). The NLL
becomes:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{NLL}} = -\log \Pi_{i=1}^n \left[\hat{p}_i^{y_i} +  (1 - \hat{p}_i)^{(1 - y_i)} \right]\]</div>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{NLL}} = - \sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1 - y_i) \log(1 - \hat{p}_i) \right]\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{NLL}}(\mathbf{w}) = \sum_{i=1}^n \left[ \log(1 + e^{-z_i}) + (1 - y_i) z_i \right] \quad \text{with } y_i \in \{0, 1\}.\]</div>
<p>Recoding output <span class="math notranslate nohighlight">\(y_i \in \{-1, +1\}\)</span>, the expression simplifies to</p>
<div class="math notranslate nohighlight">
\[\boxed{\mathcal{L}_{\text{NLL}}(\mathbf{w}) = \sum_{i=1}^n \log\left(1 + e^{-y_i \cdot z_i} \right)} \quad \text{with } y_i \in \{-1, +1\}.\]</div>
<p>For linear models: <span class="math notranslate nohighlight">\(z_i=\mathbf{w}^\top \mathbf{x}_i\)</span>. For more
details, see the <a class="reference internal" href="demo_ml_losses.html#ref-demonstration-nll"><span class="std std-ref">Demonstration of Negative Log-Likelihood
(NLL)</span></a> Loss.</p>
<p>This expression is also known as the <strong>logistic loss</strong> or <strong>binary
cross-entropy</strong>. It penalizes confident but incorrect predictions very
heavily (e.g., predicting <span class="math notranslate nohighlight">\(\hat{p} = 0.99\)</span> when <span class="math notranslate nohighlight">\(y = 0\)</span>).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the logistic loss for binary classification</span>
<span class="k">def</span><span class="w"> </span><span class="nf">logistic_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">))</span>


<span class="c1"># Input range (raw scores from the model)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>

<span class="c1"># Logistic loss for y = 1 and y = 0</span>
<span class="n">loss_y1</span> <span class="o">=</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">loss_y0</span> <span class="o">=</span> <span class="n">logistic_loss</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># equivalent to logistic loss for y=0</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">loss_y1</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Logistic Loss (y = 1)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">loss_y0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Logistic Loss (y = 0)&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic Loss as a Function of Raw Score z&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Model score z&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Loss&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_12_0.png" src="../_images/linear_classification_12_0.png" />
<p><strong>Gradient Logistic or Binary Cross-Entropy Loss for Linear Models</strong></p>
<p>For linear models where <span class="math notranslate nohighlight">\(z_i=\mathbf{w}^\top \mathbf{x}_i\)</span> the
gradient of the NLL according to the coefficients vector
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is:</p>
<div class="math notranslate nohighlight">
\[\boxed{\nabla_{\mathbf{w}} \mathcal{L}_{\text{NLL}} = \sum_{i=1}^n (\sigma(\mathbf{w}^\top \mathbf{x}_i) - y_i)\mathbf{x}_i}\]</div>
<p><strong>Multiclass Classification (Softmax Output) a.k.a. Cross-Entropy Loss</strong></p>
<p>Assume <span class="math notranslate nohighlight">\(y_i \in \{1, 2, \dots, K\}\)</span>, and the model outputs softmax
probabilities <span class="math notranslate nohighlight">\(\hat{p}_{i,k} = P(y_i = k \mid x_i; \theta)\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{NLL}} = - \sum_{i=1}^n \log \hat{p}_{i, y_i}\]</div>
<p>If you use one-hot encoded labels <span class="math notranslate nohighlight">\(\mathbf{y}_i\)</span>, the NLL is also
written as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{\text{NLL}} = - \sum_{i=1}^n \sum_{k=1}^K y_{i,k} \log(\hat{p}_{i,k})\]</div>
<p>This is equivalent to the <strong>cross-entropy loss</strong> when combined with
softmax.</p>
<p><strong>Summary</strong></p>
<ul class="simple">
<li><p>Negative Log-Likelihood = general loss for probabilistic models</p></li>
<li><p>Logistic loss = binary cross-entropy</p></li>
<li><p>Cross-Entropy loss = NLL + Softmax (for multiclass problems)</p></li>
<li><p>These losses are convex (for linear models), interpretable, and widely
used in training classifiers like logistic regression and neural
networks.</p></li>
</ul>
<p>See also <a class="reference external" href="https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation">Scikit learn
doc</a></p>
</section>
</section>
<section id="hinge-loss-or-ell-1-loss">
<h3>Hinge loss or <span class="math notranslate nohighlight">\(\ell_1\)</span> loss<a class="headerlink" href="#hinge-loss-or-ell-1-loss" title="Link to this heading">¶</a></h3>
<p>TODO</p>
</section>
<section id="logistic-regression-summary">
<h3>Logistic Regression Summary<a class="headerlink" href="#logistic-regression-summary" title="Link to this heading">¶</a></h3>
<p>Logistic regression minimizes the Cross-Entropy loss i.e. NLL with
Sigmoid (binary problems) or NLL with Softmax (multiclass problems):</p>
<div class="math notranslate nohighlight">
\[\boxed{\min_{\mathbf{w}}~\text{Logistic}(\mathbf{w}) = \mathcal{L}_{\text{NLL}}(\mathbf{w})}\]</div>
</section>
<section id="logistic-regression-with-scikit-learn">
<h3>Logistic Regression with Scikit-Learn<a class="headerlink" href="#logistic-regression-with-scikit-learn" title="Link to this heading">¶</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="n">logreg</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_logreg</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_logreg</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_logreg</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">logreg</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.05</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">5.15</span>  <span class="mf">5.57</span><span class="p">]]</span>
</pre></div>
</div>
</section>
</section>
<section id="regularization-using-penalization-of-coefficients">
<h2>Regularization using penalization of coefficients<a class="headerlink" href="#regularization-using-penalization-of-coefficients" title="Link to this heading">¶</a></h2>
<p>The penalties use in regression are also used in classification. The
only difference is the loss function generally the negative log
likelihood (cross-entropy) or the hinge loss. We will explore:</p>
<section id="summary">
<h3>Summary<a class="headerlink" href="#summary" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>Ridge (also called <span class="math notranslate nohighlight">\(\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_2^2\)</span>. It shrinks coefficients toward 0.</p></li>
<li><p>Lasso (also called <span class="math notranslate nohighlight">\(\ell_1\)</span>) penalty: <span class="math notranslate nohighlight">\(\|\mathbf{w}\|_1\)</span>.
It performs feature selection by setting some coefficients to 0.</p></li>
<li><p>ElasticNet (also called <span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>) penalty:
<span class="math notranslate nohighlight">\(\alpha \left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right)\)</span>.
It performs selection of group of correlated features by setting some
coefficients to 0.</p></li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset with some correlation</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_redundant</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Logistic Regression unpenalized</span>
<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Logistic Regression with L2 penalty</span>
<span class="n">l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>

<span class="c1"># Logistic Regression with L1 penalty</span>
<span class="c1"># use solver &#39;saga&#39; to handle L1 penalty. lambda = 1 / C</span>
<span class="n">l1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Logistic Regression with L1/L2 penalties</span>
<span class="n">l1l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;elasticnet&#39;</span><span class="p">,</span>  <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                             <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda = 1 / C!</span>


<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span>
             <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l1l2&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>         <span class="mi">0</span>     <span class="mi">1</span>     <span class="mi">2</span>     <span class="mi">3</span>     <span class="mi">4</span>     <span class="mi">5</span>     <span class="mi">6</span>     <span class="mi">7</span>     <span class="mi">8</span>     <span class="mi">9</span>
<span class="n">lr</span>    <span class="mf">0.04</span>  <span class="mf">1.14</span> <span class="o">-</span><span class="mf">0.28</span>  <span class="mf">0.57</span>  <span class="mf">0.55</span> <span class="o">-</span><span class="mf">0.03</span>  <span class="mf">0.17</span>  <span class="mf">0.37</span> <span class="o">-</span><span class="mf">0.42</span>  <span class="mf">0.39</span>
<span class="n">l2</span>   <span class="o">-</span><span class="mf">0.05</span>  <span class="mf">0.52</span> <span class="o">-</span><span class="mf">0.21</span>  <span class="mf">0.34</span>  <span class="mf">0.26</span> <span class="o">-</span><span class="mf">0.05</span>  <span class="mf">0.14</span>  <span class="mf">0.27</span> <span class="o">-</span><span class="mf">0.25</span>  <span class="mf">0.21</span>
<span class="n">l1</span>    <span class="mf">0.00</span>  <span class="mf">0.31</span>  <span class="mf">0.00</span>  <span class="mf">0.10</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.26</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>
<span class="n">l1l2</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.41</span> <span class="o">-</span><span class="mf">0.15</span>  <span class="mf">0.29</span>  <span class="mf">0.12</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.20</span> <span class="o">-</span><span class="mf">0.10</span>  <span class="mf">0.06</span>
</pre></div>
</div>
</section>
<section id="understanding-the-effect-of-penalty-using-ell-2-regularization-fishers-linear-classification">
<h3>Understanding the effect of penalty using <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization Fisher’s linear classification<a class="headerlink" href="#understanding-the-effect-of-penalty-using-ell-2-regularization-fishers-linear-classification" title="Link to this heading">¶</a></h3>
<p>When the matrix <span class="math notranslate nohighlight">\(\mathbf{S_W}\)</span> is not full rank or
<span class="math notranslate nohighlight">\(P \gg N\)</span>, the The Fisher most discriminant projection estimate of
the is not unique. This can be solved using a biased version of
<span class="math notranslate nohighlight">\(\mathbf{S_W}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\mathbf{S_W}^{Ridge} = \mathbf{S_W} + \lambda \mathbf{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(I\)</span> is the <span class="math notranslate nohighlight">\(P \times P\)</span> identity matrix. This leads to
the regularized (ridge) estimator of the Fisher’s linear discriminant
analysis:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}^{Ridge} \propto (\mathbf{S_W} + \lambda \mathbf{I})^{-1}(\mathbf{\mu_1} - \mathbf{\mu_0})\]</div>
<figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/ridge_fisher_linear_disc.png"><img alt="The Ridge Fisher most discriminant projection" src="../_images/ridge_fisher_linear_disc.png" style="width: 15cm;" />
</a>
<figcaption>
<p><span class="caption-text">The Ridge Fisher most discriminant projection</span><a class="headerlink" href="#id2" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> will:</p>
<ul class="simple">
<li><p>Shrinks the coefficients toward zero.</p></li>
<li><p>The covariance will converge toward the diagonal matrix, reducing the
contribution of the pairwise covariances.</p></li>
</ul>
</section>
</section>
<section id="ell-2-regularized-logistic-regression">
<h2><span class="math notranslate nohighlight">\(\ell_2\)</span>-regularized logistic regression<a class="headerlink" href="#ell-2-regularized-logistic-regression" title="Link to this heading">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss (Negative Log Likelihood) with a penalty of the L2 norm of
the weights vector. In the two-class case, using the 0/1 coding we
obtain:</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{w}}~\text{Logistic L2}(\mathbf{w}) = \mathcal{L}_{\text{NLL}}(\mathbf{w})
 + \lambda~\|\mathbf{w}\|^2\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="n">lrl2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="c1"># This class implements regularized logistic regression.</span>
<span class="c1"># C is the Inverse of regularization strength. Large value =&gt; no regularization.</span>

<span class="n">lrl2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_l2</span> <span class="o">=</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">prob_pred_l2</span> <span class="o">=</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Probas of 5 first samples for class 0 and class 1:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">prob_pred_l2</span><span class="p">[:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lrl2</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>

<span class="c1"># Retrieve proba from coef vector</span>
<span class="n">probas</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="n">lrl2</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)))</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Diff&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">prob_pred_l2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">probas</span><span class="p">)))</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_l2</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Probas of 5 first samples for class 0 and class 1:
[[0.89 0.11]
 [0.72 0.28]
 [0.73 0.27]
 [0.75 0.25]
 [0.48 0.52]]
Coef vector:
[[-0.05  0.52 -0.21  0.34  0.26 -0.05  0.14  0.27 -0.25  0.21]]
Diff 0.0
Nb errors=24, error rate=0.24
</pre></div>
</div>
</section>
<section id="lasso-logistic-regression-ell-1-regularization">
<h2>Lasso logistic regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-logistic-regression-ell-1-regularization" title="Link to this heading">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of the
logistic loss <span class="math notranslate nohighlight">\(-\log \mathcal{L}(\mathbf{w})\)</span> with a penalty of
the L1 norm of the weights vector. In the two-class case, using the 0/1
coding we obtain:</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{w}}~\text{Logistic Lasso}(\mathbf{w}) = \mathcal{L}_{\text{NLL}}(\mathbf{w})
 + \lambda~\|\mathbf{w}\|_1\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="n">lrl1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;saga&#39;</span><span class="p">)</span> <span class="c1"># lambda = 1 / C!</span>

<span class="c1"># This class implements regularized logistic regression. C is the Inverse of regularization strength.</span>
<span class="c1"># Large value =&gt; no regularization.</span>

<span class="n">lrl1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_lrl1</span> <span class="o">=</span> <span class="n">lrl1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_lrl1</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_lrl1</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">lrl1</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">27</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.27</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="mf">0.</span>   <span class="mf">0.31</span> <span class="mf">0.</span>   <span class="mf">0.1</span>  <span class="mf">0.</span>   <span class="mf">0.</span>   <span class="mf">0.</span>   <span class="mf">0.26</span> <span class="mf">0.</span>   <span class="mf">0.</span>  <span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="linear-support-vector-machine-ell-2-regularization-with-hinge-loss">
<h2>Linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization with Hinge loss)<a class="headerlink" href="#linear-support-vector-machine-ell-2-regularization-with-hinge-loss" title="Link to this heading">¶</a></h2>
<p>Support Vector Machine seek for separating hyperplane with maximum
margin to enforce robustness against noise. Like logistic regression it
is a <strong>discriminative method</strong> that only focuses of predictions.</p>
<p>Here we present the non separable case of Maximum Margin Classifiers
with <span class="math notranslate nohighlight">\(\pm 1\)</span> coding (ie.: <span class="math notranslate nohighlight">\(y_i \ \{-1, +1\}\)</span>).</p>
<figure class="align-default" id="id3">
<img alt="Linear lar margin classifiers" src="../_images/svm.png" />
<figcaption>
<p><span class="caption-text">Linear lar margin classifiers</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Linear SVM for classification (also called SVM-C or SVC) minimizes:</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{w}}~\text{Linear SVM}(\mathbf{w}) = \|\mathbf{w}\|_2^2 +  C~\sum_i^n\max(0, 1 - y_i~ (\mathbf{w} \cdot \mathbf{x_i}))\]</div>
<p>where <span class="math notranslate nohighlight">\(\max(0, 1 - y_i~ (\mathbf{w} \cdot \mathbf{x_i})\)</span> is the
<strong>hinge loss</strong>.</p>
<p>Here we introduced the slack variables: <span class="math notranslate nohighlight">\(\xi_i\)</span>, with
<span class="math notranslate nohighlight">\(\xi_i = 0\)</span> for points that are on or inside the correct margin
boundary and <span class="math notranslate nohighlight">\(\xi_i = |y_i - (w \ cdot  \cdot \mathbf{x_i})|\)</span> for
other points. Thus:</p>
<ol class="arabic simple">
<li><p>If <span class="math notranslate nohighlight">\(y_i (\mathbf{w} \cdot \mathbf{x_i}) \geq 1\)</span> then the point
lies outside the margin but on the correct side of the decision
boundary. In this case the constraint is thus not active for this
point. It does not contribute to the prediction.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(1 &gt; y_i (\mathbf{w} \cdot \mathbf{x_i}) \geq 0\)</span> then the
point lies inside the margin and on the correct side of the decision
boundary. In this case the constraint is active for this point. It
does contribute to the prediction as a support vector.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(0 &lt;  y_i (\mathbf{w} \cdot \mathbf{x_i})\)</span>) then the point
is on the wrong side of the decision boundary (missclassification).
In this case the constraint is active for this point. It does
contribute to the prediction as a support vector.</p></li>
</ol>
<p>So linear SVM is closed to Ridge logistic regression, using the hinge
loss instead of the logistic loss. Both will provide very similar
predictions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">svm</span>

<span class="n">svmlin</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span>
<span class="c1"># Remark: by default LinearSVC uses squared_hinge as loss</span>
<span class="n">svmlin</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_svmlin</span> <span class="o">=</span> <span class="n">svmlin</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_svmlin</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_svmlin</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svmlin</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.20</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.</span>    <span class="mf">0.32</span> <span class="o">-</span><span class="mf">0.09</span>  <span class="mf">0.17</span>  <span class="mf">0.16</span> <span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.06</span>  <span class="mf">0.13</span> <span class="o">-</span><span class="mf">0.16</span>  <span class="mf">0.13</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="lasso-linear-support-vector-machine-ell-1-regularization">
<h2>Lasso linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)<a class="headerlink" href="#lasso-linear-support-vector-machine-ell-1-regularization" title="Link to this heading">¶</a></h2>
<p>Linear SVM with l1-regularization:</p>
<div class="math notranslate nohighlight">
\[\min_{\mathbf{w}}~\text{Lasso Linear SVM}(\mathbf{w}) = \|\mathbf{w}\|_1 +  C~\sum_i^n\max(0, 1 - y_i~ (\mathbf{w} \cdot \mathbf{x_i}))\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">svm</span>

<span class="n">svmlinl1</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">dual</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1"># Remark: by default LinearSVC uses squared_hinge as loss</span>

<span class="n">svmlinl1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred_svmlinl1</span> <span class="o">=</span> <span class="n">svmlinl1</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">errors</span> <span class="o">=</span>  <span class="n">y_pred_svmlinl1</span> <span class="o">!=</span> <span class="n">y</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Nb errors=</span><span class="si">%i</span><span class="s2">, error rate=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">errors</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_pred_svmlinl1</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coef vector:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svmlinl1</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Nb</span> <span class="n">errors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">error</span> <span class="n">rate</span><span class="o">=</span><span class="mf">0.20</span>
<span class="n">Coef</span> <span class="n">vector</span><span class="p">:</span>
<span class="p">[[</span><span class="o">-</span><span class="mf">0.01</span>  <span class="mf">0.37</span> <span class="o">-</span><span class="mf">0.12</span>  <span class="mf">0.24</span>  <span class="mf">0.17</span>  <span class="mf">0.</span>    <span class="mf">0.</span>    <span class="mf">0.1</span>  <span class="o">-</span><span class="mf">0.16</span>  <span class="mf">0.13</span><span class="p">]]</span>
</pre></div>
</div>
</section>
<section id="elastic-net-classification-ell-1-ell-2-regularization">
<h2>Elastic-net classification (<span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>-regularization)<a class="headerlink" href="#elastic-net-classification-ell-1-ell-2-regularization" title="Link to this heading">¶</a></h2>
<p>The <strong>objective function</strong> to be minimized is now the combination of a
loss with combination of L1 and L2 penalties:</p>
<ul>
<li><p>For the NLL loss:</p>
<div class="math notranslate nohighlight">
\[\min~\text{Logistic Enet}(\mathbf{w}) = \mathcal{L}_{\text{NLL}}(\mathbf{w}) + \alpha~\left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right)\]</div>
</li>
<li><p>For the Hinge loss</p>
<div class="math notranslate nohighlight">
\[\min~\text{Hinge Enet}(\mathbf{w}) = \text{Hinge loss}(\mathbf{w}) + \alpha~\left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right)\]</div>
</li>
</ul>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use SGD solver</span>
<span class="n">enetlog</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;log_loss&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">enetlog</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Or saga solver:</span>
<span class="c1"># enetloglike = lm.LogisticRegression(penalty=&#39;elasticnet&#39;,</span>
<span class="c1">#                                    C=.1, l1_ratio=0.5, solver=&#39;saga&#39;)</span>

<span class="n">enethinge</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;elasticnet&quot;</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">enethinge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Hinge loss and logistic loss provide almost the same predictions.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion matrix&quot;</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">enetlog</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">enethinge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Decision_function log x hinge losses:&quot;</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">enetlog</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span>
             <span class="n">enethinge</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="s2">&quot;o&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Hinge</span> <span class="n">loss</span> <span class="ow">and</span> <span class="n">logistic</span> <span class="n">loss</span> <span class="n">provide</span> <span class="n">almost</span> <span class="n">the</span> <span class="n">same</span> <span class="n">predictions</span><span class="o">.</span>
<span class="n">Confusion</span> <span class="n">matrix</span>
<span class="n">Decision_function</span> <span class="n">log</span> <span class="n">x</span> <span class="n">hinge</span> <span class="n">losses</span><span class="p">:</span>
</pre></div>
</div>
<img alt="../_images/linear_classification_28_1.png" src="../_images/linear_classification_28_1.png" />
</section>
<section id="classification-performance-evaluation-metrics">
<h2>Classification performance evaluation metrics<a class="headerlink" href="#classification-performance-evaluation-metrics" title="Link to this heading">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">Wikipedia Sensitivity and
specificity</a></p>
<p>Imagine a study evaluating a new test that screens people for a disease.
Each person taking the test either has or does not have the disease. The
test outcome can be positive (classifying the person as having the
disease) or negative (classifying the person as not having the disease).
The test results for each subject may or may not match the subject’s
actual status. In that setting:</p>
<ul>
<li><p>True positive (TP): Sick people correctly identified as sick</p></li>
<li><p>False positive (FP): Healthy people incorrectly identified as sick</p></li>
<li><p>True negative (TN): Healthy people correctly identified as healthy</p></li>
<li><p>False negative (FN): Sick people incorrectly identified as healthy</p></li>
<li><p><strong>Accuracy</strong> (ACC):</p>
<p>ACC = (TP + TN) / (TP + FP + FN + TN)</p>
</li>
<li><p><strong>Sensitivity</strong> (SEN) or <strong>recall</strong> of the positive class or true
positive rate (TPR) or hit rate:</p>
<p>SEN = TP / P = TP / (TP+FN)</p>
</li>
<li><p><strong>Specificity</strong> (SPC) or <strong>recall</strong> of the negative class or true
negative rate:</p>
<p>SPC = TN / N = TN / (TN+FP)</p>
</li>
<li><p><strong>Precision</strong> or positive predictive value (PPV):</p>
<p>PPV = TP / (TP + FP)</p>
</li>
<li><p><strong>Balanced accuracy</strong> (bACC):is a useful performance measure is the
balanced accuracy which avoids inflated performance estimates on
imbalanced datasets (Brodersen, et al. (2010). “The balanced accuracy
and its posterior distribution”). It is defined as the arithmetic mean
of sensitivity and specificity, or the average accuracy obtained on
either class:</p>
<p>bACC = 1/2 * (SEN + SPC)</p>
</li>
<li><p>F1 Score (or F-score) which is a weighted average of precision and
recall are useful to deal with imbalanced datasets</p></li>
</ul>
<p>The four outcomes can be formulated in a 2×2 contingency table or
<a class="reference external" href="https://en.wikipedia.org/wiki/Sensitivity_and_specificity">confusion
matrix</a></p>
<p>For more precision see <a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html">Scikit-learn
doc</a></p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># The overall precision an recall</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># Recalls on individual classes: SEN &amp; SPC</span>
<span class="n">recalls</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">recalls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># is the recall of class 0: specificity</span>
<span class="n">recalls</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># is the recall of class 1: sensitivity</span>

<span class="c1"># Balanced accuracy</span>
<span class="n">b_acc</span> <span class="o">=</span> <span class="n">recalls</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># The overall precision an recall on each individual class</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
<section id="area-under-curve-auc-of-receiver-operating-characteristic-roc">
<h3>Area Under Curve (AUC) of Receiver operating characteristic (ROC)<a class="headerlink" href="#area-under-curve-auc-of-receiver-operating-characteristic-roc" title="Link to this heading">¶</a></h3>
<p>Some classifier may have found a good discriminative projection
<span class="math notranslate nohighlight">\(w\)</span>. However if the threshold to decide the final predicted class
is poorly adjusted, the performances will highlight an high specificity
and a low sensitivity or the contrary.</p>
<p>In this case it is recommended to use the AUC of a ROC analysis which
basically provide a measure of overlap of the two classes when points
are projected on the discriminative axis. See <a class="reference external" href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">Wikipedia: ROC and
AUC</a>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">score_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">.1</span><span class="p">,</span> <span class="mf">.2</span><span class="p">,</span> <span class="mf">.3</span><span class="p">,</span> <span class="mf">.4</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mf">.6</span><span class="p">,</span> <span class="mf">.7</span><span class="p">,</span> <span class="mf">.8</span><span class="p">])</span>
<span class="n">y_true</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">thres</span> <span class="o">=</span> <span class="mf">.9</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="p">(</span><span class="n">score_pred</span> <span class="o">&gt;</span> <span class="n">thres</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;With a threshold of </span><span class="si">%.2f</span><span class="s2">, the rule always predict 0. Predictions:&quot;</span> <span class="o">%</span> <span class="n">thres</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="c1"># The overall precision an recall on each individual class</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Recalls on individual classes are:&quot;</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span>
      <span class="s2">&quot;ie, 100</span><span class="si">% o</span><span class="s2">f specificity, 0</span><span class="si">% o</span><span class="s2">f sensitivity&quot;</span><span class="p">)</span>

<span class="c1"># However AUC=1 indicating a perfect separation of the two classes</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">score_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;But the AUC of </span><span class="si">%.2f</span><span class="s2"> demonstrate a good classes separation.&quot;</span> <span class="o">%</span> <span class="n">auc</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">With</span> <span class="n">a</span> <span class="n">threshold</span> <span class="n">of</span> <span class="mf">0.90</span><span class="p">,</span> <span class="n">the</span> <span class="n">rule</span> <span class="n">always</span> <span class="n">predict</span> <span class="mf">0.</span> <span class="n">Predictions</span><span class="p">:</span>
<span class="p">[</span><span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">Recalls</span> <span class="n">on</span> <span class="n">individual</span> <span class="n">classes</span> <span class="n">are</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.</span> <span class="mf">0.</span><span class="p">]</span> <span class="n">ie</span><span class="p">,</span> <span class="mi">100</span><span class="o">%</span> <span class="n">of</span> <span class="n">specificity</span><span class="p">,</span> <span class="mi">0</span><span class="o">%</span> <span class="n">of</span> <span class="n">sensitivity</span>
<span class="n">But</span> <span class="n">the</span> <span class="n">AUC</span> <span class="n">of</span> <span class="mf">1.00</span> <span class="n">demonstrate</span> <span class="n">a</span> <span class="n">good</span> <span class="n">classes</span> <span class="n">separation</span><span class="o">.</span>
</pre></div>
</div>
</section>
</section>
<section id="imbalanced-classes">
<h2>Imbalanced classes<a class="headerlink" href="#imbalanced-classes" title="Link to this heading">¶</a></h2>
<p>Learning with discriminative (logistic regression, SVM) methods is
generally based on minimizing the misclassification of training samples,
which may be unsuitable for imbalanced datasets where the recognition
might be biased in favor of the most numerous class. This problem can be
addressed with a generative approach, which typically requires more
parameters to be determined leading to reduced performances in high
dimension.</p>
<p>Dealing with imbalanced class may be addressed by three main ways (see
Japkowicz and Stephen (2002) for a review), resampling, reweighting and
one class learning.</p>
<p>In <strong>sampling strategies</strong>, either the minority class is oversampled or
majority class is undersampled or some combination of the two is
deployed. Undersampling (Zhang and Mani, 2003) the majority class would
lead to a poor usage of the left-out samples. Sometime one cannot afford
such strategy since we are also facing a small sample size problem even
for the majority class. Informed oversampling, which goes beyond a
trivial duplication of minority class samples, requires the estimation
of class conditional distributions in order to generate synthetic
samples. Here generative models are required. An alternative, proposed
in (Chawla et al., 2002) generate samples along the line segments
joining any/all of the k minority class nearest neighbors. Such
procedure blindly generalizes the minority area without regard to the
majority class, which may be particularly problematic with
high-dimensional and potentially skewed class distribution.</p>
<p><strong>Reweighting</strong>, also called cost-sensitive learning, works at an
algorithmic level by adjusting the costs of the various classes to
counter the class imbalance. Such reweighting can be implemented within
SVM (Chang and Lin, 2001) or logistic regression (Friedman et al., 2010)
classifiers. Most classifiers of Scikit learn offer such reweighting
possibilities.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">class_weight</span></code> parameter can be positioned into the <code class="docutils literal notranslate"><span class="pre">&quot;balanced&quot;</span></code>
mode which uses the values of <span class="math notranslate nohighlight">\(y\)</span> to automatically adjust weights
inversely proportional to class frequencies in the input data as
<span class="math notranslate nohighlight">\(N / (2 N_k)\)</span>.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># dataset</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
                           <span class="n">n_features</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                           <span class="n">n_informative</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">n_redundant</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">n_repeated</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                           <span class="n">n_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="s2">&quot;#samples of class </span><span class="si">%i</span><span class="s2"> = </span><span class="si">%i</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">lev</span><span class="p">))</span>
      <span class="k">for</span> <span class="n">lev</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# No Reweighting balanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_inter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">lr_inter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; The predictions are balanced in sensitivity and specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="c1"># Create imbalanced dataset, by subsampling sample of class 0: keep only 10% of</span>
<span class="c1">#  class 0&#39;s samples and all class 1&#39;s samples.</span>
<span class="n">n0</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">rint</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="o">/</span> <span class="mi">20</span><span class="p">))</span>
<span class="n">subsample_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">][:</span><span class="n">n0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
<span class="n">Ximb</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">subsample_idx</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">yimb</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">subsample_idx</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="s2">&quot;#samples of class </span><span class="si">%i</span><span class="s2"> = </span><span class="si">%i</span><span class="s2">;&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">lev</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">yimb</span> <span class="o">==</span> <span class="n">lev</span><span class="p">))</span> <span class="k">for</span> <span class="n">lev</span> <span class="ow">in</span>
        <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">yimb</span><span class="p">)])</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# No Reweighting on imbalanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">lr_inter</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Ximb</span><span class="p">,</span> <span class="n">yimb</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span>
    <span class="n">yimb</span><span class="p">,</span> <span class="n">lr_inter</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Ximb</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; Sensitivity &gt;&gt; specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# Reweighting on imbalanced dataset&#39;</span><span class="p">)</span>
<span class="n">lr_inter_reweight</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="s2">&quot;balanced&quot;</span><span class="p">)</span>
<span class="n">lr_inter_reweight</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Ximb</span><span class="p">,</span> <span class="n">yimb</span><span class="p">)</span>
<span class="n">p</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">precision_recall_fscore_support</span><span class="p">(</span><span class="n">yimb</span><span class="p">,</span>
                                                     <span class="n">lr_inter_reweight</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Ximb</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;SPC: </span><span class="si">%.3f</span><span class="s2">; SEN: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">r</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;# =&gt; The predictions are balanced in sensitivity and specificity</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#samples of class 0 = 250; #samples of class 1 = 250;</span>
<span class="c1"># No Reweighting balanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">0.940</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.928</span>
<span class="c1"># =&gt; The predictions are balanced in sensitivity and specificity</span>

<span class="c1">#samples of class 0 = 12; #samples of class 1 = 250;</span>
<span class="c1"># No Reweighting on imbalanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">0.750</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.996</span>
<span class="c1"># =&gt; Sensitivity &gt;&gt; specificity</span>

<span class="c1"># Reweighting on imbalanced dataset</span>
<span class="n">SPC</span><span class="p">:</span> <span class="mf">1.000</span><span class="p">;</span> <span class="n">SEN</span><span class="p">:</span> <span class="mf">0.980</span>
<span class="c1"># =&gt; The predictions are balanced in sensitivity and specificity</span>
</pre></div>
</div>
</section>
<section id="confidence-interval-cross-validation">
<h2>Confidence interval cross-validation<a class="headerlink" href="#confidence-interval-cross-validation" title="Link to this heading">¶</a></h2>
<p>Confidence interval CI classification accuracy measured by
cross-validation: <img alt="CI classification" src="../_images/classif_accuracy_95ci_sizes.png" /></p>
</section>
<section id="significance-of-classification-metrics">
<h2>Significance of classification metrics<a class="headerlink" href="#significance-of-classification-metrics" title="Link to this heading">¶</a></h2>
<p><strong>P-value of classification accuracy:</strong> Compare the number of correct
classifications (=accuracy <span class="math notranslate nohighlight">\(\times N\)</span>) to the null hypothesis of
Binomial distribution of parameters <span class="math notranslate nohighlight">\(p\)</span> (typically 50% of chance
level) and <span class="math notranslate nohighlight">\(N\)</span> (Number of observations). Is 60% accuracy a
significant prediction rate among 50 observations? Since this is an
exact, <strong>two-sided</strong> test of the null hypothesis, the p-value can be
divided by two since we test that the accuracy is superior to the chance
level.</p>
<p><strong>P-value of ROC-AUC:</strong> ROC-AUC measures the ranking of the two classes.
Therefore non-parametric test can be used to asses the significance of
the classes’s separation. Mason and Graham (RMetS, 2002) show that the
ROC area is equivalent to the Mann–Whitney U-statistic.</p>
<p>Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon, Wilcoxon
rank-sum test or Wilcoxon–Mann–Whitney test) is a nonparametric</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="n">N_test</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_classification</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                           <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">class_sep</span><span class="o">=</span><span class="mf">0.80</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                        <span class="n">test_size</span><span class="o">=</span><span class="n">N_test</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">proba_pred</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">acc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">bacc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">balanced_accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">auc</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">roc_auc_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">proba_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ACC=</span><span class="si">%.2f</span><span class="s2">, bACC=</span><span class="si">%.2f</span><span class="s2">, AUC=</span><span class="si">%.2f</span><span class="s2">,&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">bacc</span><span class="p">,</span> <span class="n">auc</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ACC</span><span class="o">=</span><span class="mf">0.60</span><span class="p">,</span> <span class="n">bACC</span><span class="o">=</span><span class="mf">0.61</span><span class="p">,</span> <span class="n">AUC</span><span class="o">=</span><span class="mf">0.72</span><span class="p">,</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># acc, N = 0.65, 70</span>
<span class="n">k</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">acc</span> <span class="o">*</span> <span class="n">N_test</span><span class="p">)</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binomtest</span><span class="p">(</span><span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">N_test</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">alternative</span><span class="o">=</span><span class="s1">&#39;greater&#39;</span><span class="p">)</span>
<span class="n">auc_pval</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">mannwhitneyu</span><span class="p">(</span>
    <span class="n">proba_pred</span><span class="p">[</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">0</span><span class="p">],</span> <span class="n">proba_pred</span><span class="p">[</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">pvalue</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_significant</span><span class="p">(</span><span class="n">pval</span><span class="p">):</span> <span class="k">return</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">pval</span> <span class="o">&lt;</span> <span class="mf">0.05</span> <span class="k">else</span> <span class="kc">False</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ACC=</span><span class="si">%.2f</span><span class="s2"> (pval=</span><span class="si">%.3f</span><span class="s2">, significance=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">acc_test</span><span class="o">.</span><span class="n">pvalue</span><span class="p">,</span> <span class="n">is_significant</span><span class="p">(</span><span class="n">acc_test</span><span class="o">.</span><span class="n">pvalue</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;AUC=</span><span class="si">%.2f</span><span class="s2"> (pval=</span><span class="si">%.3f</span><span class="s2">, significance=</span><span class="si">%r</span><span class="s2">)&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">auc</span><span class="p">,</span> <span class="n">auc_pval</span><span class="p">,</span> <span class="n">is_significant</span><span class="p">(</span><span class="n">auc_pval</span><span class="p">)))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ACC</span><span class="o">=</span><span class="mf">0.60</span> <span class="p">(</span><span class="n">pval</span><span class="o">=</span><span class="mf">0.101</span><span class="p">,</span> <span class="n">significance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">AUC</span><span class="o">=</span><span class="mf">0.72</span> <span class="p">(</span><span class="n">pval</span><span class="o">=</span><span class="mf">0.009</span><span class="p">,</span> <span class="n">significance</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="exercise">
<h2>Exercise<a class="headerlink" href="#exercise" title="Link to this heading">¶</a></h2>
<section id="fisher-linear-discriminant-rule">
<h3>Fisher linear discriminant rule<a class="headerlink" href="#fisher-linear-discriminant-rule" title="Link to this heading">¶</a></h3>
<p>Write a class <code class="docutils literal notranslate"><span class="pre">FisherLinearDiscriminant</span></code> that implements the Fisher’s
linear discriminant analysis. This class must be compliant with the
scikit-learn API by providing two methods: - <code class="docutils literal notranslate"><span class="pre">fit(X,</span> <span class="pre">y)</span></code> which fits
the model and returns the object itself; - <code class="docutils literal notranslate"><span class="pre">predict(X)</span></code> which returns
a vector of the predicted values. Apply the object on the dataset
presented for the LDA.</p>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Linear Models for Classification</a><ul>
<li><a class="reference internal" href="#geometric-method-naive-method">Geometric Method: Naive Method</a></li>
<li><a class="reference internal" href="#geometric-method-fishers-linear-discriminant">Geometric Method: Fisher’s Linear Discriminant</a></li>
<li><a class="reference internal" href="#generative-model-linear-discriminant-analysis-lda">Generative Model: Linear Discriminant Analysis (LDA)</a></li>
<li><a class="reference internal" href="#logistic-regression">Logistic Regression</a><ul>
<li><a class="reference internal" href="#activation-functions-for-classification-sigmoid-and-softmax">Activation Functions for Classification (Sigmoid and Softmax)</a></li>
<li><a class="reference internal" href="#loss-functions-for-classification">Loss Functions for Classification</a><ul>
<li><a class="reference internal" href="#negative-log-likelihood-nll">Negative Log-Likelihood (NLL)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#hinge-loss-or-ell-1-loss">Hinge loss or <span class="math notranslate nohighlight">\(\ell_1\)</span> loss</a></li>
<li><a class="reference internal" href="#logistic-regression-summary">Logistic Regression Summary</a></li>
<li><a class="reference internal" href="#logistic-regression-with-scikit-learn">Logistic Regression with Scikit-Learn</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regularization-using-penalization-of-coefficients">Regularization using penalization of coefficients</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#understanding-the-effect-of-penalty-using-ell-2-regularization-fishers-linear-classification">Understanding the effect of penalty using <span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization Fisher’s linear classification</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ell-2-regularized-logistic-regression"><span class="math notranslate nohighlight">\(\ell_2\)</span>-regularized logistic regression</a></li>
<li><a class="reference internal" href="#lasso-logistic-regression-ell-1-regularization">Lasso logistic regression (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li><a class="reference internal" href="#linear-support-vector-machine-ell-2-regularization-with-hinge-loss">Linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_2\)</span>-regularization with Hinge loss)</a></li>
<li><a class="reference internal" href="#lasso-linear-support-vector-machine-ell-1-regularization">Lasso linear Support Vector Machine (<span class="math notranslate nohighlight">\(\ell_1\)</span>-regularization)</a></li>
<li><a class="reference internal" href="#elastic-net-classification-ell-1-ell-2-regularization">Elastic-net classification (<span class="math notranslate nohighlight">\(\ell_1\ell_2\)</span>-regularization)</a></li>
<li><a class="reference internal" href="#classification-performance-evaluation-metrics">Classification performance evaluation metrics</a><ul>
<li><a class="reference internal" href="#area-under-curve-auc-of-receiver-operating-characteristic-roc">Area Under Curve (AUC) of Receiver operating characteristic (ROC)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#imbalanced-classes">Imbalanced classes</a></li>
<li><a class="reference internal" href="#confidence-interval-cross-validation">Confidence interval cross-validation</a></li>
<li><a class="reference internal" href="#significance-of-classification-metrics">Significance of classification metrics</a></li>
<li><a class="reference internal" href="#exercise">Exercise</a><ul>
<li><a class="reference internal" href="#fisher-linear-discriminant-rule">Fisher linear discriminant rule</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml_supervised/linear_classification.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/ml_supervised/linear_classification.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>