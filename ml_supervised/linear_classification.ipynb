{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Classification\n",
    "\n",
    "Linear vs. non-linear classifier: See [Scikit-learn Classifier comparison](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html#sphx-glr-auto-examples-classification-plot-classifier-comparison-py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Method: Naive Method\n",
    "\n",
    "Principles:\n",
    "\n",
    "- Compute classes means $\\mathbf{\\mu}_1$, $\\mathbf{\\mu}_2, \\mathbf{\\mu}_k, \\ldots$\n",
    "- Classify new point $\\mathbf{x}$ to the closest mean, i.e.: class $\\arg \\min_k \\|\\mathbf{x} - \\mathbf{\\mu}_k\\|_2$\n",
    "\n",
    "For binary classification, this is equivalent to compute the most discriminative direction as the vector between class mean:\n",
    "$$\n",
    "\\boxed{\\mathbf{w}_{\\text{naive}} = \\mathbf{\\mu}_1 - \\mathbf{\\mu}_2}\n",
    "$$\n",
    "\n",
    "And projecting a new point $\\mathbf{x_i}$ along this direction to obtain a score $z_i$:\n",
    "$$\n",
    "z_i = \\mathbf{x_i}^\\top \\mathbf{w}_{\\text{naive}}\n",
    "$$\n",
    "\n",
    "Finally the Classify the point to the closest projected class mean.\n",
    "\n",
    "Illustration:\n",
    "\n",
    "![Most discriminant projections, Naive and Fisher methods](images/fisher_linear_disc.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometric Method: Fisher's Linear Discriminant\n",
    "\n",
    "Principles:\n",
    "\n",
    "- Dimensionality reduction before later classification.\n",
    "- Find the most discriminant axis.\n",
    "- Taking account the distribution, assuming same normal distribution for all classes.\n",
    "\n",
    "Simply compute the **within class covariance** $\\mathbf{S_W}$ to rotate the projection direction according to the point (elliptic) distribution:\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathbf{w}_{\\text{Fisher}} = \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0})}.\n",
    "$$\n",
    "\n",
    "This geometric method does not make any probabilistic assumptions, instead it relies on distances. It looks for the **linear projection** of the data points onto a vector, $\\mathbf{w}$, that maximizes the between/within variance ratio, denoted $F(\\mathbf{w})$. Under a few assumptions, it will provide the same results as linear discriminant analysis (LDA), explained below.\n",
    "\n",
    "Suppose two classes of observations, $C_0$ and $C_1$, have means $\\mathbf{\\mu_0}$ and $\\mathbf{\\mu_1}$ and the same total within-class scatter (\"covariance\") matrix,\n",
    "\n",
    "$$\n",
    "    \\mathbf{S_W} &= \\sum_{i\\in C_0} (\\mathbf{x_i} - \\mathbf{\\mu_0})(\\mathbf{x_i} - \\mathbf{\\mu_0})^T + \\sum_{j\\in C_1} (\\mathbf{x_j} - \\mathbf{\\mu_1})(\\mathbf{x_j} -\\mathbf{\\mu_1})^T\\\\\n",
    "        &= \\mathbf{X_c}^T \\mathbf{X_c},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X_c}$ is the $(N \\times P)$ matrix of data centered on their respective means:\n",
    "\n",
    "$$\n",
    "\\mathbf{X_c} = \\begin{bmatrix}\n",
    "          \\mathbf{X_0} -  \\mathbf{\\mu_0} \\\\\n",
    "          \\mathbf{X_1} -  \\mathbf{\\mu_1} \n",
    "      \\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X_0}$ and $\\mathbf{X_1}$ are the $(N_0 \\times P)$ and $(N_1 \\times P)$ matrices of samples of classes $C_0$ and $C_1$.\n",
    "\n",
    "Let $\\mathbf{S_B}$ being the scatter \"between-class\" matrix, given by\n",
    "\n",
    "$$\n",
    "    \\mathbf{S_B} = (\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )(\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )^T.\n",
    "$$\n",
    "\n",
    "The linear combination of features $\\mathbf{w}^T x$ have means $\\mathbf{w}^T \\mu_i$ for $i=0,1$, and variance $\\mathbf{w}^T \n",
    "\\mathbf{X^T_c} \\mathbf{X_c} \\mathbf{w}$. Fisher defined the separation between these two distributions to be the ratio of the \n",
    "variance between the classes to the variance within the classes:\n",
    "\n",
    "$$\n",
    "F_{\\text{Fisher}}(\\mathbf{w}) &= \\frac{\\sigma_{\\text{between}}^2}{\\sigma_{\\text{within}}^2}\\\\\n",
    "                     &= \\frac{(\\mathbf{w}^T \\mathbf{\\mu_1} - \\mathbf{w}^T \\mathbf{\\mu_0})^2}{\\mathbf{w}^T  X^T_c \\mathbf{X_c} \\mathbf{w}}\\\\\n",
    "                     &= \\frac{(\\mathbf{w}^T (\\mathbf{\\mu_1} - \\mathbf{\\mu_0}))^2}{\\mathbf{w}^T  X^T_c \\mathbf{X_c} \\mathbf{w}}\\\\ \n",
    "                     &= \\frac{\\mathbf{w}^T (\\mathbf{\\mu_1} - \\mathbf{\\mu_0}) (\\mathbf{\\mu_1} - \\mathbf{\\mu_0})^T w}{\\mathbf{w}^T X^T_c \\mathbf{X_c} \\mathbf{w}}\\\\\n",
    "                     &= \\frac{\\mathbf{w}^T \\mathbf{S_B} w}{\\mathbf{w}^T \\mathbf{S_W} \\mathbf{w}}.\n",
    "$$\n",
    "\n",
    "\n",
    "In the two-class case, the maximum separation occurs by a projection on the $(\\mathbf{\\mu_1} - \\mathbf{\\mu_0})$ using the Mahalanobis \n",
    "metric $\\mathbf{S_W}^{-1}$, so that\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathbf{w} \\propto \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0})}.\n",
    "$$\n",
    "\n",
    "**Demonstration**\n",
    "\n",
    "Differentiating $F_{\\text{Fisher}}(w)$ with respect to $w$ gives\n",
    "\n",
    "$$\n",
    "    \\nabla_{\\mathbf{w}}F_{\\text{Fisher}}(\\mathbf{w}) &= 0\\\\\n",
    "    \\nabla_{\\mathbf{w}}\\left(\\frac{\\mathbf{w}^T \\mathbf{S_B} w}{\\mathbf{w}^T \\mathbf{S_W} \\mathbf{w}}\\right) &= 0\\\\\n",
    "    (\\mathbf{w}^T \\mathbf{S_W} \\mathbf{w})(2 \\mathbf{S_B} \\mathbf{w}) - (\\mathbf{w}^T \\mathbf{S_B} \\mathbf{w})(2 \\mathbf{S_W} \\mathbf{w}) &= 0\\\\\n",
    "    (\\mathbf{w}^T \\mathbf{S_W} \\mathbf{w})(\\mathbf{S_B} \\mathbf{w}) &= (\\mathbf{w}^T \\mathbf{S_B} \\mathbf{w})(\\mathbf{S_W} \\mathbf{w})\\\\\n",
    "    \\mathbf{S_B} \\mathbf{w} &= \\frac{\\mathbf{w}^T \\mathbf{S_B} \\mathbf{w}}{\\mathbf{w}^T \\mathbf{S_W} \\mathbf{w}}(\\mathbf{S_W} \\mathbf{w})\\\\\n",
    "    \\mathbf{S_B} \\mathbf{w} &= \\lambda (\\mathbf{S_W} \\mathbf{w})\\\\\n",
    "    \\mathbf{S_W}^{-1}{\\mathbf{S_B}} \\mathbf{w} &= \\lambda  \\mathbf{w}.\n",
    "$$\n",
    "\n",
    "Since we do not care about the magnitude of $\\mathbf{w}$, only its direction, we replaced the scalar factor $(\\mathbf{w}^T \\mathbf{S_B} \\mathbf{w}) / (\\mathbf{w}^T \\mathbf{S_W} \\mathbf{w})$ by $\\lambda$. \n",
    "\n",
    "In the multiple-class case, the solutions $w$ are determined by the eigenvectors of $\\mathbf{S_W}^{-1}{\\mathbf{S_B}}$ that correspond to the $K-1$ largest eigenvalues.\n",
    "\n",
    "However, in the two-class case (in which $\\mathbf{S_B} = (\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )(\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )^T$) it is easy to show that $\\mathbf{w} = \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0})$ is the unique eigenvector of $\\mathbf{S_W}^{-1}{\\mathbf{S_B}}$:\n",
    "\n",
    "$$\n",
    "    \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )(\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )^T \\mathbf{w} &= \\lambda  \\mathbf{w}\\\\\n",
    "    \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )(\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )^T \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0}) &= \\lambda  \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0}),\n",
    "$$\n",
    "\n",
    "where here $\\lambda = (\\mathbf{\\mu_1} - \\mathbf{\\mu_0} )^T \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0})$. Which leads to the result\n",
    "\n",
    "$$\n",
    "\\mathbf{w} \\propto \\mathbf{S_W}^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0}).\n",
    "$$\n",
    "\n",
    "**The separating hyperplane**\n",
    "\n",
    "The separating hyperplane is a $P-1$-dimensional hyper surface, orthogonal to the projection vector, $w$. There is no single best way to find the origin of the plane along $w$, or equivalently the classification threshold that determines whether a point should be classified as belonging to $C_0$ or to $C_1$. However, if the projected points have roughly the same distribution, then the threshold can be chosen as the hyperplane exactly between the projections of the two means, i.e. as\n",
    "\n",
    "$$\n",
    "T = \\mathbf{w} \\cdot \\frac{1}{2}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0}).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Model: Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Probabilistic generalization of Fisher's linear discriminant.\n",
    "- Generative model of the **conditional distribution** of the input data $\\mathbf{x}$ given the label $k$: $p(\\mathbf{x}|y=k)$.\n",
    "- Uses Bayes' rule to provide the **posterior distribution** of the label $k$ given the input data $\\mathbf{x}$: $p(y=k|\\mathbf{x})$.\n",
    "- Uses Bayes' rule to fix the threshold based on prior probabilities of classes.\n",
    "\n",
    "\n",
    "1. First compute the class-**conditional distributions** of $\\mathbf{x}$ given class $C_k$: $p(x|C_k) = \\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu_k}, \\mathbf{S_W})$. Where $\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu_k}, \\mathbf{S_W})$ is the multivariate Gaussian distribution defined over a P-dimensional vector $x$ of continuous variables, which is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{N}(\\mathbf{x}|\\mathbf{\\mu_k}, \\mathbf{S_W}) = \\frac{1}{(2\\pi)^{P/2}|\\mathbf{S_W}|^{1/2}}\\exp\\{-\\frac{1}{2} (\\mathbf{x} - \\mathbf{\\mu_k})^T \\mathbf{S_W}^{-1}(x - \\mathbf{\\mu_k})\\}\n",
    "$$\n",
    "\n",
    "2. Estimate the **prior probabilities** of class $k$, $p(C_k) = N_k/N$.\n",
    "\n",
    "3. Compute **posterior probabilities** (ie. the probability of a each class given a sample) combining conditional with priors using Bayes' rule:\n",
    "\n",
    "$$\n",
    "p(C_k|\\mathbf{x}) = \\frac{p(C_k) p(\\mathbf{x}|C_k)}{p(\\mathbf{x})}\n",
    "$$\n",
    "\n",
    "Where $p(x)$ is the marginal distribution obtained by summing of classes:\n",
    "As usual, the denominator in Bayes’ theorem can be found in terms of the quantities appearing in the\n",
    "numerator, because\n",
    "\n",
    "$$\n",
    "p(x) = \\sum_k p(\\mathbf{x}|C_k)p(C_k)\n",
    "$$\n",
    "\n",
    "4. Classify $\\mathbf{x}$ using the Maximum-a-Posteriori probability: $C_k= \\arg \\max_{C_k} p(C_k|\\mathbf{x})$\n",
    "\n",
    "LDA is a **generative model** since the class-conditional distributions cal be used to generate samples of each classes.\n",
    "\n",
    "LDA is useful to deal with imbalanced group sizes (eg.: $N_1 \\gg N_0$) since priors probabilities can be used to explicitly re-balance the classification by setting $p(C_0) = p(C_1) = 1/2$ or whatever seems relevant.\n",
    "\n",
    "LDA can be generalized to the multiclass case with $K>2$.\n",
    "\n",
    "With  $N_1 = N_0$, LDA lead to the same solution than Fisher's linear discriminant.\n",
    "\n",
    "**Question:** How many parameters are required to estimate to perform a LDA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application with scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "# Dataset 2 two multivariate normal\n",
    "n_samples, n_features = 100, 2\n",
    "mean0, mean1 = np.array([0, 0]), np.array([0, 2])\n",
    "Cov = np.array([[1, .8],[.8, 1]])\n",
    "np.random.seed(42)\n",
    "X0 = np.random.multivariate_normal(mean0, Cov, n_samples)\n",
    "X1 = np.random.multivariate_normal(mean1, Cov, n_samples)\n",
    "X = np.vstack([X0, X1])\n",
    "y = np.array([0] * X0.shape[0] + [1] * X1.shape[0])\n",
    "m = X.mean(axis=0)\n",
    "\n",
    "# Naive rule\n",
    "w_naive = mean1 - mean0\n",
    "w_naive = w_naive / np.linalg.norm(w_naive, ord=2) * 2\n",
    "score_naive = np.dot(X, w_naive)\n",
    "\n",
    "# Fischer rule\n",
    "Xc = np.vstack([X0 - mean0, X1 - mean1])\n",
    "Sw = np.dot(Xc.T , Xc)\n",
    "w_fisher = np.dot(np.linalg.inv(Sw), mean1 - mean0)\n",
    "w_fisher = w_fisher / np.linalg.norm(w_fisher, ord=2) * 2\n",
    "score_fisher = np.dot(X, w_fisher)\n",
    "\n",
    "\n",
    "# LDA with scikit-learn\n",
    "lda = LDA()\n",
    "score_lda = lda.fit(X, y).transform(X).ravel()\n",
    "w_lda = lda.coef_.ravel()\n",
    "w_lda = w_lda / np.linalg.norm(w_lda, ord=2) * 2\n",
    "y_pred_lda = lda.predict(X)\n",
    "\n",
    "errors =  y_pred_lda != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % \n",
    "      (errors.sum(), errors.sum() / len(y_pred_lda)))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(fig_w * 0.5, fig_w * 0.5))\n",
    "data = pd.DataFrame(np.hstack([X, y.reshape(-1, 1)]), columns=(\"x1\", \"x2\", \"y\"))\n",
    "ax_ = sns.scatterplot(data=data, x=\"x1\", y=\"x2\", hue=\"y\")\n",
    "ax_.quiver([m[0]] * 3, [m[1]] * 3,\n",
    "           [w_naive[0], w_fisher[0], w_lda[0]],\n",
    "           [w_naive[1], w_fisher[1], w_lda[1]],\n",
    "           units='xy', scale=1)\n",
    "\n",
    "scores = [(\"Naive\", score_naive), (\"Fisher\", score_fisher), (\"LDA\", score_lda)]\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "#colors = [colors[i] for i in [0, 2]]\n",
    "\n",
    "#Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(fig_w * 2, fig_h * .5), sharey=True)\n",
    "for ax, (title, score) in zip(axes, scores):\n",
    "    for lab in np.unique(y):\n",
    "        sns.histplot(score[y == lab], ax=ax,  label=f\"Label {lab}\", kde=True, color=colors[lab])\n",
    "    ax.set_title(title)\n",
    "    ax.legend()\n",
    "fig.suptitle('Projection on discriminative directions', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "![Linear (logistic) classification](images/linear_logistic.png)[width=15cm]\n",
    "\n",
    "Principles:\n",
    "\n",
    "1. Map the output of a linear model: $\\mathbf{w}^\\top \\mathbf{x}$ into a score $z$. This step performs a **projection** or a **rotation** of input sample into a good discriminative one-dimensional sub-space, that best discriminate sample of current class vs sample of other classes.\n",
    "\n",
    "2. Using an activation funtion $\\sigma(.)$, this score (a.k.a decision function) is transformed, to a \"posterior probabilities\" of a class $k$: $P(y=k| \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$.\n",
    "\n",
    "Properties:\n",
    "\n",
    "- Consider only the posterior probability of a class $k$: $P(y=k| \\mathbf{x})$\n",
    "- Multiclass Classification problems can be seen as several binary classification problems $y_i \\in \\{0, 1\\}$ where the classifier aims to discriminate the sample of the current class (label 1) versus the samples of other classes (label 0).\n",
    "- The decision surfaces (orthogonal hyperplan to $\\mathbf{w}$) correspond to $\\sigma(z)=\\text{constant}$, so that $\\mathbf{x}^T \\mathbf{w}=\\text{constant}$ and hence the decision surfaces are linear functions of $\\mathbf{x}$, even if the function $\\sigma(.)$ is nonlinear.\n",
    "- A thresholding of the activation (shifted by the bias or intercept) provides the predicted class label.\n",
    "\n",
    "\n",
    "**Linear Discriminant Analysis (LDA) vs. Logistic Regression**\n",
    "\n",
    "| Feature                     | **Linear Discriminant Analysis (LDA)**                          | **Logistic Regression**                                  |\n",
    "|-----------------------------|------------------------------------------------------------------|----------------------------------------------------------|\n",
    "| **Model Type**              | Generative model                                                 | Discriminative model                                     |\n",
    "| **What It Models**          | Joint probability: $P(x, y) = P(x \\mid y) P(y)$             | Posterior probability: $P(y \\mid x)$              |\n",
    "| **Assumptions**             | Gaussian class-conditional distributions with equal covariance  | No distributional assumption on features                |\n",
    "| **Decision Boundary**       | Linear (under equal covariance assumption)                      | Linear                                                   |\n",
    "| **Probability Estimation**  | Uses Bayes' rule + Gaussian likelihood                          | Uses sigmoid of linear function                         |\n",
    "| **Interpretability**        | Less intuitive, based on data distribution                      | Coefficients directly reflect impact on class log-odds  |\n",
    "| **Performance**             | Can outperform logistic regression when assumptions hold        | More robust when assumptions (e.g., normality) are violated |\n",
    "| **Use in Multiclass**       | Naturally extends to multiclass                                | Extends via one-vs-rest or softmax (multinomial logistic) |\n",
    "| **Regularization**          | Not built-in (requires extensions like shrinkage LDA)           | Easily regularized (e.g., with L1/L2 penalties)         |\n",
    "\n",
    "\n",
    "**Key Insights**\n",
    "\n",
    "- **LDA** is **generative**: it models how the data was generated (distribution of features given class), then uses Bayes’ theorem to classify.\n",
    "- **Logistic regression** is **discriminative**: it models the boundary between classes directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions for Classification (Sigmoid and Softmax)\n",
    "\n",
    "The **sigmoid** and **softmax** functions are closely related—they both transform real-valued inputs (*logits*) into probabilities, but they are used in different settings:\n",
    "\n",
    "\n",
    "**The sigmoid function** maps real-valued inputs (called *logits*) into the interval $[0, 1]$, making them interpretable as probabilities (for scalar $z$):\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "It has the following properties:\n",
    "\n",
    "- $\\sigma(z) \\to 1$ as $z \\to +\\infty$\n",
    "- $\\sigma(z) \\to 0$ as $z \\to -\\infty$\n",
    "- $\\sigma(0) = 0.5$\n",
    "\n",
    "In binary classification, we use $\\sigma(\\mathbf{w}^\\top \\mathbf{x})$ to estimate the probability of the positive class. This function is differentiable, which is essential for optimization via gradient descent.\n",
    "\n",
    "\n",
    "**The softmax function** converts raw scores $z_k$ into probabilities:\n",
    "\n",
    "$$\n",
    "\\hat{p}_k = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}\n",
    "$$\n",
    "\n",
    "It ensures that output probabilities are positive and sum to 1, making it suitable for use with cross-entropy.\n",
    "\n",
    "\n",
    "**Softmax function** (for vector $\\mathbf{z} \\in \\mathbb{R}^K$):\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}} \\quad \\text{for } i = 1, \\dots, K\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "The **sigmoid function is a special case of the softmax** function when you have **two classes (binary classification)**.\n",
    "Given two logits $z_0$ and $z_1$, the softmax for class 1 is:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_1) = \\frac{e^{z_1}}{e^{z_0} + e^{z_1}}\n",
    "$$\n",
    "\n",
    "If we define $z = z_1 - z_0$, then:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_1) = \\frac{1}{1 + e^{-(z_1 - z_0)}} = \\sigma(z)\n",
    "$$\n",
    "\n",
    "So:  **Sigmoid = Softmax over 2 logits, if one logit is fixed as 0 (reference class)**\n",
    "\n",
    "Intuition\n",
    "\n",
    "- **Sigmoid** gives the probability of one class (positive class) vs. its complement.\n",
    "- **Softmax** generalizes this to $K > 2$ classes, ensuring the sum of probabilities is 1.\n",
    "\n",
    "\n",
    "\n",
    "**Summary**\n",
    "\n",
    "| Function   | Use Case                         | Output                         |\n",
    "|------------|----------------------------------|---------------------------------|\n",
    "| Sigmoid | Binary classification         | Scalar in $(0, 1)$          |\n",
    "| Softmax | Multiclass classification     | Vector of probabilities summing to 1 over classes |\n",
    "\n",
    "\n",
    "- Sigmoid and Softmax maps logits to probabilities over classes  \n",
    "- The **sigmoid function is equivalent to a 2-class softmax**.\n",
    "- Both map logits to probabilities but are used in different classification settings.\n",
    "- Softmax ensures a **normalized probability distribution** over multiple classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-6, 6, 100)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.grid(True)\n",
    "plt.title('Sigmoid (Logistic)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions for Classification\n",
    "\n",
    "#### Negative Log-Likelihood (NLL) {#test-nll}\n",
    "\n",
    "The Negative Log-Likelihood (NLL) for a dataset of observations $\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n$, where each $x_i$ is an input and $y_i$ is the corresponding label.\n",
    "\n",
    "**General Formulation of NLL**\n",
    "\n",
    "Given a **probabilistic model** that predicts $P(y_i \\mid x_i; \\theta)$, where $\\theta$ are the model parameters (e.g., weights in a neural network or logistic regression), the **Negative Log-Likelihood** over the dataset is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\theta) = - \\sum_{i=1}^n \\log P(y_i \\mid x_i; \\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "This expression encourages the model to assign **high probability** to the **correct labels**.\n",
    "\n",
    "\n",
    "**Binary Classification (Sigmoid Output) a.k.a. Logistic or Binary Cross-Entropy Loss**\n",
    "\n",
    "For binary labels $y_i \\in \\{0, 1\\}$, and using:\n",
    "\n",
    "$$\n",
    "P(y_i = 1 \\mid x_i; \\theta) = \\hat{p}_i = \\sigma(z_i)\n",
    "$$\n",
    "\n",
    "where $\\hat{p}$ is the predicted probability of the positive class (obtained via a sigmoid activation, $\\sigma(z_i)$). The NLL becomes:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}} = -\\log \\Pi_{i=1}^n \\left[\\hat{p}_i^{y_i} +  (1 - \\hat{p}_i)^{(1 - y_i)} \\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}} = - \\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1 - y_i) \\log(1 - \\hat{p}_i) \\right]\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}}(\\mathbf{w}) = \\sum_{i=1}^n \\left[ \\log(1 + e^{-z_i}) + (1 - y_i) z_i \\right] \\quad \\text{with } y_i \\in \\{0, 1\\}.\n",
    "$$\n",
    "\n",
    "Recoding output $y_i \\in \\{-1, +1\\}$, the expression simplifies to\n",
    "\n",
    "$$\n",
    "\\boxed{\\mathcal{L}_{\\text{NLL}}(\\mathbf{w}) = \\sum_{i=1}^n \\log\\left(1 + e^{-y_i \\cdot z_i} \\right)} \\quad \\text{with } y_i \\in \\{-1, +1\\}.\n",
    "$$\n",
    "\n",
    "For linear models: $z_i=\\mathbf{w}^\\top \\mathbf{x}_i$. For more details, see the \n",
    "[Demonstration of Negative Log-Likelihood (NLL)](ref:demonstration-nll) Loss.\n",
    "\n",
    "This expression is also known as the **logistic loss** or **binary cross-entropy**. It penalizes confident but incorrect predictions very heavily (e.g., predicting $\\hat{p} = 0.99$ when $y = 0$).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic loss for binary classification\n",
    "def logistic_loss(z, y):\n",
    "    return np.log(1 + np.exp(-y * z))\n",
    "\n",
    "\n",
    "# Input range (raw scores from the model)\n",
    "z = np.linspace(-10, 10, 400)\n",
    "\n",
    "# Logistic loss for y = 1 and y = 0\n",
    "loss_y1 = logistic_loss(z, 1)\n",
    "loss_y0 = logistic_loss(z, -1)  # equivalent to logistic loss for y=0\n",
    "\n",
    "# Plotting\n",
    "plt.figure()\n",
    "plt.plot(z, loss_y1, label=\"Logistic Loss (y = 1)\")\n",
    "plt.plot(z, loss_y0, label=\"Logistic Loss (y = 0)\", linestyle=\"--\")\n",
    "plt.title(\"Logistic Loss as a Function of Raw Score z\")\n",
    "plt.xlabel(\"Model score z\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Logistic or Binary Cross-Entropy Loss for Linear Models**\n",
    "\n",
    "For linear models where $z_i=\\mathbf{w}^\\top \\mathbf{x}_i$ the gradient of the NLL according to the coefficients vector $\\mathbf{w}$ is:\n",
    "\n",
    "  $$\n",
    "  \\boxed{\\nabla_{\\mathbf{w}} \\mathcal{L}_{\\text{NLL}} = \\sum_{i=1}^n (\\sigma(\\mathbf{w}^\\top \\mathbf{x}_i) - y_i)\\mathbf{x}_i}\n",
    "  $$\n",
    "  \n",
    "**Multiclass Classification (Softmax Output) a.k.a. Cross-Entropy Loss**\n",
    "\n",
    "Assume $y_i \\in \\{1, 2, \\dots, K\\}$, and the model outputs softmax probabilities $\\hat{p}_{i,k} = P(y_i = k \\mid x_i; \\theta)$. Then:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}} = - \\sum_{i=1}^n \\log \\hat{p}_{i, y_i}\n",
    "$$\n",
    "\n",
    "If you use one-hot encoded labels $\\mathbf{y}_i$, the NLL is also written as:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{NLL}} = - \\sum_{i=1}^n \\sum_{k=1}^K y_{i,k} \\log(\\hat{p}_{i,k})\n",
    "$$\n",
    "\n",
    "This is equivalent to the **cross-entropy loss** when combined with softmax.\n",
    "\n",
    "**Summary**\n",
    "\n",
    "- Negative Log-Likelihood = general loss for probabilistic models\n",
    "- Logistic loss = binary cross-entropy\n",
    "- Cross-Entropy loss = NLL + Softmax (for multiclass problems)\n",
    "- These losses are convex (for linear models), interpretable, and widely used in training classifiers like logistic regression and neural networks.\n",
    "\n",
    "See also [Scikit learn doc](https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation)\n",
    "\n",
    "### Hinge loss or $\\ell_1$ loss\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Summary\n",
    "\n",
    "Logistic regression minimizes the Cross-Entropy loss i.e. NLL with Sigmoid (binary problems) or NLL with Softmax (multiclass problems):\n",
    "\n",
    "$$\n",
    "\\boxed{\\min_{\\mathbf{w}}~\\text{Logistic}(\\mathbf{w}) = \\mathcal{L}_{\\text{NLL}}(\\mathbf{w})}\n",
    "$$\n",
    "\n",
    "### Logistic Regression with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "logreg = lm.LogisticRegression(penalty=None).fit(X, y)\n",
    "\n",
    "logreg.fit(X, y)\n",
    "y_pred_logreg = logreg.predict(X)\n",
    "\n",
    "errors =  y_pred_logreg != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % \n",
    "      (errors.sum(), errors.sum() / len(y_pred_logreg)))\n",
    "print(logreg.coef_.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization using penalization of coefficients\n",
    "\n",
    "The penalties use in regression are also used in classification. The only difference is the loss function generally the negative log likelihood (cross-entropy) or the hinge loss. We will explore:\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Ridge (also called $\\ell_2$) penalty: $\\|\\mathbf{w}\\|_2^2$. It shrinks coefficients toward 0.\n",
    "- Lasso (also called $\\ell_1$) penalty: $\\|\\mathbf{w}\\|_1$. It performs feature selection by setting some coefficients to 0.\n",
    "- ElasticNet (also called $\\ell_1\\ell_2$) penalty: $\\alpha \\left(\\rho~\\|\\mathbf{w}\\|_1 + (1-\\rho)~\\|\\mathbf{w}\\|_2^2 \\right)$. It performs selection of group of correlated features by setting some coefficients to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with some correlation\n",
    "X, y = make_classification(n_samples=100, n_features=10,\n",
    "                           n_informative=5, n_redundant=3,\n",
    "                           n_classes=2, random_state=3, shuffle=False)\n",
    "\n",
    "# Logistic Regression unpenalized\n",
    "lr = lm.LogisticRegression(penalty=None).fit(X, y)\n",
    "\n",
    "# Logistic Regression with L2 penalty\n",
    "l2 = lm.LogisticRegression(penalty='l2', C=.1).fit(X, y)  # lambda = 1 / C!\n",
    "\n",
    "# Logistic Regression with L1 penalty\n",
    "# use solver 'saga' to handle L1 penalty. lambda = 1 / C\n",
    "l1 = lm.LogisticRegression(penalty='l1', C=.1, solver='saga').fit(X, y)\n",
    "\n",
    "# Logistic Regression with L1/L2 penalties\n",
    "l1l2 = lm.LogisticRegression(penalty='elasticnet',  C=.1, l1_ratio=0.5,\n",
    "                             solver='saga').fit(X, y)  # lambda = 1 / C!\n",
    "\n",
    "\n",
    "print(pd.DataFrame(np.vstack((lr.coef_, l2.coef_, l1.coef_, l1l2.coef_)),\n",
    "             index=['lr', 'l2', 'l1', 'l1l2']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the effect of penalty using $\\ell_2$-regularization Fisher's linear classification\n",
    "\n",
    "When the matrix $\\mathbf{S_W}$ is not full rank or $P \\gg N$, the The Fisher most discriminant projection estimate of the is not unique. This can be solved using a biased version of $\\mathbf{S_W}$:\n",
    "$$\n",
    "\\mathbf{S_W}^{Ridge} = \\mathbf{S_W} + \\lambda \\mathbf{I}\n",
    "$$\n",
    "\n",
    "where $I$ is the $P \\times P$ identity matrix. This leads to the regularized (ridge) estimator of the Fisher's linear discriminant analysis: \n",
    "$$\n",
    "    \\mathbf{w}^{Ridge} \\propto (\\mathbf{S_W} + \\lambda \\mathbf{I})^{-1}(\\mathbf{\\mu_1} - \\mathbf{\\mu_0})\n",
    "$$\n",
    "\n",
    "![The Ridge Fisher most discriminant projection](images/ridge_fisher_linear_disc.png){width=15cm}\n",
    "\n",
    "Increasing $\\lambda$ will:\n",
    "\n",
    "- Shrinks the coefficients toward zero.\n",
    "- The covariance will converge toward the diagonal matrix, reducing the contribution of the pairwise covariances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$-regularized logistic regression\n",
    "\n",
    "The **objective function** to be minimized is now the combination of the logistic loss (Negative Log Likelihood) with a penalty of the L2 norm of the weights vector. In the two-class case, using the 0/1 coding we obtain:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}}~\\text{Logistic L2}(\\mathbf{w}) = \\mathcal{L}_{\\text{NLL}}(\\mathbf{w})\n",
    " + \\lambda~\\|\\mathbf{w}\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "lrl2 = lm.LogisticRegression(penalty='l2', C=.1)\n",
    "# This class implements regularized logistic regression.\n",
    "# C is the Inverse of regularization strength. Large value => no regularization.\n",
    "\n",
    "lrl2.fit(X, y)\n",
    "y_pred_l2 = lrl2.predict(X)\n",
    "prob_pred_l2 = lrl2.predict_proba(X)\n",
    "\n",
    "print(\"Probas of 5 first samples for class 0 and class 1:\")\n",
    "print(prob_pred_l2[:5, :].round(2))\n",
    "\n",
    "print(\"Coef vector:\")\n",
    "print(lrl2.coef_.round(2))\n",
    "\n",
    "# Retrieve proba from coef vector\n",
    "probas = 1 / (1 + np.exp(- (np.dot(X, lrl2.coef_.T) + lrl2.intercept_))).ravel()\n",
    "print(\"Diff\", np.max(np.abs(prob_pred_l2[:, 1] - probas)))\n",
    "\n",
    "errors =  y_pred_l2 != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso logistic regression ($\\ell_1$-regularization)\n",
    "\n",
    "The **objective function** to be minimized is now the combination of the logistic loss $-\\log \\mathcal{L}(\\mathbf{w})$ with a penalty of the L1 norm of the weights vector. In the two-class case, using the 0/1 coding we obtain:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}}~\\text{Logistic Lasso}(\\mathbf{w}) = \\mathcal{L}_{\\text{NLL}}(\\mathbf{w})\n",
    " + \\lambda~\\|\\mathbf{w}\\|_1\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm\n",
    "lrl1 = lm.LogisticRegression(penalty='l1', C=.1, solver='saga') # lambda = 1 / C!\n",
    "\n",
    "# This class implements regularized logistic regression. C is the Inverse of regularization strength.\n",
    "# Large value => no regularization.\n",
    "\n",
    "lrl1.fit(X, y)\n",
    "y_pred_lrl1 = lrl1.predict(X)\n",
    "\n",
    "errors =  y_pred_lrl1 != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_lrl1)))\n",
    "\n",
    "print(\"Coef vector:\")\n",
    "print(lrl1.coef_.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine ($\\ell_2$-regularization with Hinge loss)\n",
    "\n",
    "Support Vector Machine seek for separating hyperplane with maximum margin to enforce robustness against noise. Like logistic regression it is a **discriminative method** that only focuses of predictions.\n",
    "\n",
    "Here we present the non separable case of Maximum Margin Classifiers with $\\pm 1$ coding (ie.: $y_i \\ \\{-1, +1\\}$).\n",
    "\n",
    "![Linear lar margin classifiers](images/svm.png)\n",
    "\n",
    "Linear SVM for classification (also called SVM-C or SVC) minimizes:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}}~\\text{Linear SVM}(\\mathbf{w}) = \\|\\mathbf{w}\\|_2^2 +  C~\\sum_i^n\\max(0, 1 - y_i~ (\\mathbf{w} \\cdot \\mathbf{x_i}))\n",
    "$$\n",
    "where $\\max(0, 1 - y_i~ (\\mathbf{w} \\cdot \\mathbf{x_i})$ is the **hinge loss**.\n",
    "\n",
    "\n",
    "Here we introduced the slack variables: $\\xi_i$, with $\\xi_i = 0$ for points that are on or inside the correct margin boundary and $\\xi_i = |y_i - (w \\ cdot  \\cdot \\mathbf{x_i})|$ for other points. Thus:\n",
    "\n",
    "1. If $y_i (\\mathbf{w} \\cdot \\mathbf{x_i}) \\geq 1$ then the point lies outside the margin but on the correct side of the decision boundary. In this case the constraint is thus not active for this point. It does not contribute to the prediction.\n",
    "\n",
    "2. If $1 > y_i (\\mathbf{w} \\cdot \\mathbf{x_i}) \\geq 0$ then the point lies inside the margin and on the correct side of the decision boundary. In this case the constraint is active for this point. It does contribute to the prediction as a support vector.\n",
    "\n",
    "3. If $0 <  y_i (\\mathbf{w} \\cdot \\mathbf{x_i})$) then the point is on the wrong side of the decision boundary (missclassification). In this case the constraint is active for this point. It does contribute to the prediction as a support vector.\n",
    "\n",
    "\n",
    "So linear SVM is closed to Ridge logistic regression, using the hinge loss instead of the logistic loss. Both will provide very similar predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svmlin = svm.LinearSVC(C=.1)\n",
    "# Remark: by default LinearSVC uses squared_hinge as loss\n",
    "svmlin.fit(X, y)\n",
    "y_pred_svmlin = svmlin.predict(X)\n",
    "\n",
    "errors =  y_pred_svmlin != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" %\n",
    "      (errors.sum(), errors.sum() / len(y_pred_svmlin)))\n",
    "print(\"Coef vector:\")\n",
    "print(svmlin.coef_.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso linear Support Vector Machine ($\\ell_1$-regularization)\n",
    "\n",
    "Linear SVM with l1-regularization:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}}~\\text{Lasso Linear SVM}(\\mathbf{w}) = \\|\\mathbf{w}\\|_1 +  C~\\sum_i^n\\max(0, 1 - y_i~ (\\mathbf{w} \\cdot \\mathbf{x_i}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "svmlinl1 = svm.LinearSVC(penalty='l1', dual=False)\n",
    "# Remark: by default LinearSVC uses squared_hinge as loss\n",
    "\n",
    "svmlinl1.fit(X, y)\n",
    "y_pred_svmlinl1 = svmlinl1.predict(X)\n",
    "\n",
    "errors =  y_pred_svmlinl1 != y\n",
    "print(\"Nb errors=%i, error rate=%.2f\" % (errors.sum(), errors.sum() / len(y_pred_svmlinl1)))\n",
    "print(\"Coef vector:\")\n",
    "print(svmlinl1.coef_.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-net classification ($\\ell_1\\ell_2$-regularization)\n",
    "\n",
    "The **objective function** to be minimized is now the combination of a loss with combination of L1 and L2 penalties:\n",
    "\n",
    "- For the NLL loss:\n",
    "$$\n",
    "\\min~\\text{Logistic Enet}(\\mathbf{w}) = \\mathcal{L}_{\\text{NLL}}(\\mathbf{w}) + \\alpha~\\left(\\rho~\\|\\mathbf{w}\\|_1 + (1-\\rho)~\\|\\mathbf{w}\\|_2^2 \\right)\n",
    "$$\n",
    "\n",
    "- For the Hinge loss\n",
    "$$\n",
    "\\min~\\text{Hinge Enet}(\\mathbf{w}) = \\text{Hinge loss}(\\mathbf{w}) + \\alpha~\\left(\\rho~\\|\\mathbf{w}\\|_1 + (1-\\rho)~\\|\\mathbf{w}\\|_2^2 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SGD solver\n",
    "enetlog = lm.SGDClassifier(loss=\"log_loss\", penalty=\"elasticnet\",\n",
    "                            alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "enetlog.fit(X, y)\n",
    "\n",
    "# Or saga solver:\n",
    "# enetloglike = lm.LogisticRegression(penalty='elasticnet',\n",
    "#                                    C=.1, l1_ratio=0.5, solver='saga')\n",
    "\n",
    "enethinge = lm.SGDClassifier(loss=\"hinge\", penalty=\"elasticnet\",\n",
    "                            alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "enethinge.fit(X, y)\n",
    "\n",
    "print(\"Hinge loss and logistic loss provide almost the same predictions.\")\n",
    "print(\"Confusion matrix\")\n",
    "metrics.confusion_matrix(enetlog.predict(X), enethinge.predict(X))\n",
    "\n",
    "print(\"Decision_function log x hinge losses:\")\n",
    "_ = plt.plot(enetlog.decision_function(X),\n",
    "             enethinge.decision_function(X), \"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification performance evaluation metrics\n",
    "\n",
    "[Wikipedia Sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n",
    "\n",
    "Imagine a study evaluating a new test that screens people for a disease. Each person taking the test either has or does not have the disease. The test outcome can be positive (classifying the person as having the disease) or negative (classifying the person as not having the disease). The test results for each subject may or may not match the subject's actual status. In that setting:\n",
    "\n",
    "- True positive (TP): Sick people correctly identified as sick\n",
    "\n",
    "- False positive (FP): Healthy people incorrectly identified as sick\n",
    "\n",
    "- True negative (TN): Healthy people correctly identified as healthy\n",
    "\n",
    "- False negative (FN): Sick people incorrectly identified as healthy\n",
    "\n",
    "- **Accuracy** (ACC):\n",
    "\n",
    "    ACC = (TP + TN) / (TP + FP + FN + TN)\n",
    "\n",
    "\n",
    "- **Sensitivity** (SEN) or **recall** of the positive class or true positive rate (TPR) or hit rate:\n",
    "\n",
    "    SEN = TP / P = TP / (TP+FN)\n",
    "\n",
    "\n",
    "- **Specificity** (SPC) or **recall** of the negative class or true negative rate:\n",
    "\n",
    "    SPC = TN / N = TN / (TN+FP) \n",
    "\n",
    "\n",
    "- **Precision** or positive predictive value (PPV):\n",
    "\n",
    "    PPV = TP / (TP + FP)\n",
    "\n",
    "\n",
    "- **Balanced accuracy** (bACC):is a useful performance measure is the balanced accuracy which avoids inflated performance estimates on imbalanced datasets (Brodersen, et al. (2010). \"The balanced accuracy and its posterior distribution\"). It is defined as the arithmetic mean of sensitivity and specificity, or the average accuracy obtained on either class:\n",
    "\n",
    "    bACC = 1/2 * (SEN + SPC) \n",
    "\n",
    "- F1 Score (or F-score) which is a weighted average of precision and recall are useful to deal with imbalanced datasets\n",
    "    \n",
    "The four outcomes can be formulated in a 2×2 contingency table or [confusion matrix](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)\n",
    "\n",
    "For more precision see [Scikit-learn doc](http://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "y_pred = [0, 1, 0, 0]\n",
    "y_true = [0, 1, 0, 1]\n",
    "\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "# The overall precision an recall\n",
    "metrics.precision_score(y_true, y_pred)\n",
    "metrics.recall_score(y_true, y_pred)\n",
    "\n",
    "# Recalls on individual classes: SEN & SPC\n",
    "recalls = metrics.recall_score(y_true, y_pred, average=None)\n",
    "recalls[0] # is the recall of class 0: specificity\n",
    "recalls[1] # is the recall of class 1: sensitivity\n",
    "\n",
    "# Balanced accuracy\n",
    "b_acc = recalls.mean()\n",
    "\n",
    "# The overall precision an recall on each individual class\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area Under Curve (AUC) of Receiver operating characteristic (ROC)\n",
    "\n",
    "Some classifier may have found a good discriminative projection $w$. However if the threshold to decide the final predicted class is poorly adjusted, the performances will highlight an high specificity and a low sensitivity or the contrary.\n",
    "\n",
    "In this case it is recommended to use the AUC of a ROC analysis which basically provide a measure of overlap of the two classes when points are projected on the discriminative axis. See [Wikipedia: ROC and AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pred = np.array([.1, .2, .3, .4, .5, .6, .7, .8])\n",
    "y_true = np.array([0, 0, 0, 0, 1, 1, 1, 1])\n",
    "thres = .9\n",
    "y_pred = (score_pred > thres).astype(int)\n",
    "\n",
    "print(\"With a threshold of %.2f, the rule always predict 0. Predictions:\" % thres)\n",
    "print(y_pred)\n",
    "metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "# The overall precision an recall on each individual class\n",
    "r = metrics.recall_score(y_true, y_pred, average=None)\n",
    "print(\"Recalls on individual classes are:\", r,\n",
    "      \"ie, 100% of specificity, 0% of sensitivity\")\n",
    "\n",
    "# However AUC=1 indicating a perfect separation of the two classes\n",
    "auc = metrics.roc_auc_score(y_true, score_pred)\n",
    "print(\"But the AUC of %.2f demonstrate a good classes separation.\" % auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced classes\n",
    "\n",
    "Learning with discriminative (logistic regression, SVM) methods is generally based on minimizing the misclassification of training samples, which may be unsuitable for imbalanced datasets where the recognition might be biased in favor of\n",
    "the most numerous class. This problem can be addressed with a generative approach, which typically requires\n",
    "more parameters to be determined leading to reduced performances in high dimension.\n",
    "\n",
    "Dealing with imbalanced class may be addressed by three main ways (see Japkowicz and Stephen (2002) for a review), resampling, reweighting and one class learning.\n",
    "\n",
    "In **sampling strategies**, either the minority class is oversampled or majority class is undersampled or some combination of the two is deployed. Undersampling (Zhang and Mani, 2003) the majority class would lead to a poor usage of the left-out samples. Sometime one cannot afford such strategy since we are also facing a small sample size problem even for the majority class.\n",
    "Informed oversampling, which goes beyond a trivial duplication of minority class samples, requires the estimation of class conditional distributions in order to generate synthetic samples. Here generative models are required. An alternative, proposed in (Chawla et al., 2002) generate samples along the line segments joining any/all of the k minority class nearest neighbors. Such procedure blindly generalizes the minority area without regard to the majority class, which may be particularly problematic with high-dimensional and potentially skewed class distribution. \n",
    "\n",
    "**Reweighting**, also called cost-sensitive learning, works at an algorithmic level by adjusting the costs of the various classes to counter the class imbalance. Such reweighting can be implemented within SVM (Chang and Lin, 2001) or logistic regression (Friedman et al., 2010) classifiers. Most classifiers of Scikit learn offer such reweighting possibilities. \n",
    "\n",
    "The ``class_weight`` parameter can be positioned into the ``\"balanced\"`` mode which uses the values of $y$ to automatically adjust weights inversely proportional to class frequencies in the input data as $N / (2 N_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "X, y = make_classification(n_samples=500,\n",
    "                           n_features=5,\n",
    "                           n_informative=2,\n",
    "                           n_redundant=0,\n",
    "                           n_repeated=0,\n",
    "                           n_classes=2,\n",
    "                           random_state=1,\n",
    "                           shuffle=False)\n",
    "\n",
    "print(*[\"#samples of class %i = %i;\" % (lev, np.sum(y == lev))\n",
    "      for lev in np.unique(y)])\n",
    "\n",
    "print('# No Reweighting balanced dataset')\n",
    "lr_inter = lm.LogisticRegression(C=1)\n",
    "lr_inter.fit(X, y)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(y, lr_inter.predict(X))\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => The predictions are balanced in sensitivity and specificity\\n')\n",
    "\n",
    "# Create imbalanced dataset, by subsampling sample of class 0: keep only 10% of\n",
    "#  class 0's samples and all class 1's samples.\n",
    "n0 = int(np.rint(np.sum(y == 0) / 20))\n",
    "subsample_idx = np.concatenate((np.where(y == 0)[0][:n0], np.where(y == 1)[0]))\n",
    "Ximb = X[subsample_idx, :]\n",
    "yimb = y[subsample_idx]\n",
    "print(*[\"#samples of class %i = %i;\" % (lev, np.sum(yimb == lev)) for lev in\n",
    "        np.unique(yimb)])\n",
    "\n",
    "print('# No Reweighting on imbalanced dataset')\n",
    "lr_inter = lm.LogisticRegression(C=1)\n",
    "lr_inter.fit(Ximb, yimb)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(\n",
    "    yimb, lr_inter.predict(Ximb))\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => Sensitivity >> specificity\\n')\n",
    "\n",
    "print('# Reweighting on imbalanced dataset')\n",
    "lr_inter_reweight = lm.LogisticRegression(C=1, class_weight=\"balanced\")\n",
    "lr_inter_reweight.fit(Ximb, yimb)\n",
    "p, r, f, s = metrics.precision_recall_fscore_support(yimb,\n",
    "                                                     lr_inter_reweight.predict(Ximb))\n",
    "print(\"SPC: %.3f; SEN: %.3f\" % tuple(r))\n",
    "print('# => The predictions are balanced in sensitivity and specificity\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence interval cross-validation\n",
    "\n",
    "Confidence interval CI classification accuracy measured by cross-validation:\n",
    "![CI classification](images/classif_accuracy_95ci_sizes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significance of classification metrics\n",
    "\n",
    "**P-value of classification accuracy:** Compare the number of correct classifications (=accuracy $\\times N$) to the null hypothesis of Binomial distribution of parameters $p$ (typically 50% of chance level) and $N$ (Number of observations). Is 60% accuracy a significant prediction rate among 50 observations?\n",
    "Since this is an exact, **two-sided** test of the null hypothesis, the p-value can be divided by two since we test that the accuracy is superior to the chance level.\n",
    "\n",
    "**P-value of ROC-AUC:** ROC-AUC measures the ranking of the two classes. Therefore non-parametric test can be used to asses the significance of the classes's separation. Mason and Graham (RMetS, 2002) show that the ROC area is equivalent to the Mann–Whitney U-statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon, Wilcoxon\n",
    "rank-sum test or Wilcoxon–Mann–Whitney test) is a nonparametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "N_test = 50\n",
    "X, y = make_classification(n_samples=200, random_state=42,\n",
    "                           shuffle=False, class_sep=0.80)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                        test_size=N_test, random_state=42)\n",
    "\n",
    "lr = lm.LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "proba_pred = lr.predict_proba(X_test)[:, 1]\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "bacc = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "auc = metrics.roc_auc_score(y_test, proba_pred)\n",
    "\n",
    "print(\"ACC=%.2f, bACC=%.2f, AUC=%.2f,\" % (acc, bacc, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# acc, N = 0.65, 70\n",
    "k = int(acc * N_test)\n",
    "acc_test = stats.binomtest(k=k, n=N_test, p=0.5, alternative='greater')\n",
    "auc_pval = stats.mannwhitneyu(\n",
    "    proba_pred[y_test == 0], proba_pred[y_test == 1]).pvalue\n",
    "\n",
    "\n",
    "def is_significant(pval): return True if pval < 0.05 else False\n",
    "\n",
    "\n",
    "print(\"ACC=%.2f (pval=%.3f, significance=%r)\" %\n",
    "      (acc, acc_test.pvalue, is_significant(acc_test.pvalue)))\n",
    "print(\"AUC=%.2f (pval=%.3f, significance=%r)\" %\n",
    "      (auc, auc_pval, is_significant(auc_pval)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "### Fisher linear discriminant rule\n",
    "\n",
    "Write a class ``FisherLinearDiscriminant`` that implements the Fisher's linear discriminant analysis. This class must be compliant with the scikit-learn API by providing two methods: \n",
    "- ``fit(X, y)`` which fits the model and returns the object itself;\n",
    "- ``predict(X)`` which returns a vector of the predicted values.\n",
    "Apply the object on the dataset presented for the LDA."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
