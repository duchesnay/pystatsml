<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Models for Regression &#8212; Statistics and Machine Learning in Python 0.8 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=27fed22d" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <script src="../_static/documentation_options.js?v=a0e24af7"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="search" type="application/opensearchdescription+xml"
          title="Search within Statistics and Machine Learning in Python 0.8 documentation"
          href="../_static/opensearch.xml"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Linear Models for Classification" href="linear_classification.html" />
    <link rel="prev" title="Clustering" href="../ml_unsupervised/clustering.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="linear-models-for-regression">
<h1>Linear Models for Regression<a class="headerlink" href="#linear-models-for-regression" title="Link to this heading">¶</a></h1>
<figure class="align-default" id="id3">
<img alt="Linear regression" src="../_images/linear_regression.png" />
<figcaption>
<p><span class="caption-text">Linear regression</span><a class="headerlink" href="#id3" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<section id="minimizing-the-sum-of-squared-errors-sse-loss">
<h2>Minimizing the Sum of Squared Errors (SSE) Loss<a class="headerlink" href="#minimizing-the-sum-of-squared-errors-sse-loss" title="Link to this heading">¶</a></h2>
<p>A linear model assumes a linear relationship between input features of
observation <span class="math notranslate nohighlight">\(i\)</span> <span class="math notranslate nohighlight">\(\mathbf{x}_i \in \mathbb{R}^{P}\)</span> and an
output <span class="math notranslate nohighlight">\(y \in \mathrm{R}\)</span> using:</p>
<div class="math notranslate nohighlight">
\[y_i = \mathbf{x}_i^\top \mathbf{w}+ \varepsilon_i,\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{\varepsilon}_i \in \mathrm{R}\)</span> is the
<strong>residual</strong> or the error of the prediction. <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is the
weight vector (model’s parameters) of coefficients. <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>
is found by minimizing an <strong>objective function</strong>, which is the <strong>loss
function</strong>, <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span>, i.e. the error measured on the data.
This error is the <strong>sum of squared errors (SSE) loss</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{SSE}(\mathbf{w}) = \sum_i^N (y_i - \mathbf{x}_i^\top\mathbf{w})^2\]</div>
<p>The equivalent matrix notation is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{y} = \mathbf{X} \mathbf{w} + \boldsymbol{\varepsilon},\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> be the <span class="math notranslate nohighlight">\(N \times P\)</span> matrix with each row
an input vector and similarly let <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> the
<span class="math notranslate nohighlight">\(N\)</span>-dimensional vector of outputs.</p>
<p>The loss is given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{SSE}(\mathbf{w}) = (\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w})\\
= \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2,\end{split}\]</div>
<p>Minimizing the SSE is the Ordinary Least Square <strong>OLS</strong> regression as
objective function. which is a simple <strong>ordinary least squares (OLS)</strong>
minimization whose analytic solution is:</p>
<div class="math notranslate nohighlight">
\[\mathbf{w}_{\text{OLS}} = (\mathbf{X}^\top\mathbf{X})^{-1} \mathbf{X}^\top \mathbf{y}\]</div>
<p>The solution could also be found using gradient descent using the
gradient of the loss:</p>
<div class="math notranslate nohighlight">
\[\partial\frac{SSE(\mathbf{w})}{\partial\mathbf{w}} = 2 \sum_i \mathbf{x}_i (\mathbf{x}_i \cdot \mathbf{w} - y_i)\]</div>
<p>Linear regression of <code class="docutils literal notranslate"><span class="pre">Advertising.csv</span></code> dataset with TV and Radio
advertising as input features and Sales as target. The linear model that
minimizes the MSE is a plan (2 input features) defined as: Sales = 0.05
TV + .19 Radio + 3:</p>
<figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/linear_regression_plan.png"><img alt="Linear regression" src="../_images/linear_regression_plan.png" style="width: 10cm;" />
</a>
<figcaption>
<p><span class="caption-text">Linear regression</span><a class="headerlink" href="#id4" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="linear-regression-with-scikit-learn">
<h2>Linear regression with scikit-learn<a class="headerlink" href="#linear-regression-with-scikit-learn" title="Link to this heading">¶</a></h2>
<p>Scikit-learn offers many models for supervised learning, and they all
follow the same <strong>Application Programming Interface (API)</strong>, namely:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">est</span> <span class="o">=</span> <span class="n">Estimator</span><span class="p">()</span>
<span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="c1"># Plot</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="c1"># Plot parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;seaborn-v0_8-whitegrid&#39;</span><span class="p">)</span>
<span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">rcParams</span><span class="p">[</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fig_w</span><span class="p">,</span> <span class="n">fig_h</span> <span class="o">*</span> <span class="mf">.5</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">lm</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">metrics</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset with some correlation</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                      <span class="n">n_informative</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                                      <span class="n">effective_rank</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">coef</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="regularization-using-penalization-of-coefficients">
<h2>Regularization using penalization of coefficients<a class="headerlink" href="#regularization-using-penalization-of-coefficients" title="Link to this heading">¶</a></h2>
<p>Regarding linear models, overfitting generally leads to excessively
complex solutions (coefficient vectors), accounting for noise or
spurious correlations within predictors. <strong>Regularization</strong> aims to
alleviate this phenomenon by constraining (biasing or reducing) the
capacity of the learning algorithm in order to promote simple solutions.
Regularization penalizes “large” solutions forcing the coefficients to
be small, i.e. to shrink them toward zeros.</p>
<p>The objective function <span class="math notranslate nohighlight">\(J(\mathbf{w})\)</span> to minimize with respect to
<span class="math notranslate nohighlight">\(\mathbf{w}\)</span> is composed of a loss function <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span>
for goodness-of-fit and a penalty term <span class="math notranslate nohighlight">\(\Omega(\mathbf{w})\)</span>
(regularization to avoid overfitting). This is a trade-off where the
respective contribution of the loss and the penalty terms is controlled
by the regularization parameter <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p>Therefore the <strong>loss function</strong> <span class="math notranslate nohighlight">\(L(\mathbf{w})\)</span> is combined with a
<strong>penalty function</strong> <span class="math notranslate nohighlight">\(\Omega(\mathbf{w})\)</span> leading to the general
form:</p>
<div class="math notranslate nohighlight">
\[J(\mathbf{w}) = \sum_i^N (y_i - \mathbf{x}_i^\top\mathbf{w})^2 + \lambda \Omega(\mathbf{w})\]</div>
<p>The respective contribution of the loss and the penalty is controlled by
the <strong>regularization hyper-parameter</strong> <span class="math notranslate nohighlight">\(\lambda\)</span>.</p>
<p><strong>Regularization exploits Occam’s razor is a principle</strong></p>
<p>Occam’s razor is a principle stating that, among competing hypotheses,
the one with the fewest assumptions should be selected. This is also
known as the principle of parsimony, favoring simpler explanations or
models when possible. In machine learning, this translates to preferring
models with similar loss but lower complexity.</p>
</section>
<section id="ridge-regression-ell-2-or-l2-regularization">
<h2>Ridge Regression (<span class="math notranslate nohighlight">\(\ell_2\)</span> or L2 regularization)<a class="headerlink" href="#ridge-regression-ell-2-or-l2-regularization" title="Link to this heading">¶</a></h2>
<ul>
<li><p><strong>Penalty</strong>:
<span class="math notranslate nohighlight">\(\Omega(\mathbf{w}) = \|\mathbf{w}\|_2^2 = \sum_{j} w_j^2\)</span></p></li>
<li><p><strong>Objective function</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{Ridge}(\mathbf{w}) = \sum_i^N (y_i - \mathbf{x}_i^\top\mathbf{w})^2 + \lambda \|\mathbf{w}\|_2^2\]</div>
</li>
<li><p><strong>Effect</strong>: Shrinks all coefficients smoothly towards zero, but <strong>does
not enforce sparsity</strong> (no coefficients become exactly zero).</p></li>
<li><p>The gradient of the loss:</p>
<div class="math notranslate nohighlight">
\[\partial\frac{L(\mathbf{w}, \mathbf{X}, \mathbf{y})}{\partial\mathbf{w}} = 2 (\sum_i \mathbf{x}_i (\mathbf{x}_i \cdot \mathbf{w} - y_i) + \lambda \mathbf{w})\]</div>
</li>
</ul>
<section id="numerical-interpretation">
<h3>Numerical Interpretation<a class="headerlink" href="#numerical-interpretation" title="Link to this heading">¶</a></h3>
<p>Using the matrix notation, he Ridge objective function can be rewritten
as</p>
<div class="math notranslate nohighlight">
\[\text{Ridge}(\mathbf{w}) = \|\mathbf{y} - \mathbf{X}\mathbf{w}\|_2^2 + \lambda \|\mathbf{w}\|_2^2\]</div>
<p>The <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> that minimizes <span class="math notranslate nohighlight">\(F_{Ridge}(\mathbf{w})\)</span> can
be found by the following derivation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\nabla_{\mathbf{w}}\text{Ridge}(\mathbf{w}) = 0\\
\nabla_{\mathbf{w}}\big((\mathbf{y} - \mathbf{X}\mathbf{w})^\top (\mathbf{y} - \mathbf{X}\mathbf{w}) + \lambda \mathbf{w}^\top\mathbf{w}\big) = 0\\
\nabla_{\mathbf{w}}\big(\mathbf{y}^\top\mathbf{y} - 2 \mathbf{w}^\top\mathbf{X}^\top\mathbf{y} + \mathbf{w}^\top\mathbf{X}^\top\mathbf{X}\mathbf{w} + \lambda \mathbf{w}^\top\mathbf{w}\big) = 0\\
-2\mathbf{X}^\top\mathbf{y} + 2 \mathbf{X}^\top\mathbf{X}\mathbf{w} + 2 \lambda \mathbf{w} = 0\\
-\mathbf{X}^\top\mathbf{y} + (\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{I}) \mathbf{w} = 0\\
(\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{I}) \mathbf{w} = \mathbf{X}^\top\mathbf{y}\\
\mathbf{w} = (\mathbf{X}^\top\mathbf{X} + \lambda \mathbf{I})^{-1} \mathbf{X}^\top\mathbf{y}\end{split}\]</div>
<ul class="simple">
<li><p>The solution adds a positive constant to the diagonal of
<span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> before inversion. This makes the
problem nonsingular, even if <span class="math notranslate nohighlight">\(\mathbf{X}^\top\mathbf{X}\)</span> is not
of full rank, and was the main motivation behind ridge regression.</p></li>
<li><p>Increasing <span class="math notranslate nohighlight">\(\lambda\)</span> shrinks the <span class="math notranslate nohighlight">\(\mathbf{w}\)</span> coefficients
toward 0.</p></li>
<li><p>Solutions with large coefficients become unattractive.</p></li>
</ul>
</section>
</section>
<section id="lasso-regression-ell-1-or-l1-regularization">
<h2>Lasso Regression (<span class="math notranslate nohighlight">\(\ell_1\)</span> or L1 regularization)<a class="headerlink" href="#lasso-regression-ell-1-or-l1-regularization" title="Link to this heading">¶</a></h2>
<ul>
<li><p><strong>Penalty</strong>:
<span class="math notranslate nohighlight">\(\Omega(\mathbf{w}) = \|\mathbf{w}\|_1 = \sum_{j} |w_j|\)</span></p></li>
<li><p><strong>Objective function</strong>:</p>
<div class="math notranslate nohighlight">
\[\text{Lasso}(\mathbf{w}) = \sum_i^N (y_i - \mathbf{x}_i^\top\mathbf{w})^2 + \lambda \sum_{j} |w_j|\]</div>
</li>
<li><p><strong>Effect</strong>: Promotes <strong>sparsity</strong>, i.e., forces some coefficients to
be exactly zero, ie. performs feature selection.</p></li>
<li><p><strong>No closed-form solution</strong>, It is convex but not differentiable.
Requires specific optimization algorithms, such as the fast iterative
shrinkage-thresholding algorithm (FISTA): Amir Beck and Marc Teboulle,
<em>A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse
Problems</em> SIAM J. Imaging Sci., 2009.</p></li>
</ul>
<section id="geometric-interpretation">
<h3>Geometric interpretation<a class="headerlink" href="#geometric-interpretation" title="Link to this heading">¶</a></h3>
<figure class="align-default" id="id5">
<img alt="Sparsity of L1 norm" src="../_images/l1_sparse.png" />
<figcaption>
<p><span class="caption-text">Sparsity of L1 norm</span><a class="headerlink" href="#id5" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="elastic-net">
<h2>Elastic Net<a class="headerlink" href="#elastic-net" title="Link to this heading">¶</a></h2>
<ul class="simple">
<li><p><strong>Penalty</strong>: Combination of L1 and L2:
<span class="math notranslate nohighlight">\(\Omega(\mathbf{w}) = \alpha \|\mathbf{w}\|_1 + (1 - \alpha) \|\mathbf{w}\|_2^2\)</span>
where <span class="math notranslate nohighlight">\(\alpha \in [0, 1]\)</span> controls the mix of Lasso and Ridge.</p></li>
<li><p><strong>Effect</strong>: Balances sparsity (L1) and stability (L2), especially
useful when features are correlated.</p></li>
</ul>
<p>The Elastic-net estimator combines the <span class="math notranslate nohighlight">\(\ell_1\)</span> and <span class="math notranslate nohighlight">\(\ell_2\)</span>
penalties, and results in the problem to</p>
<div class="math notranslate nohighlight">
\[\text{Enet}(\mathbf{w}) = \sum_i^N (y_i - \mathbf{x}_i^\top\mathbf{w})^2 + \alpha \left(\rho~\|\mathbf{w}\|_1 + (1-\rho)~\|\mathbf{w}\|_2^2 \right),\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> acts as a global penalty and <span class="math notranslate nohighlight">\(\rho\)</span> as an
<span class="math notranslate nohighlight">\(\ell_1 / \ell_2\)</span> ratio.</p>
<section id="rational">
<h3>Rational<a class="headerlink" href="#rational" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p>If there are groups of highly correlated variables, Lasso tends to
arbitrarily select only one from each group. These models are
difficult to interpret because covariates that are strongly associated
with the outcome are not included in the predictive model. Conversely,
the elastic net encourages a grouping effect, where strongly
correlated predictors tend to be in or out of the model together.</p></li>
<li><p>Studies on real world data and simulation studies show that the
elastic net often outperforms the lasso, while enjoying a similar
sparsity of representation.</p></li>
</ul>
</section>
<section id="penalization-summary-and-and-application-with-scikit-learn">
<h3>Penalization: summary and and Application with scikit-learn<a class="headerlink" href="#penalization-summary-and-and-application-with-scikit-learn" title="Link to this heading">¶</a></h3>
<section id="summary">
<h4>Summary<a class="headerlink" href="#summary" title="Link to this heading">¶</a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Penalty
Type</p></th>
<th class="head"><p>Promotes
Sparsity</p></th>
<th class="head"><p>Handles
M
ulticollinearity</p></th>
<th class="head"><p>Closed-Form
Solution</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Ridge</p></td>
<td><p>L2:
:m
ath:<cite>sum
_j w_j^2</cite></p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-odd"><td><p>Lasso</p></td>
<td><p>L1:
:mat
h:<a href="#id1"><span class="problematic" id="id2">`</span></a>sum_j</p>
<blockquote>
<div><p>|w_j|`</p>
</div></blockquote>
</td>
<td><p>Yes</p></td>
<td><p>No (can be
unstable)</p></td>
<td><p>No</p></td>
</tr>
<tr class="row-even"><td><p>E
lastic
Net</p></td>
<td><p>L1 + L2</p></td>
<td><p>Yes (less
than
Lasso)</p></td>
<td><p>Yes</p></td>
<td><p>No</p></td>
</tr>
</tbody>
</table>
</section>
<section id="effect-on-problems-with-increasing-dimension">
<h4>Effect on problems with increasing dimension<a class="headerlink" href="#effect-on-problems-with-increasing-dimension" title="Link to this heading">¶</a></h4>
<p>The next figure shows the predicted performance (r-squared) on train and
test sets with an increasing number of input features. The number of
predictive features is always 10% of the total number of input features.
Therefore, the signal to noise ratio (SNR) increases by increasing the
number of input features. The performances on the training set rapidly
reach 100% (R2=1). However, the performance on the test set decreases
with the increase of the input dimensionality. The difference between
the train and test performances (blue shaded region) depicts the
overfitting phenomena. Regularization using penalties of the coefficient
vector norm greatly limits the overfitting phenomena.</p>
</section>
<section id="effect-on-solution-finding">
<h4>Effect on solution finding<a class="headerlink" href="#effect-on-solution-finding" title="Link to this heading">¶</a></h4>
<p>The ridge penalty shrinks the coefficients toward zero. The figure
illustrates: the OLS solution on the left. The <span class="math notranslate nohighlight">\(\ell_1\)</span> and
<span class="math notranslate nohighlight">\(\ell_2\)</span> penalties in the middle pane. The penalized OLS in the
right pane. The right pane shows how the penalties shrink the
coefficients toward zero. The black points are the minimum found in each
case, and the white points represents the true solution used to generate
the data.</p>
<figure class="align-default" id="id6">
<img alt=":math:`\ell_1` and :math:`\ell_2` shrinkages" src="../_images/ols_l1_l2.png" />
<figcaption>
<p><span class="caption-text"><span class="math notranslate nohighlight">\(\ell_1\)</span> and <span class="math notranslate nohighlight">\(\ell_2\)</span> shrinkages</span><a class="headerlink" href="#id6" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="application-with-scikit-learn">
<h4>Application with scikit-learn<a class="headerlink" href="#application-with-scikit-learn" title="Link to this heading">¶</a></h4>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda is alpha!</span>

<span class="n">l1</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  <span class="c1"># lambda is alpha !</span>

<span class="n">l1l2</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">ElasticNet</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">.1</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">.9</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">coef</span><span class="p">,</span> <span class="n">lr</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">l1l2</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span>
             <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;True&#39;</span><span class="p">,</span> <span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="s1">&#39;l1l2&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>          <span class="mi">0</span>     <span class="mi">1</span>      <span class="mi">2</span>     <span class="mi">3</span>      <span class="mi">4</span>      <span class="mi">5</span>      <span class="mi">6</span>     <span class="mi">7</span>     <span class="mi">8</span>     <span class="mi">9</span>
<span class="kc">True</span>  <span class="mf">28.49</span>  <span class="mf">0.00</span>  <span class="mf">13.17</span>  <span class="mf">0.00</span>  <span class="mf">48.97</span>  <span class="mf">70.44</span>  <span class="mf">39.70</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span>
<span class="n">lr</span>    <span class="mf">28.49</span>  <span class="mf">0.00</span>  <span class="mf">13.17</span>  <span class="mf">0.00</span>  <span class="mf">48.97</span>  <span class="mf">70.44</span>  <span class="mf">39.70</span>  <span class="mf">0.00</span>  <span class="mf">0.00</span> <span class="o">-</span><span class="mf">0.00</span>
<span class="n">l2</span>     <span class="mf">1.03</span>  <span class="mf">0.21</span>   <span class="mf">0.93</span> <span class="o">-</span><span class="mf">0.32</span>   <span class="mf">1.82</span>   <span class="mf">1.57</span>   <span class="mf">2.10</span> <span class="o">-</span><span class="mf">1.14</span> <span class="o">-</span><span class="mf">0.84</span> <span class="o">-</span><span class="mf">1.02</span>
<span class="n">l1</span>     <span class="mf">0.00</span> <span class="o">-</span><span class="mf">0.00</span>   <span class="mf">0.00</span> <span class="o">-</span><span class="mf">0.00</span>  <span class="mf">24.40</span>  <span class="mf">25.16</span>  <span class="mf">25.36</span> <span class="o">-</span><span class="mf">0.00</span> <span class="o">-</span><span class="mf">0.00</span> <span class="o">-</span><span class="mf">0.00</span>
<span class="n">l1l2</span>   <span class="mf">0.78</span>  <span class="mf">0.00</span>   <span class="mf">0.51</span> <span class="o">-</span><span class="mf">0.00</span>   <span class="mf">7.20</span>   <span class="mf">5.71</span>   <span class="mf">8.95</span> <span class="o">-</span><span class="mf">1.38</span> <span class="o">-</span><span class="mf">0.00</span> <span class="o">-</span><span class="mf">0.40</span>
</pre></div>
</div>
</section>
</section>
<section id="sparsity-of-the-ell-1-norm">
<h3>Sparsity of the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm<a class="headerlink" href="#sparsity-of-the-ell-1-norm" title="Link to this heading">¶</a></h3>
<section id="occams-razor">
<h4>Occam’s razor<a class="headerlink" href="#occams-razor" title="Link to this heading">¶</a></h4>
<p>Occam’s razor (also written as Ockham’s razor, and <strong>lex parsimoniae</strong>
in Latin, which means law of parsimony) is a problem solving principle
attributed to William of Ockham (1287-1347), who was an English
Franciscan friar and scholastic philosopher and theologian. The
principle can be interpreted as stating that <strong>among competing
hypotheses, the one with the fewest assumptions should be selected</strong>.</p>
</section>
<section id="principle-of-parsimony">
<h4>Principle of parsimony<a class="headerlink" href="#principle-of-parsimony" title="Link to this heading">¶</a></h4>
<p>The simplest of two competing theories is to be preferred. Definition of
parsimony: Economy of explanation in conformity with Occam’s razor.</p>
<p>Among possible models with similar loss, choose the simplest one:</p>
<ul class="simple">
<li><p>Choose the model with the smallest coefficient vector, i.e. smallest
<span class="math notranslate nohighlight">\(\ell_2\)</span> (<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_2\)</span>) or <span class="math notranslate nohighlight">\(\ell_1\)</span>
(<span class="math notranslate nohighlight">\(\|\mathbf{w}\|_1\)</span>) norm of <span class="math notranslate nohighlight">\(\mathbf{w}\)</span>,
i.e. <span class="math notranslate nohighlight">\(\ell_2\)</span> or <span class="math notranslate nohighlight">\(\ell_1\)</span> penalty. See also bias-variance
tradeoff.</p></li>
<li><p>Choose the model that uses the smallest number of predictors. In other
words, choose the model that has many predictors with zero weights.
Two approaches are available to obtain this: (i) Perform a feature
selection as a preprocessing prior to applying the learning algorithm,
or (ii) embed the feature selection procedure within the learning
process.</p></li>
</ul>
</section>
</section>
</section>
<section id="regression-performance-evaluation-metrics-r-squared-mse-and-mae">
<h2>Regression performance evaluation metrics: R-squared, MSE and MAE<a class="headerlink" href="#regression-performance-evaluation-metrics-r-squared-mse-and-mae" title="Link to this heading">¶</a></h2>
<p>Common regression <a class="reference external" href="https://scikit-learn.org/stable/modules/model_evaluation.html">Scikit-learn
Metrics</a>
are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(R^2\)</span> : R-squared</p></li>
<li><p>MSE: Mean Squared Error</p></li>
<li><p>MAE: Mean Absolute Error</p></li>
</ul>
<section id="r-squared">
<h3>R-squared<a class="headerlink" href="#r-squared" title="Link to this heading">¶</a></h3>
<p>The goodness of fit of a statistical model describes how well it fits a
set of observations. Measures of goodness of fit typically summarize the
discrepancy between observed values and the values expected under the
model in question. We will consider the <strong>explained variance</strong> also
known as the coefficient of determination, denoted <span class="math notranslate nohighlight">\(R^2\)</span>
pronounced <strong>R-squared</strong>.</p>
<p>The total sum of squares, <span class="math notranslate nohighlight">\(SS_\text{tot}\)</span> is the sum of the sum of
squares explained by the regression, <span class="math notranslate nohighlight">\(SS_\text{reg}\)</span>, plus the sum
of squares of residuals unexplained by the regression,
<span class="math notranslate nohighlight">\(SS_\text{res}\)</span>, also called the SSE, i.e. such that</p>
<div class="math notranslate nohighlight">
\[SS_\text{tot} = SS_\text{reg} + SS_\text{res}\]</div>
<figure class="align-default" id="id7">
<img alt="title" src="../_images/Coefficient_of_Determination.png" />
<figcaption>
<p><span class="caption-text">title</span><a class="headerlink" href="#id7" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>The mean of <span class="math notranslate nohighlight">\(y\)</span> is</p>
<div class="math notranslate nohighlight">
\[\bar{y} = \frac{1}{n}\sum_i y_i.\]</div>
<p>The total sum of squares is the total squared sum of deviations from the
mean of <span class="math notranslate nohighlight">\(y\)</span>, i.e.</p>
<div class="math notranslate nohighlight">
\[SS_\text{tot}=\sum_i (y_i-\bar{y})^2\]</div>
<p>The regression sum of squares, also called the explained sum of squares:</p>
<div class="math notranslate nohighlight">
\[SS_\text{reg} = \sum_i (\hat{y}_i -\bar{y})^2,\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{y}_i = \beta x_i + \beta_0\)</span> is the estimated value of
salary <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> given a value of experience <span class="math notranslate nohighlight">\(x_i\)</span>.</p>
<p>The sum of squares of the residuals (<strong>SSE, Sum Squared Error</strong>), also
called the residual sum of squares (RSS) is:</p>
<div class="math notranslate nohighlight">
\[SS_\text{res}=\sum_i (y_i - \hat{y_i})^2.\]</div>
<p><span class="math notranslate nohighlight">\(R^2\)</span> is the explained sum of squares of errors. It is the
variance explain by the regression divided by the total variance, i.e.</p>
<div class="math notranslate nohighlight">
\[R^2 = \frac{\text{explained SS}}{\text{total SS}}
    = \frac{SS_\text{reg}}{SS_{tot}}
    = 1 - {SS_{res}\over SS_{tot}}.\]</div>
<p><em>Test</em></p>
<p>Let <span class="math notranslate nohighlight">\(\hat{\sigma}^2 = SS_\text{res} / (n-2)\)</span> be an estimator of
the variance of <span class="math notranslate nohighlight">\(\epsilon\)</span>. The <span class="math notranslate nohighlight">\(2\)</span> in the denominator stems
from the 2 estimated parameters: intercept and coefficient.</p>
<ul class="simple">
<li><p><strong>Unexplained variance</strong>:
<span class="math notranslate nohighlight">\(\frac{SS_\text{res}}{\hat{\sigma}^2} \sim \chi_{n-2}^2\)</span></p></li>
<li><p><strong>Explained variance</strong>:
<span class="math notranslate nohighlight">\(\frac{SS_\text{reg}}{\hat{\sigma}^2} \sim \chi_{1}^2\)</span>. The
single degree of freedom comes from the difference between
<span class="math notranslate nohighlight">\(\frac{SS_\text{tot}}{\hat{\sigma}^2} (\sim \chi^2_{n-1})\)</span> and
<span class="math notranslate nohighlight">\(\frac{SS_\text{res}}{\hat{\sigma}^2} (\sim \chi_{n-2}^2)\)</span>,
i.e. <span class="math notranslate nohighlight">\((n-1) - (n-2)\)</span> degree of freedom.</p></li>
</ul>
<p>The Fisher statistics of the ratio of two variances:</p>
<div class="math notranslate nohighlight">
\[F = \frac{\text{Explained variance}}{\text{Unexplained variance}} = \frac{SS_\text{reg} / 1}{ SS_\text{res} / (n - 2)} \sim F(1, n-2)\]</div>
<p>Using the <span class="math notranslate nohighlight">\(F\)</span>-distribution, compute the probability of observing a
value greater than <span class="math notranslate nohighlight">\(F\)</span> under <span class="math notranslate nohighlight">\(H_0\)</span>, i.e.:
<span class="math notranslate nohighlight">\(P(x &gt; F|H_0)\)</span>, i.e. the survival function
<span class="math notranslate nohighlight">\((1 - \text{Cumulative Distribution Function})\)</span> at <span class="math notranslate nohighlight">\(x\)</span> of
the given <span class="math notranslate nohighlight">\(F\)</span>-distribution.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">metrics</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">make_regression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="n">lr</span> <span class="o">=</span> <span class="n">lm</span><span class="o">.</span><span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">r2</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;r2: </span><span class="si">%.3f</span><span class="s2">, mae: </span><span class="si">%.3f</span><span class="s2">, mse: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">r2</span><span class="p">:</span> <span class="mf">0.053</span><span class="p">,</span> <span class="n">mae</span><span class="p">:</span> <span class="mf">71.712</span><span class="p">,</span> <span class="n">mse</span><span class="p">:</span> <span class="mf">7866.759</span>
</pre></div>
</div>
<p>In pure numpy:</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">res</span> <span class="o">=</span> <span class="n">y_test</span> <span class="o">-</span> <span class="n">lr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">y_mu</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
<span class="n">ss_tot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_test</span> <span class="o">-</span> <span class="n">y_mu</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">ss_res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">res</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">r2</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ss_res</span> <span class="o">/</span> <span class="n">ss_tot</span><span class="p">)</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">res</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">res</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;r2: </span><span class="si">%.3f</span><span class="s2">, mae: </span><span class="si">%.3f</span><span class="s2">, mse: </span><span class="si">%.3f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">r2</span><span class="p">,</span> <span class="n">mae</span><span class="p">,</span> <span class="n">mse</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">r2</span><span class="p">:</span> <span class="mf">0.053</span><span class="p">,</span> <span class="n">mae</span><span class="p">:</span> <span class="mf">71.712</span><span class="p">,</span> <span class="n">mse</span><span class="p">:</span> <span class="mf">7866.759</span>
</pre></div>
</div>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
  <div>
    <h3><a href="../index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Linear Models for Regression</a><ul>
<li><a class="reference internal" href="#minimizing-the-sum-of-squared-errors-sse-loss">Minimizing the Sum of Squared Errors (SSE) Loss</a></li>
<li><a class="reference internal" href="#linear-regression-with-scikit-learn">Linear regression with scikit-learn</a></li>
<li><a class="reference internal" href="#regularization-using-penalization-of-coefficients">Regularization using penalization of coefficients</a></li>
<li><a class="reference internal" href="#ridge-regression-ell-2-or-l2-regularization">Ridge Regression (<span class="math notranslate nohighlight">\(\ell_2\)</span> or L2 regularization)</a><ul>
<li><a class="reference internal" href="#numerical-interpretation">Numerical Interpretation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#lasso-regression-ell-1-or-l1-regularization">Lasso Regression (<span class="math notranslate nohighlight">\(\ell_1\)</span> or L1 regularization)</a><ul>
<li><a class="reference internal" href="#geometric-interpretation">Geometric interpretation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#elastic-net">Elastic Net</a><ul>
<li><a class="reference internal" href="#rational">Rational</a></li>
<li><a class="reference internal" href="#penalization-summary-and-and-application-with-scikit-learn">Penalization: summary and and Application with scikit-learn</a><ul>
<li><a class="reference internal" href="#summary">Summary</a></li>
<li><a class="reference internal" href="#effect-on-problems-with-increasing-dimension">Effect on problems with increasing dimension</a></li>
<li><a class="reference internal" href="#effect-on-solution-finding">Effect on solution finding</a></li>
<li><a class="reference internal" href="#application-with-scikit-learn">Application with scikit-learn</a></li>
</ul>
</li>
<li><a class="reference internal" href="#sparsity-of-the-ell-1-norm">Sparsity of the <span class="math notranslate nohighlight">\(\ell_1\)</span> norm</a><ul>
<li><a class="reference internal" href="#occams-razor">Occam’s razor</a></li>
<li><a class="reference internal" href="#principle-of-parsimony">Principle of parsimony</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#regression-performance-evaluation-metrics-r-squared-mse-and-mae">Regression performance evaluation metrics: R-squared, MSE and MAE</a><ul>
<li><a class="reference internal" href="#r-squared">R-squared</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/ml_supervised/linear_regression.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2020, Edouard Duchesnay, NeuroSpin CEA Université Paris-Saclay, France.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.2.3</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/ml_supervised/linear_regression.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>