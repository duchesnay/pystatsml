{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models for Regression\n",
    "\n",
    "![Linear regression](images/linear_regression.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing the Sum of Squared Errors (SSE) Loss\n",
    "\n",
    "Linear regression models the **output**, or **target** variable $y \\in \\mathrm{R}$ as a linear combination of the $P$-dimensional input $\\mathbf{x} \\in \\mathbb{R}^{P}$. Let $\\mathbf{X}$ be the $N \\times P$ matrix with each row an input vector (with a 1 in the first position), and similarly let $\\mathbf{y}$ be the $N$-dimensional vector of outputs in the **training set**, the linear model will predict the $\\mathbf{y}$ given $\\mathbf{x}$ using the **parameter vector**, or **weight vector** $\\mathbf{w} \\in \\mathbb{R}^P$ according to\n",
    "\n",
    "$$\n",
    "y_i = \\mathbf{x}_i^T \\mathbf{w}+ \\varepsilon_i, \n",
    "$$\n",
    "for all observation $i$. For the whole dataset this is equivalent to:\n",
    "\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X} \\mathbf{w} + \\boldsymbol{\\varepsilon},\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\varepsilon} \\in \\mathrm{R}^N$ are the **residuals** ($\\varepsilon_i$), or the errors of the prediction. The $\\mathbf{w}$ is found by minimizing an **objective function**, which is the **loss function**, $L(\\mathbf{w})$, i.e. the error measured on the data. This error is the **sum of squared errors (SSE) loss**.\n",
    "\n",
    "\\begin{align}\n",
    "L(\\mathbf{w}) &= \\text{SSE}(\\mathbf{w})\\\\\n",
    "               &= \\sum_i^N (y_i - \\mathbf{x}_i^T\\mathbf{w})^2\\\\\n",
    "               &= (\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w})\\\\\n",
    "               &= \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2,\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Minimizing the SSE is the Ordinary Least Square **OLS** regression as objective function.\n",
    "which is a simple **ordinary least squares (OLS)** minimization whose analytic solution is:\n",
    "$$\n",
    "\\mathbf{w}_{\\text{OLS}} = (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "$$\n",
    "\n",
    "The solution could also be found using gradient descent using the gradient of the loss:\n",
    "$$\n",
    "\\partial\\frac{L(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})}{\\partial\\mathbf{w}} = 2 \\sum_i \\mathbf{x}_i (\\mathbf{x}_i \\cdot \\mathbf{w} - y_i)\n",
    "$$\n",
    "\n",
    "Linear regression of `Advertising.csv` dataset with TV and Radio advertising as input features and Sales as target. The linear model that minimizes the MSE is a plan (2 input features) defined as: Sales = 0.05 TV + .19 Radio + 3:\n",
    "\n",
    "![Linear regression](images/linear_regression_plan.png){width=10cm}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with scikit-learn\n",
    "\n",
    "Scikit-learn offers many models for supervised learning, and they all follow the same **Application Programming Interface (API)**, namely:\n",
    "\n",
    "```\n",
    "est = Estimator()\n",
    "est.fit(X, y)\n",
    "predictions = est.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "\n",
    "from sklearn import datasets\n",
    "import sklearn.linear_model as lm\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset with some correlation\n",
    "\n",
    "X, y, coef = datasets.make_regression(n_samples=100, n_features=10,\n",
    "                                      n_informative=5, random_state=0,\n",
    "                                      effective_rank=3, coef=True)\n",
    "\n",
    "lr = lm.LinearRegression().fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization using penalization of coefficients\n",
    "\n",
    "Regarding linear models, overfitting generally leads to excessively complex solutions (coefficient vectors), accounting for noise or spurious correlations within predictors. **Regularization** aims to alleviate this phenomenon by constraining (biasing or reducing) the capacity of the learning algorithm in order to promote simple solutions. Regularization penalizes \"large\" solutions forcing the coefficients to be small, i.e. to shrink them toward zeros.\n",
    "\n",
    "The objective function $J(\\mathbf{w})$ to minimize with respect to $\\mathbf{w}$ is composed of a loss function $L(\\mathbf{w})$ for goodness-of-fit and a penalty term $\\Omega(\\mathbf{w})$ (regularization to avoid overfitting). This is a trade-off where the respective contribution of the loss and the penalty terms is controlled by the regularization parameter $\\lambda$.\n",
    "\n",
    "Therefore the **loss function** $L(\\mathbf{w})$ is combined with a **penalty function** $\\Omega(\\mathbf{w})$ leading to the general form:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = L(\\mathbf{w}) + \\lambda \\Omega(\\mathbf{w}).\n",
    "$$\n",
    "\n",
    "The respective contribution of the loss and the penalty is controlled by the **regularization parameter** $\\lambda$.\n",
    "\n",
    "Popular penalties are:\n",
    "\n",
    "- Ridge (also called $\\ell_2$) penalty: $\\|\\mathbf{w}\\|_2^2$. It shrinks coefficients toward 0.\n",
    "- Lasso (also called $\\ell_1$) penalty: $\\|\\mathbf{w}\\|_1$. It performs feature selection by setting some coefficients to 0.\n",
    "- ElasticNet (also called $\\ell_1\\ell_2$) penalty: $\\alpha \\left(\\rho~\\|\\mathbf{w}\\|_1 + (1-\\rho)~\\|\\mathbf{w}\\|_2^2 \\right)$. It performs selection of group of correlated features by setting some coefficients to 0.\n",
    "\n",
    "\n",
    "The next figure shows the predicted performance (r-squared) on train and test sets with an increasing number of input features. The number of predictive features is always 10% of the total number of input features. Therefore, the signal to noise ratio (SNR) increases by increasing the number of input features. The performances on the training set rapidly reach 100% (R2=1). However, the performance on the test set decreases with the increase of the input dimensionality. The difference between the train and test performances (blue shaded region) depicts the overfitting phenomena. Regularisation using penalties of the coefficient vector norm greatly limits the overfitting phenomena. \n",
    "\n",
    "![Penalties: prediction performances as a function of number of input features](images/linear_regression_penalties.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = lm.Ridge(alpha=10).fit(X, y)  # lambda is alpha!\n",
    "\n",
    "l1 = lm.Lasso(alpha=.1).fit(X, y)  # lambda is alpha !\n",
    "\n",
    "l1l2 = lm.ElasticNet(alpha=.1, l1_ratio=.9).fit(X, y)\n",
    "\n",
    "print(pd.DataFrame(np.vstack((coef, lr.coef_, l2.coef_, l1.coef_, l1l2.coef_)),\n",
    "             index=['True', 'lr', 'l2', 'l1', 'l1l2']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge regression ($\\ell_2$-regularization)\n",
    "\n",
    "Ridge regression impose a $\\ell_2$ penalty on the coefficients, i.e. it penalizes with the Euclidean norm of the coefficients while minimizing SSE. The objective function becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Ridge}(\\mathbf{w}) &= \\sum_i^N (y_i - \\mathbf{x}_i^T\\mathbf{w})^2 + \\lambda \\|\\mathbf{w}\\|_2^2\\\\\n",
    "&= \\|\\mathbf{y} - \\mathbf{x}\\mathbf{w}\\|_2^2 + \\lambda \\|\\mathbf{w}\\|_2^2.\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The $\\mathbf{w}$ that minimises $F_{Ridge}(\\mathbf{w})$ can be found by the following derivation:\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla_{\\mathbf{w}}\\text{Ridge}(\\mathbf{w}) &= 0\\\\\n",
    "\\nabla_{\\mathbf{w}}\\big((\\mathbf{y} - \\mathbf{X}\\mathbf{w})^T (\\mathbf{y} - \\mathbf{X}\\mathbf{w}) + \\lambda \\mathbf{w}^T\\mathbf{w}\\big) &= 0\\\\\n",
    "\\nabla_{\\mathbf{w}}\\big((\\mathbf{y}^T\\mathbf{y} - 2 \\mathbf{w}^T\\mathbf{X}^T\\mathbf{y} + \\mathbf{w}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{w} + \\lambda \\mathbf{w}^T\\mathbf{w})\\big) &= 0\\\\\n",
    "-2\\mathbf{X}^T\\mathbf{y} + 2 \\mathbf{X}^T\\mathbf{X}\\mathbf{w} + 2 \\lambda \\mathbf{w} &= 0\\\\\n",
    "-\\mathbf{X}^T\\mathbf{y} + (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}) \\mathbf{w} &= 0\\\\\n",
    "(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}) \\mathbf{w} &= \\mathbf{X}^T\\mathbf{y}\\\\\n",
    "\\mathbf{w} &= (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "- The solution adds a positive constant to the diagonal of $\\mathbf{X}^T\\mathbf{X}$ before inversion. This makes the problem nonsingular, even if $\\mathbf{X}^T\\mathbf{X}$ is not of full rank, and was the main motivation behind ridge regression.\n",
    "\n",
    "- Increasing $\\lambda$ shrinks the $\\mathbf{w}$ coefficients toward 0.\n",
    "\n",
    "- This approach **penalizes** the objective function by the **Euclidian ($\\ell_2$) norm** of the coefficients such that solutions with large coefficients become unattractive.\n",
    "\n",
    "The gradient of the loss:\n",
    "$$\n",
    "\\partial\\frac{L(\\mathbf{w}, \\mathbf{X}, \\mathbf{y})}{\\partial\\mathbf{w}} = 2 (\\sum_i \\mathbf{x}_i (\\mathbf{x}_i \\cdot \\mathbf{w} - y_i) + \\lambda \\mathbf{w})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression ($\\ell_1$-regularization)\n",
    "\n",
    "Lasso regression penalizes the coefficients by the $\\ell_1$ norm. This constraint will reduce (bias) the capacity of the learning algorithm. To add such a penalty forces the coefficients to be small, i.e. it shrinks them toward zero. The objective function to minimize becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Lasso}(\\mathbf{w}) &= \\sum_i^N (y_i - \\mathbf{x}_i^T\\mathbf{w})^2 + \\lambda\\|\\mathbf{w}\\|_1.\n",
    "\\end{align}\n",
    "\n",
    "This penalty forces some coefficients to be exactly zero, providing a feature selection property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparsity of the $\\ell_1$ norm\n",
    "\n",
    "#### Occam's razor\n",
    "\n",
    "Occam's razor (also written as Ockham's razor, and **lex parsimoniae** in Latin, which means law of parsimony) is a problem solving principle attributed to William of Ockham (1287-1347), who was an English Franciscan friar and scholastic philosopher and theologian. The principle can be interpreted as stating that **among competing hypotheses, the one with the fewest assumptions should be selected**.\n",
    "\n",
    "#### Principle of parsimony\n",
    "\n",
    "The simplest of two competing theories is to be preferred. Definition of parsimony: Economy of explanation in conformity with Occam's razor.\n",
    "\n",
    "Among possible models with similar loss, choose the simplest one: \n",
    "\n",
    "- Choose the model with the smallest coefficient vector, i.e. smallest $\\ell_2$ ($\\|\\mathbf{w}\\|_2$) or $\\ell_1$ ($\\|\\mathbf{w}\\|_1$) norm of $\\mathbf{w}$, i.e. $\\ell_2$ or $\\ell_1$ penalty. See also bias-variance tradeoff.\n",
    "\n",
    "- Choose the model that uses the smallest number of predictors. In other words, choose the model that has many predictors with zero weights. Two approaches are available to obtain this: (i) Perform a feature selection as a preprocessing prior to applying the learning algorithm, or (ii) embed the feature selection procedure within the learning process.\n",
    "\n",
    "#### Sparsity-induced penalty or embedded feature selection with the $\\ell_1$ penalty\n",
    "\n",
    "The penalty based on the $\\ell_1$ norm promotes **sparsity** (scattered, or not dense): it forces many coefficients to be exactly zero. This also makes the coefficient vector scattered.\n",
    "\n",
    "The figure bellow illustrates the OLS loss under a constraint acting on the $\\ell_1$ norm of the coefficient vector. I.e., it illustrates the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\underset{\\mathbf{w}}{\\text{minimize}} ~& \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2 \\\\\n",
    "    \\text{subject to}                 ~& \\|\\mathbf{w}\\|_1 \\leq 1.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![Sparsity of L1 norm](images/shrinkage/l1_sparse.png)\n",
    "\n",
    "### Optimization issues\n",
    "\n",
    "*Section to be completed*\n",
    "\n",
    "- No more closed-form solution.\n",
    "\n",
    "- Convex but not differentiable.\n",
    "\n",
    "- Requires specific optimization algorithms, such as the fast iterative shrinkage-thresholding algorithm (FISTA): Amir Beck and Marc Teboulle, *A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems* SIAM J. Imaging Sci., 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ridge penalty shrinks the coefficients toward zero. The figure illustrates: the OLS solution on the left. The $\\ell_1$ and $\\ell_2$ penalties in the middle pane. The penalized OLS in the right pane. The right pane shows how the penalties shrink the coefficients toward zero. The black points are the minimum found in each case, and the white points represents the true solution used to generate the data.\n",
    "\n",
    "![$\\ell_1$ and $\\ell_2$ shrinkages](images/shrinkage/ols_l1_l2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic-net regression ($\\ell_1$-$\\ell_2$-regularization)\n",
    "\n",
    "The Elastic-net estimator combines the $\\ell_1$ and $\\ell_2$ penalties, and results in the problem to\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Enet}(\\mathbf{w}) &= \\sum_i^N (y_i - \\mathbf{x}_i^T\\mathbf{w})^2 + \\alpha \\left(\\rho~\\|\\mathbf{w}\\|_1 + (1-\\rho)~\\|\\mathbf{w}\\|_2^2 \\right),\n",
    "\\end{align}\n",
    "\n",
    "where $\\alpha$ acts as a global penalty and $\\rho$ as an $\\ell_1 / \\ell_2$ ratio.\n",
    "\n",
    "### Rational\n",
    "\n",
    "- If there are groups of highly correlated variables, Lasso tends to arbitrarily select only one from each group. These models are difficult to interpret because covariates that are strongly associated with the outcome are not included in the predictive model. Conversely, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.\n",
    "\n",
    "- Studies on real world data and simulation studies show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression performance evaluation metrics: R-squared, MSE and MAE\n",
    "\n",
    "Common regression [metrics](https://scikit-learn.org/stable/modules/model_evaluation.html) are:\n",
    "\n",
    "- $R^2$ : R-squared\n",
    "- MSE: Mean Squared Error\n",
    "- MAE: Mean Absolute Error\n",
    "\n",
    "\n",
    "### R-squared\n",
    "\n",
    "The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. We will consider the **explained variance**  also known as the coefficient of determination, denoted $R^2$ pronounced **R-squared**.\n",
    "\n",
    "The total sum of squares, $SS_\\text{tot}$ is the sum of the sum of squares explained by the regression, $SS_\\text{reg}$, plus the sum of squares of residuals unexplained by the regression, $SS_\\text{res}$, also called the SSE, i.e. such that\n",
    "\n",
    "$$\n",
    "SS_\\text{tot} = SS_\\text{reg} + SS_\\text{res}\n",
    "$$\n",
    "\n",
    "![title](images/Coefficient_of_Determination.png)\n",
    "\n",
    "The mean of $y$ is\n",
    "\n",
    "$$\n",
    "\\bar{y} = \\frac{1}{n}\\sum_i y_i.\n",
    "$$\n",
    "\n",
    "The total sum of squares is the total squared sum of deviations from the mean of $y$, i.e.\n",
    "\n",
    "$$\n",
    "SS_\\text{tot}=\\sum_i (y_i-\\bar{y})^2\n",
    "$$\n",
    "\n",
    "The regression sum of squares, also called the explained sum of squares:\n",
    "\n",
    "$$\n",
    "SS_\\text{reg} = \\sum_i (\\hat{y}_i -\\bar{y})^2,\n",
    "$$\n",
    "\n",
    "where $\\hat{y}_i = \\beta x_i + \\beta_0$ is the estimated value of salary $\\hat{y}_i$ given a value of experience $x_i$.\n",
    "\n",
    "The sum of squares of the residuals (**SSE, Sum Squared Error**), also called the residual sum of squares (RSS) is:\n",
    "\n",
    "$$\n",
    "SS_\\text{res}=\\sum_i (y_i - \\hat{y_i})^2.\n",
    "$$\n",
    "\n",
    "$R^2$ is the explained sum of squares of errors. It is the variance explain by the regression divided by the total variance, i.e.\n",
    "\n",
    "$$\n",
    "R^2 = \\frac{\\text{explained SS}}{\\text{total SS}}\n",
    "    = \\frac{SS_\\text{reg}}{SS_{tot}} \n",
    "    = 1 - {SS_{res}\\over SS_{tot}}.\n",
    "$$\n",
    "\n",
    "_Test_\n",
    "\n",
    "Let $\\hat{\\sigma}^2 = SS_\\text{res} / (n-2)$ be an estimator of the variance of $\\epsilon$. The $2$ in the denominator stems from the 2 estimated parameters: intercept and coefficient.\n",
    "\n",
    "- **Unexplained variance**: $\\frac{SS_\\text{res}}{\\hat{\\sigma}^2} \\sim \\chi_{n-2}^2$\n",
    "\n",
    "- **Explained variance**: $\\frac{SS_\\text{reg}}{\\hat{\\sigma}^2} \\sim \\chi_{1}^2$. The single degree of freedom comes from the difference between $\\frac{SS_\\text{tot}}{\\hat{\\sigma}^2} (\\sim \\chi^2_{n-1})$ and $\\frac{SS_\\text{res}}{\\hat{\\sigma}^2} (\\sim \\chi_{n-2}^2)$, i.e. $(n-1) - (n-2)$ degree of freedom.\n",
    "\n",
    "The Fisher statistics of the ratio of two variances:\n",
    "$$\n",
    "    F = \\frac{\\text{Explained variance}}{\\text{Unexplained variance}} = \\frac{SS_\\text{reg} / 1}{ SS_\\text{res} / (n - 2)} \\sim F(1, n-2)\n",
    "$$\n",
    "\n",
    "Using the $F$-distribution, compute the probability of observing a value greater than $F$ under $H_0$, i.e.: $P(x > F|H_0)$, i.e. the survival function $(1 - \\text{Cumulative Distribution Function})$ at $x$ of the given $F$-distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = datasets.make_regression(random_state=0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "lr = lm.LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "yhat = lr.predict(X_test)\n",
    "\n",
    "r2 = metrics.r2_score(y_test, yhat)\n",
    "mse = metrics.mean_squared_error(y_test, yhat)\n",
    "mae = metrics.mean_absolute_error(y_test, yhat)\n",
    "\n",
    "print(\"r2: %.3f, mae: %.3f, mse: %.3f\" % (r2, mae, mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pure numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = y_test - lr.predict(X_test)\n",
    "\n",
    "y_mu = np.mean(y_test)\n",
    "ss_tot = np.sum((y_test - y_mu) ** 2)\n",
    "ss_res = np.sum(res ** 2)\n",
    "\n",
    "r2 = (1 - ss_res / ss_tot)\n",
    "mse = np.mean(res ** 2)\n",
    "mae = np.mean(np.abs(res))\n",
    "\n",
    "print(\"r2: %.3f, mae: %.3f, mse: %.3f\" % (r2, mae, mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
