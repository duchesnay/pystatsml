{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Natural language processing\n",
    "\n",
    "\n",
    "1. Preparing text data (pre-processing)\n",
    "    - Standardization: removing irrelevant information, such as punctuation, special characters, lower-upper case, and stopwords.\n",
    "    - Tokenization (text splitting)\n",
    "    - Stemming/Lemmatization\n",
    "    \n",
    "2. Encode texts into a numerical vectors (features extraction)\n",
    "    - Bag of Words Vectorization-based Models: consider phrases as **sets** of words. Words are encoded as vectors independently of the context in which they appear in corpus.\n",
    "    - Embeding: phrases are **sequences** of words. words are encoded as vectors integrating their context of appearance in corpus.\n",
    "\n",
    "3. Predictive analysis\n",
    "\n",
    "    - Text classification: \"What’s the topic of this text?\"\n",
    "    - Content filtering: “Does this text contain abuse?”,  spam detection,\n",
    "    - [Sentiment analysis](https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/): Does this text sound positive or negative?\n",
    "\n",
    "3. Generate new text\n",
    "\n",
    "    - Translation\n",
    "    - Chatbot/summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preparing text data\n",
    "\n",
    "### Standardization and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "\n",
    "text = \"\"\"Check out the new http://example.com website! It's awesome.\n",
    "Hé, it is for programmers that like to program with programming language.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "The **Do It Yourself** way\n",
    "\n",
    "Basic standardization consist of:\n",
    "- Lower case words\n",
    "- Remove numbers\n",
    "- Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regex\n",
    "import re\n",
    "\n",
    "# Convert to lower case\n",
    "lower_string = text.lower()\n",
    " \n",
    "# Remove numbers\n",
    "no_number_string = re.sub(r'\\d+','', lower_string)\n",
    " \n",
    "# Remove all punctuation except words and space\n",
    "no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string) \n",
    " \n",
    "# Remove white spaces\n",
    "no_wspace_string = no_punc_string.strip()\n",
    "\n",
    "# Tokenization\n",
    "print(no_wspace_string.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "NLTK to perform more sophisticated standardization, including:\n",
    "\n",
    "Basic standardization consist of:\n",
    "- Lower case words\n",
    "- Remove URLs\n",
    "- Remove strip accents\n",
    "- **stop words** are commonly used words that are often removed from text during preprocessing to focus on the more informative words. These words typically include articles, prepositions, conjunctions, and pronouns such as \"the,\" \"is,\" \"in,\" \"and,\" \"but,\" \"on,\" etc. The rationale behind removing stop words is that they occur very frequently in the language and generally do not contribute significant meaning to the analysis or understanding of the text. By eliminating stop words, NLP models can reduce the dimensionality of the data and improve computational efficiency without losing important information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def strip_accents(text):\n",
    "    # Normalize the text to NFKD form and strip accents\n",
    "    text = unicodedata.normalize('NFKD', text)\n",
    "    text = ''.join([c for c in text if not unicodedata.combining(c)])\n",
    "    return text\n",
    "\n",
    "def standardize_tokenize(text, stemming=False, lemmatization=False):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    \n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    # string.punctuation provides a string of all punctuation characters.\n",
    "    # str.maketrans() creates a translation table that maps each punctuation\n",
    "    # character to None.\n",
    "    # text.translate(translator) uses this translation table to remove all \n",
    "    # punctuation characters from the input string.\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Strip accents\n",
    "    text = strip_accents(text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Remove repeated words\n",
    "    words = list(dict.fromkeys(words))\n",
    "    \n",
    "    # Initialize stemmer and lemmatizer\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Apply stemming and lemmatization\n",
    "\n",
    "    words = [stemmer.stem(word) for word in words] if stemming \\\n",
    "        else words\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word) for word in words] if lemmatization \\\n",
    "        else words\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Create callable with default values\n",
    "import functools\n",
    "standardize_tokenize_stemming = functools.partial(standardize_tokenize, stemming=True)\n",
    "standardize_tokenize_lemmatization = functools.partial(standardize_tokenize, lemmatization=True)\n",
    "standardize_tokenize_stemming_lemmatization = functools.partial(standardize_tokenize, stemming=True, lemmatization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "\n",
    "Stemming and lemmatization are techniques used to reduce words to their base or root form, which helps in standardizing text and improving the performance of various NLP tasks.\n",
    "\n",
    "**Stemming** is the process of reducing a word to its base or root form, often by removing suffixes or prefixes. The resulting stem may not be a valid word but is intended to capture the word's core meaning. Stemming algorithms, such as the Porter Stemmer or Snowball Stemmer, use heuristic rules to chop off common morphological endings from words.\n",
    "\n",
    "Example: The words \"running,\" \"runner,\" and \"ran\" might all be reduced to \"run.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize_tokenize(text, stemming=True)\n",
    "standardize_tokenize_stemming(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "**Lemmatization** is the process of reducing a word to its lemma, which is its canonical or dictionary form. Unlike stemming, lemmatization considers the word's part of speech and uses a more comprehensive approach to ensure that the transformed word is a valid word in the language. Lemmatization typically requires more linguistic knowledge and is implemented using libraries like WordNet.\n",
    "\n",
    "Example: The words \"running\" and \"ran\" would both be reduced to \"run,\" while \"better\" would be reduced to \"good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize_tokenize(text, lemmatization=True)\n",
    "standardize_tokenize_lemmatization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "While both stemming and lemmatization aim to reduce words to a common form, lemmatization is generally more accurate and produces words that are meaningful in the context of the language. However, stemming is faster and simpler to implement. The choice between the two depends on the specific requirements and constraints of the NLP task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize_tokenize(text, stemming=True, lemmatization=True)\n",
    "standardize_tokenize_stemming_lemmatization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "**Scikit-learn analyzer** is simple and will be sufficient most of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "analyzer = CountVectorizer(strip_accents='unicode', stop_words='english').build_analyzer()\n",
    "analyzer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Bag of Words (BOWs) Encoding\n",
    "\n",
    "[Source: text feature extraction with scikit-learn](https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "### Simple Count Vectorization\n",
    "\n",
    "[CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):_\"\n",
    "Convert a collection of text documents to a matrix of token counts. Note that `CountVectorizer` preforms the standardization and the tokenization.\"_\n",
    "\n",
    "It creates one feature (column) for each tokens (words) in the corpus, and returns one line per sentence, counting the occurence of each tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document. This DOCUMENT is in english.',\n",
    "    'in French, some letters have accents, like é.',\n",
    "    'Is this document in French?',\n",
    "]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(strip_accents='unicode', stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Note thatthe shape of the array is:\n",
    "# number of sentences by number of existing token \n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "**Word n-grams** are contiguous sequences of 'n' words from a given text. They are used to **capture the context** and structure of language by considering the relationships between words within these sequences. The value of 'n' determines the length of the word sequence:\n",
    "\n",
    "- Unigram (1-gram): A single word (e.g., \"natural\").\n",
    "- Bigram (2-gram): A sequence of two words (e.g., \"natural language\").\n",
    "- Trigram (3-gram): A sequence of three words (e.g., \"natural language processing\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2),\n",
    "                              strip_accents='unicode', stop_words='english')\n",
    "X2 = vectorizer2.fit_transform(corpus)\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(X2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorization approach:\n",
    "\n",
    "\n",
    "[TF-IDF (Term Frequency-Inverse Document Frequency)](https://kinder-chen.medium.com/introduction-to-natural-language-processing-tf-idf-1507e907c19) feature extraction:\n",
    "\n",
    "_\"TF-IDF (Term Frequency-Inverse Document Frequency) integrates two metrics: Term Frequency (TF) and Inverse Document Frequency (IDF). This method is employed when working with multiple documents, operating on the principle that rare words provide more insight into a document's content than frequently occurring words across the entire document set.\"_\n",
    "\n",
    "_\"A challenge with relying solely on word frequency is that commonly used words may overshadow the document, despite offering less \"informational content\" compared to rarer, potentially domain-specific terms. To address this, one can adjust the frequency of words by considering their prevalence across all documents, thereby reducing the scores of frequently used words that are common across the corpus.\"_\n",
    "\n",
    "**Term Frequency**: Provide large weight to frequent words. Given a token $t$ (term, word), a doccument $d$\n",
    "$$\n",
    "TF(t, d) = \\frac{\\text{number of times t appears in d}}{\\text{total number of term in d}}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency**: Give more importance to rare \"meaningfull\" words a appear in few doduments.\n",
    "\n",
    "If N is the total number of documents, and df is the number of documents with token t, then:\n",
    "\n",
    "$$\n",
    "IDF(t) = \\frac{N}{1 + df}\n",
    "$$\n",
    "$IDF(t) \\approx 1$ if $t$ appears in all documents, while $IDF(t) \\approx N$ if $t$ is a rare meaningfull word that appears in only one document.\n",
    "\n",
    "Finally:\n",
    "$$\n",
    "TF\\text{-}IDF(t, d) = TF(t, d) * IDF(t)\n",
    "$$\n",
    "[TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer):\n",
    "\n",
    "Convert a collection of raw documents to a matrix of TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode', stop_words='english')\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray().round(3))\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Lab 1: Sentiment Analysis of Financial data \n",
    "\n",
    "Sources: [Sentiment Analysis of Financial data](https://www.analyticsvidhya.com/blog/2022/07/sentiment-analysis-using-python/)\n",
    "\n",
    "\n",
    "The data is intended for advancing financial sentiment analysis research. It's two datasets (FiQA, Financial PhraseBank) combined into one easy-to-use CSV file. It provides financial sentences with sentiment labels.\n",
    "Citations _Malo, Pekka, et al. \"Good debt or bad debt: Detecting semantic orientations in economic texts.\" Journal of the Association for Information Science and Technology 65.4 (2014): 782-796._\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ML\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../datasets/FinancialSentimentAnalysis.csv')\n",
    "\n",
    "print(\"Shape:\", data.shape, \"columns:\", data.columns)\n",
    "print(data.describe())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Sentiment']\n",
    "y.value_counts(), y.value_counts(normalize=True).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "Input data: BOWs encoding\n",
    "\n",
    "Choose tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Tesla to recall 2,700 Model X SUVs over seat issue https://t.co/OdPraN59Xq $TSLA https://t.co/xvn4blIwpy https://t.co/ThfvWTnRPs'\n",
    "vectorizer = CountVectorizer(stop_words='english', strip_accents='unicode')\n",
    "\n",
    "tokenizer_sklearn = vectorizer.build_analyzer()\n",
    "print(\" \".join(analyzer(text)))\n",
    "print(\"Shape: \", CountVectorizer(tokenizer=tokenizer_sklearn).fit_transform(data['Sentence']).shape)\n",
    "\n",
    "print(\" \".join(standardize_tokenize(text)))\n",
    "print(\"Shape: \", CountVectorizer(tokenizer=standardize_tokenize).fit_transform(data['Sentence']).shape)\n",
    "\n",
    "print(\" \".join(standardize_tokenize_stemming(text)))\n",
    "print(\"Shape: \", CountVectorizer(tokenizer=standardize_tokenize_stemming).fit_transform(data['Sentence']).shape)\n",
    "\n",
    "print(\" \".join(standardize_tokenize_lemmatization(text)))\n",
    "print(\"Shape: \", CountVectorizer(tokenizer=standardize_tokenize_lemmatization).fit_transform(data['Sentence']).shape)\n",
    "\n",
    "print(\" \".join(standardize_tokenize_stemming_lemmatization(text)))\n",
    "print(\"Shape: \", CountVectorizer(tokenizer=standardize_tokenize_stemming_lemmatization).fit_transform(data['Sentence']).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = CountVectorizer(stop_words='english', strip_accents='unicode')\n",
    "# vectorizer = CountVectorizer(tokenizer=standardize_tokenize)\n",
    "# vectorizer = CountVectorizer(tokenizer=standardize_tokenize_stemming)\n",
    "# vectorizer = CountVectorizer(tokenizer=standardize_tokenize_lemmatization)\n",
    "vectorizer = CountVectorizer(tokenizer=standardize_tokenize_stemming_lemmatization)\n",
    "# vectorizer = TfidfVectorizer(stop_words='english', strip_accents='unicode')\n",
    "# vectorizer = TfidfVectorizer(tokenizer=standardize_tokenize_stemming_lemmatization)\n",
    "\n",
    "\n",
    "# Retrieve the analyzer to store transformed sentences in dataframe\n",
    "tokenizer = vectorizer.build_analyzer()\n",
    "data['Sentence_stdz'] = [\" \".join(tokenizer(s)) for s in data['Sentence']]\n",
    "\n",
    "X = vectorizer.fit_transform(data['Sentence'])\n",
    "print(\"Tokens:\", vectorizer.get_feature_names_out())\n",
    "print(\"Nb of tokens:\", len(vectorizer.get_feature_names_out()))\n",
    "print(\"Dimension of input data\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Classification with scikit-learn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = LogisticRegression(class_weight='balanced', max_iter=3000)\n",
    "# clf = GradientBoostingClassifier()\n",
    "clf = MultinomialNB()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "idx = np.arange(y.shape[0])\n",
    "X_train, X_test, x_str_train, x_str_test, y_train, y_test, idx_train, idx_test = \\\n",
    "    train_test_split(X, data['Sentence'], y, idx, test_size=0.25, random_state=5, stratify=y)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "Display prediction performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "cm = metrics.confusion_matrix(y_test, y_pred, normalize='true')\n",
    "cm_ = metrics.ConfusionMatrixDisplay(cm, display_labels=clf.classes_)\n",
    "\n",
    "cm_.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "Print some samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = pd.DataFrame(clf.predict_proba(X), columns=clf.classes_)\n",
    "df = pd.concat([data, probas], axis=1)\n",
    "df['SentimentPred'] = clf.predict(X)\n",
    "\n",
    "df.to_excel(\"/tmp/test.xlsx\")\n",
    "\n",
    "# Keep only test data, correctly classified, ordered by \n",
    "df = df.iloc[idx_test]\n",
    "df = df[df['SentimentPred'] == df['Sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    " Positive sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_positive = df[df['Sentiment'] == 'positive'].sort_values(by='positive', ascending=False)['Sentence_stdz']\n",
    "print(\"Most positive sentence\", sentence_positive[:5])\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "collocations=False).generate(\" \".join(sentence_positive))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    " Negative sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_negative = df[df['Sentiment'] == 'negative'].sort_values(by='negative', ascending=False)['Sentence_stdz']\n",
    "print(\"Most negative sentence\", sentence_negative[:5])\n",
    "\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "collocations=False).generate(\" \".join(sentence_negative))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### Lab2: Twitter Sentiment Analysis\n",
    "\n",
    "- Source [Twitter Sentiment Analysis Using Python | Introduction & Techniques](https://www.analyticsvidhya.com/blog/2021/06/twitter-sentiment-analysis-a-nlp-use-case-for-beginners/)\n",
    "- Dataset [Sentiment140 dataset with 1.6 million twe](https://www.kaggle.com/datasets/kazanova/sentiment140)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "Step-1: Import the Necessary Dependencies\n",
    "\n",
    "Install some packages:\n",
    "\n",
    "```\n",
    "conda install wordcloud\n",
    "conda install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Step-2: Read and Load the Dataset\n",
    "\n",
    "[Download the dataset from Kaggle](https://www.kaggle.com/datasets/ferno2/training1600000processednoemoticoncsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "DATASET_COLUMNS=['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "df = pd.read_csv('~/data/NLP/training.1600000.processed.noemoticon.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Step-3: Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns names:\", df.columns)\n",
    "print(\"Shape of data:\", df.shape)\n",
    "print(\"type of data:\\n\", df.dtypes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Step-4: Data Visualization of Target Variables\n",
    "\n",
    "- Selecting the text and Target column for our further analysis\n",
    "- Replacing the values to ease understanding. (Assigning 1 to Positive sentiment 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['text','target']]\n",
    "data['target'] = data['target'].replace(4,1)\n",
    "print(data['target'].unique())\n",
    "\n",
    "import seaborn as sns\n",
    "sns.countplot(x='target', data=data)\n",
    "\n",
    "print(\"Count and proportion of target\")\n",
    "data.target.value_counts(),  data.target.value_counts(normalize=True).round(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Step-5: Data Preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "5.4: Separating positive and negative tweets\n",
    "5.5: Taking 20000 positive and negatives sample from the data so we can run it on our machine easily\n",
    "5.6: Combining positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = data[data['target'] == 1]\n",
    "data_neg = data[data['target'] == 0]\n",
    "data_pos = data_pos.iloc[:20000]\n",
    "data_neg = data_neg.iloc[:20000]\n",
    "dataset = pd.concat([data_pos, data_neg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "5.7: Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_stemming_lemmatization(text):\n",
    "    out =  \" \".join(standardize_tokenize_stemming_lemmatization(text))\n",
    "    return out\n",
    "\n",
    "dataset['text_stdz'] = dataset['text'].apply(lambda x: standardize_stemming_lemmatization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "QC, check for empty standardized strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm = dataset['text_stdz'].isnull() | (dataset['text_stdz'].str.len() == 0)\n",
    "\n",
    "print(rm.sum(), \"row are empty of null, to be removed\")\n",
    "dataset = dataset[~rm]\n",
    "print(dataset.shape)\n",
    "\n",
    "# Save dataset to excel file to explore\n",
    "dataset.to_excel('/tmp/test.xlsx', sheet_name='data', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "5.18: Plot a cloud of words for negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neg = dataset.loc[dataset.target == 0, 'text_stdz']\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_neg))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "5.18: Plot a cloud of words for positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = dataset.loc[dataset.target == 1, 'text_stdz']\n",
    "plt.figure(figsize = (20,20))\n",
    "wc = WordCloud(max_words = 1000 , width = 1600 , height = 800,\n",
    "               collocations=False).generate(\" \".join(data_pos))\n",
    "plt.imshow(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "Step-6: Splitting Our Data Into Train and Test Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.text_stdz, dataset.target\n",
    "# Separating the 95% data for training data and 5% for testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=26105111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "Step-7: Transforming the Dataset Using TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = TfidfVectorizer(ngram_range=(1,2), max_features=500000)\n",
    "vectoriser.fit(X_train)\n",
    "#print('No. of feature_words: ', len(vectoriser.get_feature_names()))\n",
    "\n",
    "X_train = vectoriser.transform(X_train)\n",
    "X_test  = vectoriser.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65",
   "metadata": {},
   "source": [
    "Step-8: Function for Model Evaluation\n",
    "\n",
    "After training the model, we then apply the evaluation measures to check how the model is performing. Accordingly, we use the following evaluation parameters to check the performance of the models respectively:\n",
    "\n",
    "- Accuracy Score\n",
    "- Confusion Matrix with Plot\n",
    "- ROC-AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_Evaluate(model):\n",
    "    # Predict values for Test dataset\n",
    "    y_pred = model.predict(X_test)\n",
    "    # Print the evaluation metrics for the dataset.\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    # Compute and plot the Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    categories = ['Negative','Positive']\n",
    "    group_names = ['True Neg','False Pos', 'False Neg','True Pos']\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in cf_matrix.flatten() / np.sum(cf_matrix)]\n",
    "    labels = [f'{v1}n{v2}' for v1, v2 in zip(group_names,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cf_matrix, annot = labels, cmap = 'Blues',fmt = '',\n",
    "    xticklabels = categories, yticklabels = categories)\n",
    "    plt.xlabel(\"Predicted values\", fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.ylabel(\"Actual values\" , fontdict = {'size':14}, labelpad = 10)\n",
    "    plt.title (\"Confusion Matrix\", fontdict = {'size':18}, pad = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Step-9: Model Building\n",
    "\n",
    "In the problem statement, we have used three different models respectively :\n",
    "\n",
    "- Bernoulli Naive Bayes Classifier\n",
    "- SVM (Support Vector Machine)\n",
    "- Logistic Regression\n",
    "\n",
    "The idea behind choosing these models is that we want to try all the classifiers on the dataset ranging from simple ones to complex models, and then try to find out the one which gives the best performance among them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "BNBmodel = BernoulliNB()\n",
    "BNBmodel.fit(X_train, y_train)\n",
    "model_Evaluate(BNBmodel)\n",
    "y_pred1 = BNBmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "8.2: Plot the ROC-AUC Curve for model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred1)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC CURVE')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
