{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import pystatsml.plot_utils\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "\n",
    "- [Patrick Walls course](https://patrickwalls.github.io/mathematicalpython/differentiation/differentiation/) of Dept of Mathematics, University of British Columbia.\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Numerical_differentiation)\n",
    "\n",
    "The derivative of a function  at is the limit\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "For a fixed step size $h$, the previous formula provides the slope of the function using the forward difference approximation of the derivative. Equivalently, the slope could be estimated using backward approximation with positions $x - h$ and $x$.\n",
    "\n",
    "The most efficient numerical derivative use the central difference formula with step size is the average of the forward and backward approximation (known as symmetric difference quotient):\n",
    "\n",
    "$$\n",
    "f'(a) \\approx \\frac{1}{2} \\left( \\frac{f(a + h) - f(a)}{h} + \\frac{f(a) - f(a - h)}{h} \\right) = \\frac{f(a + h) - f(a - h)}{2h}\n",
    "$$\n",
    "\n",
    "Chose the step size depends of two issues\n",
    "\n",
    "\n",
    "1. Numerical precision: if $h$ is chosen too small, the subtraction will yield a large rounding error due to cancellation will produce a value of zero. For basic central differences, the optimal (Sauer, Timothy (2012). Numerical Analysis. Pearson. p.248.) step is the cube-root of machine epsilon ($2.2*10^{-16}$ for double precision), i.e.: $h\\approx 10^{-5}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(np.float64).eps\n",
    "print(\"Machine epsilon: {:e}, Min step size: {:e}\".format(eps, np.cbrt(eps)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The error of the central difference approximation is upper bounded by a function in $\\mathcal{O}(h^2)$. I.e., large step size $h=10^{-2}$  leads to large error of $10^{-4}$. Small step size e.g., $h=10^{-4}$ provide accurate slope estimation in $10^{-8}$.\n",
    "\n",
    "Those two points argue for a step size $h \\in [10^{-3}, 10^{-6}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Numerical differentiation of the function:\n",
    " \n",
    "$$\n",
    "f(x) = \\frac{7x^3-5x+1}{2x^4+x^2+1} \\ , \\ x \\in [-5,5]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical differentiation** with Numpy [gradient](https://numpy.org/doc/stable/reference/generated/numpy.gradient.html) given values `y` and `x` (or spacing `dx`) of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = [-5, 5]\n",
    "dx = 1e-3\n",
    "n = int((range_[1] - range_[0]) / dx) \n",
    "x = np.linspace(range_[0], range_[1], n)\n",
    "f = lambda x:  (7 * x ** 3  - 5 * x + 1) / (2 * x ** 4 + x ** 2 + 1)\n",
    "\n",
    "y = f(x) # values\n",
    "dydx = np.gradient(y, dx) # values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symbolic differentiation** with [sympy](https://docs.sympy.org/latest/tutorials/intro-tutorial/calculus.html) to compute true derivative $f'$\n",
    "\n",
    "Installation:\n",
    "\n",
    "    conda install conda-forge::sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy import lambdify\n",
    "x_s  = sp.symbols('x', real=True) # defining the variables\n",
    "\n",
    "f_sym = (7 * x_s ** 3  - 5 * x_s + 1) / (2 * x_s ** 4 + x_s ** 2 + 1)\n",
    "dfdx_sym =  sp.simplify(sp.diff(f_sym))\n",
    "print(\"f =\", f_sym)\n",
    "print(\"f'=\", dfdx_sym)\n",
    "dfdx_sym = lambdify(x_s, dfdx_sym,  \"numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(x, y, label=\"f\")\n",
    "plt.plot(x[1:-1], dydx[1:-1], lw=4, label=\"f' Num. Approx.\")\n",
    "plt.plot(x, dfdx_sym(x), \"--\", label=\"f'\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Numerical differentiation** with [numdifftools](https://github.com/pbrod/numdifftools)\n",
    "\n",
    "\n",
    "The `numdifftools` numerical differentiation problems in one or more variables.\n",
    "\n",
    "Installation:\n",
    "\n",
    "    conda install conda-forge::numdifftools\n",
    "\n",
    "[numdifftools.Derivative](https://numdifftools.readthedocs.io/en/latest/reference/numdifftools.html#numdifftools.core.Derivative) computes the derivatives of order 1 through 10 on any scalar function. It takes a function `f` as argument and return a function `dfdx` that compute the derivatives at `x` values. Example of first and second order derivative of $f(x) = x^2, f^\\prime(x) = 2 x, f^{\\prime\\prime}(x) = 2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numdifftools as nd\n",
    "\n",
    "# Example f(x) = x ** 2\n",
    "\n",
    "# First order derivative: dfdx = 2 x\n",
    "print(\"dfdx = 2 x:\", nd.Derivative(lambda x: x ** 2)([1, 2, 3]))\n",
    "\n",
    "# Second order derivative df^2dx^2 = 2 (Cte)\n",
    "print(\"df2dx2 = 2:\", nd.Derivative(lambda x: x ** 2, n=2)([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example with $f=x^3 - 27 x - 1$. We have $f^\\prime= 3x^2 - 27$, with roots $(-3, 3)$, and $f^{\\prime\\prime}=6x$, with root $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_ = [-5.5, 5.5]\n",
    "dx = 1e-3\n",
    "n = int((range_[1] - range_[0]) / dx) \n",
    "x = np.linspace(range_[0], range_[1], n)\n",
    "f = lambda x:  1 * (x ** 3) - 27 * x - 1\n",
    "\n",
    "# First derivative (! callable function, not values)\n",
    "dfdx = nd.Derivative(f)\n",
    "\n",
    "# Second derivative  (! callable function, not values)\n",
    "df2dx2 = nd.Derivative(f, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Second-order derivative](https://en.wikipedia.org/wiki/Second_derivative), \"the rate of change of the rate of change\" corresponds to the **curvature or concavity** of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(range_[0], range_[1], n)\n",
    "plt.plot(x, f(x), color=colors[0], lw=2, label=\"f\")\n",
    "plt.plot(x, dfdx(x),  color=colors[1], label=\"f'\")\n",
    "plt.plot(x, df2dx2(x), color=colors[2], label=\"f''\")\n",
    "plt.axvline(x=-3, ls='--', color=colors[1])\n",
    "plt.axvline(x= 3, ls='--', color=colors[1])\n",
    "plt.axvline(x= 0, ls='--', color=colors[2])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f'' < 0, x<0, f$ is concave down.\n",
    "- $f'' > 0, x>0$, $f$ is concave up.\n",
    "- $f'' = 0, x=0$, is an inflection point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate functions\n",
    "\n",
    "$f(\\textbf{x})$ is function of a vector $\\textbf{x}$ of several $p$ variables $\\textbf{x} = [x_1, ..., x_p]^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: $f(x, y) = x^2 + y^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x:  x[0] ** 2 + x[1] ** 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "# Make data.\n",
    "x = np.arange(-5, 5, 0.25)\n",
    "y = np.arange(-5, 5, 0.25)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "zz = f([xx, yy])\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(xx, yy, zz, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False, alpha=0.5, zorder=10)\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Gradient](https://en.wikipedia.org/wiki/Gradient) at a given point $\\textbf{x}$ is the vector of partial derivative of $f$ at gives the direction of **fastest increase**.\n",
    "\n",
    "$$\n",
    "\\nabla f(\\textbf{x}) = \\begin{bmatrix} \\partial f / \\partial x_1 \\\\ \\vdots \\\\ \\partial f / \\partial x_p \\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_grad = nd.Gradient(f)\n",
    "print(f_grad([0, 0]))\n",
    "print(f_grad([1, 1]))\n",
    "print(f_grad([-1, 2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Hessian](https://en.wikipedia.org/wiki/Hessian_matrix) matrix contains the second-order partial derivatives of $f$. It describes the **local curvature** of a function of many variables. It is noted:\n",
    "$$\n",
    "f''(\\textbf{x}_k) = \\nabla^2 f(\\textbf{x}_k) = \\textbf{H}_{f(\\textbf{x}_k)}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial^2x_1} \\ldots \\frac{\\partial^2 f}{\\partial x_p \\partial x_1}\\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial^2 f}{\\partial x_1 \\partial x_p}  \\ldots \\frac{\\partial^2 f}{\\partial^2x_p}\n",
    "\\end{bmatrix},\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = nd.Hessian(f)([0, 0])\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Integration\n",
    "\n",
    "- Principles [Patrick Walls course](https://patrickwalls.github.io/mathematicalpython/integration/).\n",
    "- Library: [Scipy integrate](https://docs.scipy.org/doc/scipy/tutorial/integrate.html) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import pystatsml.plot_utils\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods based on sums of patches over intervals\n",
    "\n",
    "Methods for integrating functions given fixed samples: $[(x_1, f(x_1)), ..., (x_i, f(x_i)), ...(x_N, f(x_N)]$.\n",
    "\n",
    "[Riemann sums of rectangles](https://patrickwalls.github.io/mathematicalpython/integration/riemann-sums/) to approximate the area.\n",
    "$$\n",
    "\\sum_{i=1}^N f(x_i^ * ) (x_i - x_{i-1}) \\ \\ , \\ x_i^* \\in [x_{i-1},x_i]\n",
    "$$\n",
    "The error is in $\\mathcal{O}(\\frac{1}{N})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x : 1 / (1 + x ** 2)\n",
    "a, b, N = 0, 5, 10\n",
    "dx = (b - a) / N\n",
    "\n",
    "x = np.linspace(a, b, N+1)\n",
    "y = f(x)\n",
    "\n",
    "x_ = np.linspace(a,b, 10*N+1) # 10 * N points to plot the function smoothly\n",
    "plt.plot(x_, f(x_), 'b')\n",
    "x_left = x[:-1] # Left endpoints\n",
    "y_left = y[:-1]\n",
    "plt.plot(x_left,y_left,'b.',markersize=10)\n",
    "plt.bar(x_left,y_left,width=dx, alpha=0.2,align='edge',edgecolor='b')\n",
    "plt.title('Left Riemann Sum, N = {}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Riemann sums with 100 points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, N = 0, 5, 50\n",
    "dx = (b - a) / N\n",
    "x = np.linspace(a, b, N+1)\n",
    "\n",
    "y = f(x)\n",
    "print(\"Integral:\", np.sum(f(x[:-1]) * np.diff(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Trapezoid Rule](https://patrickwalls.github.io/mathematicalpython/integration/trapezoid-rule/)** sum the trapezoids connecting the points. The error is in $\\mathcal{O}(\\frac{1}{N^2})$. Use\n",
    "[scipy.integrate.trapezoid](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.trapezoid.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "integrate.trapezoid(f(x[:-1]), dx=dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Simpson's rule](https://patrickwalls.github.io/mathematicalpython/integration/simpsons-rule/)** uses a quadratic polynomial on each subinterval of a partition to approximate the function and to compute the definite integral. The error is in $\\mathcal{O}(\\frac{1}{N^4})$.\n",
    "Use\n",
    "[scipy.integrate.simpson](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.simpson.html#scipy.integrate.simpson) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "integrate.simpson(f(x[:-1]), dx=dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Gauss-Legendre Quadrature](https://en.wikipedia.org/wiki/Gauss%E2%80%93Legendre_quadrature)** approximate the integral of a function as a weighted sum of Legendre polynomials.\n",
    "\n",
    "Methods for Integrating functions given function object $f()$ that could be evaluated for any value $x$ in a range $[a, b]$.\n",
    "\n",
    "Use\n",
    "[scipy.integrate.quad](https://docs.scipy.org/doc/scipy/tutorial/integrate.html#general-integration-quad) function. The first argument to `quad` is a “callable” Python object (i.e., a function, method, or class instance). Notice the use of a lambda- function in this case as the argument. The next two arguments are the limits of integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "integrate.quad(f, a=a, b=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The return values are the estimated of the integral and the estimate of the absolute integration error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystatsml_teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
