{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Differentiation & Integration\n",
    "\n",
    "See course on [Mathematical Python]((https://patrickwalls.github.io/mathematicalpython/)) from Patrick Walls of Dept of Mathematics, University of British Columbia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#import pystatsml.plot_utils\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Differentiation\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Patrick Walls course](https://patrickwalls.github.io/mathematicalpython/differentiation/differentiation/)\n",
    "- [Wikipedia](https://en.wikipedia.org/wiki/Numerical_differentiation)\n",
    "\n",
    "The derivative of a function  at is the limit\n",
    "\n",
    "$$\n",
    "f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}\n",
    "$$\n",
    "\n",
    "For a fixed step size $h$, the previous formula provides the slope of the function using the forward difference approximation of the derivative. Equivalently, the slope could be estimated using backward approximation with positions $x - h$ and $x$.\n",
    "\n",
    "The most efficient numerical derivative use the central difference formula with step size is the average of the forward and backward approximation (known as symmetric difference quotient):\n",
    "\n",
    "$$\n",
    "f'(a) \\approx \\frac{1}{2} \\left( \\frac{f(a + h) - f(a)}{h} + \\frac{f(a) - f(a - h)}{h} \\right) = \\frac{f(a + h) - f(a - h)}{2h}\n",
    "$$\n",
    "\n",
    "Chose the step size depends of two issues\n",
    "\n",
    "\n",
    "1. Numerical precision: if $h$ is chosen too small, the subtraction will yield a large rounding error due to cancellation will produce a value of zero. For basic central differences, the optimal (Sauer, Timothy (2012). Numerical Analysis. Pearson. p.248.) step is the cube-root of machine epsilon ($2.2*10^{-16}$ for double precision), i.e.: $h\\approx 10^{-5}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = np.finfo(np.float64).eps\n",
    "\n",
    "print(\"Machine epsilon: {:e}, Min step size: {:e}\".format(eps, np.cbrt(eps)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The error of the central difference approximation is upper bounded by a function in $\\mathcal{O}(h^2)$. I.e., large step size $h=10^{-2}$  leads to large error of $10^{-4}$. Small step size e.g., $h=10^{-4}$ provide accurate slope estimation in $10^{-8}$.\n",
    "\n",
    "Those two points argue for a step size $h \\in [10^{-3}, 10^{-6}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy [gradient](https://numpy.org/doc/stable/reference/generated/numpy.gradient.html) function for unmerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range = [-3, 3]\n",
    "dx = 1e-3\n",
    "n = int((range[1] - range[0]) / dx)\n",
    "x = np.linspace(range[0], range[1], n)\n",
    "f = lambda x: 3*np.exp(x) / (x**2 + x + 1)\n",
    "\n",
    "y = f(x)\n",
    "dydx = np.gradient(y, dx)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.plot(x[1:-1], dydx[1:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Numerical differenciation of the function\n",
    " \n",
    "$$\n",
    "f(x) = \\frac{7x^3-5x+1}{2x^4+x^2+1} \\ , \\ x \\in [-5,5]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range = [-5, 5]\n",
    "dx = 1e-3\n",
    "n = int((range[1] - range[0]) / dx) \n",
    "x = np.linspace(range[0], range[1], n)\n",
    "f = lambda x:  (7 * x ** 3  - 5 * x + 1) / (2 * x ** 4 + x ** 2 + 1)\n",
    "\n",
    "y = f(x)\n",
    "dydx = np.gradient(y, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Symbolic Differentiation with sympy to compute true derivative $f'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "from sympy import lambdify\n",
    "x_s  = sp.symbols('x', real=True) # defining the variables\n",
    "\n",
    "f_sym = (7 * x_s ** 3  - 5 * x_s + 1) / (2 * x_s ** 4 + x_s ** 2 + 1)\n",
    "df_sym =  sp.simplify(sp.diff(f_sym))\n",
    "print(\"f =\", f_sym)\n",
    "print(\"f'=\", df_sym)\n",
    "df_sym = lambdify(x_s, df_sym,  \"numpy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(x, y, label=\"f\")\n",
    "plt.plot(x[1:-1], dydx[1:-1], lw=4, label=\"f' Num. Approx.\")\n",
    "plt.plot(x, df_sym(x), \"--\", label=\"f'\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced numerical differenciation with [numdifftools](https://github.com/pbrod/numdifftools)\n",
    "\n",
    "The numdifftools library addresses numerical differentiation with one or more variables:\n",
    "\n",
    "- Derivative: Compute the derivatives of order 1 through 10 on any scalar function.\n",
    "- directionaldiff: Compute directional derivative of a function of n variables\n",
    "- Gradient: Compute the gradient vector of a scalar function of one or more variables.\n",
    "- Jacobian: Compute the Jacobian matrix of a vector valued function of one or more variables.\n",
    "- Hessian: Compute the Hessian matrix of all 2nd partial derivatives of a scalar function of one or more variables.\n",
    "- Hessdiag: Compute only the diagonal elements of the Hessian matrix\n",
    "\n",
    "To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Integration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystatsml_teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
