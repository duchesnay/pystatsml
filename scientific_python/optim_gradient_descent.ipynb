{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization (Minimization) by Gradient Descent\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "![Optimization (minimization) by gradient descent](./images/minimization.png)\n",
    "\n",
    "[Gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) is an optimization algorithm to minimize a **cost or loss function** $f$ given its parameter $w$. The algorithm **iteratively moves in the direction of steepest descent** as defined by the **opposite direction the gradient**. In machine learning, we use gradient descent to update the parameters of our model. **Parameters** refer to **coefficients** in **Linear Regression** and **weights** in **neural networks**.\n",
    "\n",
    "Local minimization of $f$ at point $x_k$ make use of first-order [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series) local estimation of $f$ (in one dimension):\n",
    "\n",
    "$$\n",
    "f(w_k + t) \\approx f(w_k) + f'(w_k)t\n",
    "$$\n",
    "\n",
    "Therefore, to minimize $f(w_k + t)$ we just have to move in the opposite direction of the derivative $f'(w_k)$:\n",
    "\n",
    " $$\n",
    " w_{k+1} =  w_{k} - \\gamma f'(w_{k})\n",
    " $$\n",
    "\n",
    "With a **learning rate** $\\gamma$ that determines the step size at each iteration while moving toward a minimum of a cost function.\n",
    "\n",
    "In multidimensional problems $\\textbf{w}_{k} \\in \\mathbb{R}^p$, where:\n",
    "$$\n",
    "\\textbf{w}_k = \\begin{bmatrix} w_1 \\\\ \\vdots \\\\ w_p \\end{bmatrix}_k,\n",
    "$$\n",
    "the derivative $f'(\\textbf{w}_k)$ is the gradient (direction) of $f$ at $\\textbf{w}_k$:\n",
    "$$\n",
    "\\nabla f(\\textbf{w}_{k}) = \\begin{bmatrix} \\partial f / \\partial w_1 \\\\ \\vdots \\\\ \\partial f / \\partial w_p \\end{bmatrix}_k,\n",
    "$$\n",
    "Leading to the minimization scheme:\n",
    "$$\n",
    "\\textbf{w}_{k+1} =  \\textbf{w}_{k} - \\gamma \\nabla f(\\textbf{w}_{k})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the step size\n",
    "\n",
    "\n",
    "With large learning rate $\\gamma$ we can cover more ground each step, but we **risk overshooting the lowest point** since the slope of the hill is constantly changing.\n",
    "\n",
    "With a very small learning rate**, we can confidently move in the direction of the negative gradient since we are recalculating it so frequently. A small learning rate is more precise, but calculating the gradient is time-consuming, leading too slow convergence\n",
    "\n",
    "![jeremyjordan](./images/learning_rate_choice.png)\n",
    "\n",
    "**Line search** can be used (or more sophisticated [Backtracking line search](https://en.wikipedia.org/wiki/Backtracking_line_search)) to find value of $\\gamma$ such that $f(\\textbf{w}_{k} - \\gamma \\nabla f(\\textbf{w}_{k}))$ is minimum. However such simple method ignore possible change of the curvature.\n",
    "\n",
    "- Benefit of gradient decent: simplicity and versatility, almost any function with a gradient can be minimized.\n",
    "- Limitations:\n",
    "\n",
    "    * Local minima (local optimization) for non-convex problem.\n",
    "    * Convergence speed: With fast changing curvature (gradient direction) the estimation of gradient will rapidly become wrong moving away from $x_k$ suggesting small step-size. This also suggest the integration of change of the gradient direction in the calculus of the step size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example gradient descent to minimizes linear regression\n",
    "\n",
    "The cost function $f$ is the Mean Squared Error:\n",
    "$$\n",
    "f(w_0,w_1) = \\frac{1}{N} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0))^2,\n",
    "$$\n",
    "\n",
    "The gradient is:\n",
    "\n",
    "$$ \n",
    "\\begin{split}\\nabla f(w_0,w_1) =\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{\\partial f}{\\partial w_0}\\\\\n",
    "     \\frac{\\partial f}{\\partial w_1}\\\\\n",
    "    \\end{bmatrix}\n",
    "=\n",
    "   \\begin{bmatrix}\n",
    "     \\frac{-2}{N} \\sum ((w_1x_i + w_0) - y_i ) \\\\\n",
    "     \\frac{-2}{N} \\sum  x_i((w_1x_i + w_0) - y_i) \\\\\n",
    "    \\end{bmatrix}\n",
    "    \\end{split}\n",
    "$$\n",
    "\n",
    "**Use numpy with explicit gradient formula**\n",
    "\n",
    "Code obtained with chatgpt using the prompt: \"provides the python code performing a gradient descent minimization of a mean squared error of the simple linear regression with a slope 'w1' and an intercept 'w0' given an input variable x with is a numpy array of length n_samples and an output variable x with is a numpy array of length n_samples. The code must make use of numpy library\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(x, y, learning_rate=0.01, n_iterations=1000):\n",
    "    # Initialize parameters\n",
    "    w0 = 0  # Intercept\n",
    "    w1 = 0  # Slope\n",
    "    n_samples = len(y)\n",
    "\n",
    "    # Perform gradient descent\n",
    "    for _ in range(n_iterations):\n",
    "        # Compute the predictions\n",
    "        y_pred = w0 + w1 * x\n",
    "\n",
    "        # Compute the mean squared error\n",
    "        mse = (1 / n_samples) * np.sum((y - y_pred) ** 2)\n",
    "\n",
    "        # Compute the gradients\n",
    "        w0_gradient = -(2 / n_samples) * np.sum(y - y_pred)\n",
    "        w1_gradient = -(2 / n_samples) * np.sum((y - y_pred) * x)\n",
    "\n",
    "        # Update the parameters\n",
    "        w0 -= learning_rate * w0_gradient\n",
    "        w1 -= learning_rate * w1_gradient\n",
    "\n",
    "        # Optionally, print the progress (remove the comment to enable)\n",
    "        # print(f\"Iteration {_+1}: w0={w0}, w1={w1}, MSE={mse}\")\n",
    "\n",
    "    return w0, w1\n",
    "\n",
    "# Example usage:\n",
    "x = np.random.rand(100)\n",
    "y = 2 * x - 3\n",
    "\n",
    "w0, w1 = gradient_descent(x, y, learning_rate=0.01, n_iterations=10000)\n",
    "\n",
    "print(\"Solution: {:e} x + {:e}\".format(w1, w0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's method in optimization\n",
    "\n",
    "[Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) integrates the change of the curvature (ie, change of gradient direction) in the minimization process. Since gradient direction is the change of $f$, i.e., the first order derivative, thus the change of gradient is second order derivative of $f$.\n",
    "See [Visually Explained: Newton's Method in Optimization](https://www.youtube.com/watch?v=W7S94pq5Xuo)\n",
    "\n",
    "For univariate functions. Like gradient descent Newton's method try to locally minimize $f(w_k + t)$ given a current position $w_k$. However, while gradient descent use first order local estimation of $f$, Newton's method increases this approximation using second-order [Taylor expansion](https://en.wikipedia.org/wiki/Taylor_series) of $f$ around an iterate $w_k$:\n",
    "$$\n",
    "f(w_k + t) \\approx f(w_k) + f'(w_k)t + \\frac{1}{2} f''(w_k)t^2\n",
    "$$\n",
    "Cancelling the derivative of this expression: $\\frac{d}{dt}(f(w_k) + f'(w_k)t + \\frac{1}{2} f''(w_k)t^2)=0$, provides $f'(w_k) + f''(w_k)t = 0$, and thus $t = \\frac{f'(w_k)}{f''(w_k)}$.\n",
    "The learning rate is $\\gamma = \\frac{1}{f''(w_k)}$, and optimization scheme becomes:\n",
    "$$\n",
    "w_{k+1} =  w_{k} - \\frac{1}{f''(w_k)} f'(w_{k}).\n",
    "$$\n",
    "\n",
    "In multidimensional problems $\\textbf{w}_{k} \\in \\mathbb{R}^p$, $[f''(\\textbf{w}_k)]^{-1}$ is the inverse of the $(p \\times p)$ [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix) containing the second-order partial derivatives of $f$.\n",
    "\n",
    "- Benefit of Newton's method: Convergence speed considering the change of the curvature of $f$ to adapt the learning rate and direction.\n",
    "- Problems:\n",
    "* Local minima (local optimization) for non-convex problem.\n",
    "* In large dimension, computing the Newton direction $-[f''(w_k)]^{-1} f'(w_{k}$ can be an expensive operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton's method to minimizes linear regression\n",
    "\n",
    "Given a linear model where the output is a weighted sum of the inputs, the model can be expressed as:\n",
    "$$\n",
    "y_i = \\sum_p w_p x_{ip}\n",
    "$$\n",
    "where:\n",
    "\n",
    "- $y_i$ is the predicted output for the i-th sample,\n",
    "- $w_p$ are the weights (parameters) of the model,\n",
    "- $x_{ip}$ is the p-th feature of the i-th sample.\n",
    "\n",
    "The objective in least squares minimization is to minimize the following cost function $J(\\textbf{w})$:\n",
    "$$\n",
    "J(\\textbf{w}) = \\frac{1}{2} \\sum_i \\left(y_i - \\sum_p w_p x_{ip}\\right)^2\n",
    "$$\n",
    "where w is the vector of weights $\\textbf{w} = [w_1, w_2, \\ldots, w_p]^T$.\n",
    "\n",
    "**Hessian Matrix of the Least Squares Problem**\n",
    "\n",
    "The Hessian matrix $H$ of the least squares problem is a square matrix of second-order partial derivatives of the cost function with respect to the model parameters. It is given by:\n",
    "$$\n",
    "H = \\nabla^2 J(\\textbf{w}) \n",
    "$$\n",
    "\n",
    "For the least squares cost function, $J(\\textbf{w})$, the Hessian is calculated as follows:\n",
    "\n",
    "1. Gradient of the cost function $\\nabla J(\\textbf{w})$:\n",
    "$$\n",
    "\\nabla J(\\textbf{w}) = \\frac{\\partial J(\\textbf{w})}{\\partial w_p} \\sum_i\\left(\\sum_q w_q x_{iq} - y_i\\right)x_{ip}\n",
    "$$\n",
    "\n",
    "2. Second-order partial derivatives (Hessian):\n",
    "\n",
    "The Hessian $H$ is the matrix of second derivatives of $J(\\textbf{w})$ with respect to $w_p$ and $w_q$.\n",
    "$H$ is a measure of the curvature of $J$: The eigenvectors of $H$ point in the directions of the major and minor axes.\n",
    "The eigenvalues measure the steepness of $J$ along the corresponding eigendirection. Thus, each eigenvalue of $H$ is also a measure of the covariance or spread of the inputs along the corresponding eigendirection.\n",
    "\n",
    "$$\n",
    "H_{pq} = \\frac{\\partial^2 J(\\textbf{w})}{\\partial w_p \\partial w_q}\n",
    "$$\n",
    "Given the form of the gradient, the second derivative with respect to $w_p$ and $w_q$ simplifies to:\n",
    "$$\n",
    "H_{pq} = \\sum_i x_{ip}x_{iq}\n",
    "$$\n",
    "This can be written more compactly in matrix form as:\n",
    "$$\n",
    "H = \\textbf{X}^T\\textbf{X}\n",
    "$$\n",
    "where $X$ is the matrix of input features (each row corresponds to a sample, and each column corresponds to a feature) with $X_{ip}=x_{ip}$.\n",
    "\n",
    "In this case the Hessian turns out the be the same as the covariance matrix of the inputs.\n",
    "Thus, each eigenvalue of $H$ is also a measure of the covariance or spread of the inputs along the corresponding eigendirection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Methods\n",
    "\n",
    "[Quasi-Newton Methods](https://en.wikipedia.org/wiki/Quasi-Newton_method) are an alternative of Newton Methods when Hessian is unavailable or is too expensive to compute at every iteration.\n",
    "\n",
    "The most popular quasi-Newton method is the Broyden–Fletcher–Goldfarb–Shanno algorithm [BFGS](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm)\n",
    "\n",
    "\n",
    "### Example: Minimizes linear regression\n",
    "\n",
    "Note, that we provide the function to be minimized (Mean Squared Error) but the expression of the gradient which is estimated numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "def mse(params, x, y):\n",
    "    \"\"\"\n",
    "    Mean Squared Error function for linear regression.\n",
    "    \n",
    "    params: list or array, where params[0] = b0 (intercept),\n",
    "    params[1] = b1 (slope)\n",
    "    x: independent variable, numpy array\n",
    "    y: dependent variable, numpy array\n",
    "    \"\"\"\n",
    "    b0, b1 = params\n",
    "    y_pred = b0 + b1 * x\n",
    "    return np.mean((y - y_pred) ** 2)\n",
    "\n",
    "\n",
    "result = minimize(fun=mse, x0=[0, 0], args=(x, y), method='BFGS')\n",
    "b0, b1 = result.x\n",
    "\n",
    "print(\"Solution: {:e} x + {:e}\".format(b1, b0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Variants with Datasets\n",
    "\n",
    "There are three variants of gradient descent, which differ on the use of the dataset made of $n$ samples of input data $\\textbf{x}_i$'s, and possibly their corresponding targets $y_i$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "Batch gradient descent, known also as Vanilla gradient descent, computes the gradient of the cost function with respect to the parameters $\\theta$  for the **entire training dataset** :\n",
    "\n",
    "- Choose an initial vector of parameters $\\textbf{w}_0$ and learning rate $\\gamma$.\n",
    "- Repeat until an approximate minimum is obtained:\n",
    "    * $\\textbf{w}_{k+1} =  \\textbf{w}_{k} - \\gamma \\sum_{i=1}^n \\nabla f(\\textbf{w}_{k}, \\textbf{x}_i, y_i)$\n",
    "\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Batch Gradient Descent is suited for convex or relatively smooth error manifolds. Since it directly towards an optimum solution.\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Fast convergence toward \"bad\" local minimum (on non-convex functions)\n",
    "- As we need to calculate the gradients for the whole dataset is intractable for datasets that don't fit in memory and doesn't allow us to update our model online.\n",
    "\n",
    "\n",
    "### Stochastic gradient descent\n",
    "\n",
    "[Stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) (SGD) in contrast performs a parameter update for each training example $x^{(i)}$ and $y^{(i)}$. A complete passes through the training dataset is called an **epoch**. The number of epochs is a hyperparameter to be determined observing the convergence.\n",
    "\n",
    "- Choose an initial vector of parameters $\\textbf{w}_0$ and learning rate $\\gamma$.\n",
    "- Repeat epochs until an approximate minimum is obtained:\n",
    "    - Randomly shuffle examples in the training set.\n",
    "    - For $i \\in 1, \\dots, n$\n",
    "        * $\\textbf{w}_{k+1} =  \\textbf{w}_{k} - \\gamma \\nabla f(\\textbf{w}_{k}, \\textbf{x}_i, y_i)$\n",
    "\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Often provide better local minimum. Minimization will not be smooth but rather slightly erratic and jumpy. But this 'random walk,' of SGD's fluctuation, enables it to jump from a basin to another, with possibly deeper, local minimum.\n",
    "- Online learning\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Large fluctuation that ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.\n",
    "- Slow down computation by not taking advantage of vectorized numerical libraries.\n",
    "\n",
    "\n",
    "\n",
    "### Mini-batch gradient descent\n",
    "\n",
    "\n",
    "Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch (subset of) training samples:\n",
    "\n",
    "- Divide the training set in subsets of size $m$.\n",
    "- Choose an initial vector of parameters $\\textbf{w}_0$ and learning rate $\\gamma$.\n",
    "- Repeat epochs until an approximate minimum is obtained:\n",
    "    - Randomly pick a mini-batch.\n",
    "    - For each mini-batch $b$\n",
    "        * $\\textbf{w}_{k+1} =  \\textbf{w}_{k} - \\gamma \\sum_{i=b}^{b+m} \\nabla f(\\textbf{w}_{k}, \\textbf{x}_i, y_i)$\n",
    "\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Reduces the variance of the parameter updates, which can lead to more stable convergence.\n",
    "- Make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. \n",
    "\n",
    "Mini-batch gradient descent is typically the algorithm of choice when training a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Learning Rate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Momentum\n",
    "\n",
    "SGD has trouble navigating ravines (areas where the surface curves much more steeply in one dimension than in another), which are common around local optima. In these scenarios, SGD oscillates across the slopes of the ravine while only making hesitant progress, along the bottom towards the local optimum as in the image below.\n",
    "\n",
    "[Source](https://distill.pub/2017/momentum/)\n",
    "\n",
    "![No momentum: oscillations toward local largest gradient](./images/grad_descent_no_momentum.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "<center><small>No momentum: moving toward local largest gradient create oscillations.</small></center>\n",
    "\n",
    "<center>\n",
    "    \n",
    "![With momentum: accumulate velocity to avoid oscillations](./images/grad_descent_momentum.png)\n",
    "\n",
    "</center>\n",
    "\n",
    "<center><small>With momentum: accumulate velocity to avoid oscillations.</small></center>\n",
    "\n",
    "Momentum* is a method that helps to accelerate SGD in the relevant direction and dampens oscillations as can be seen in image above. It does this by adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector.\n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "\\begin{split} \n",
    "v_t &= \\rho v_{t-1} + \\nabla_\\theta J( \\theta) \\\\ \n",
    "\\theta &= \\theta - v_t \n",
    "\\end{split} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "```python\n",
    "vx = 0\n",
    "while True:\n",
    "    dx = gradient(J, x)\n",
    "    vx = rho * vx + dx\n",
    "    x -= learning_rate * vx\n",
    "```\n",
    "\n",
    "Note: The momentum term **$\\rho$** is usually set to 0.9 or a similar value.\n",
    "\n",
    "Essentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way, until it reaches its terminal velocity if there is air resistance, i.e.  $\\rho$ <1. \n",
    "\n",
    "The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad: adaptive learning rates\n",
    "\n",
    "- Added element-wise scaling of the gradient based on the historical sum of squares in each dimension.\n",
    "\n",
    "- \"Per-parameter learning rates\" or \"adaptive learning rates\"\n",
    "\n",
    "```python\n",
    "grad_squared = 0\n",
    "while True:\n",
    "    dx = gradient(J, x)\n",
    "    grad_squared += dx * dx\n",
    "    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n",
    "```\n",
    "\n",
    "- Progress along “steep” directions is damped.\n",
    "- Progress along “flat” directions is accelerated.\n",
    "- Problem: step size over long time => Decays to zero.\n",
    "\n",
    "### RMSProp: “Leaky AdaGrad”\n",
    "\n",
    "```python\n",
    "grad_squared = 0\n",
    "while True:\n",
    "    dx = gradient(J, x)\n",
    "    grad_squared += decay_rate * grad_squared + (1 - decay_rate) * dx * dx\n",
    "    x -= learning_rate * dx / (np.sqrt(grad_squared) + 1e-7)\n",
    "```\n",
    "\n",
    "- `decay_rate = 1`: gradient descent\n",
    "- `decay_rate = 0`: AdaGrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov accelerated gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a ball that rolls down a hill, blindly following the slope, is highly **unsatisfactory**. We'd like to have a smarter ball, a ball that has **a notion of where it is going** so that it **knows to slow down before the hill slopes up again**.\n",
    "Nesterov accelerated gradient (NAG) is a way to give **our momentum term this kind of prescience**. We know that we will use our momentum term $\\gamma v_{t-1}$ to move the parameters  $\\theta$. \n",
    "\n",
    "Computing $\\theta - \\gamma v_{t-1}$ thus gives us **an approximation of the next position of the parameters** (the gradient is missing for the full update), a rough idea where our parameters are going to be.<br>\n",
    "We can now effectively look ahead by calculating the gradient not w.r.t. to our current parameters $\\theta$ but w.r.t. the approximate future position of our parameters:\n",
    "\n",
    "\\begin{align} \n",
    "\\begin{split} \n",
    "v_t &= \\gamma v_{t-1} + \\eta \\nabla_\\theta J( \\theta - \\gamma v_{t-1} ) \\\\ \n",
    "\\theta &= \\theta - v_t \n",
    "\\end{split} \n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we set the momentum term $\\gamma$ to a value of around 0.9. While **Momentum first computes the current gradient and then takes a big jump in the direction of the updated accumulated gradient** , **NAG** first **makes a big jump in the direction of the previous accumulated gradient, measures the gradient and then makes a correction, which results in the complete NAG update**. This anticipatory update **prevents** us from **going too fast** and results in **increased responsiveness**, which has significantly **increased the performance of RNNs** on a number of tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adaptive Moment Estimation (Adam)** is a method that computes **adaptive learning rates** for each parameter. In addition to storing an **exponentially decaying average of past squared gradients $v_t$**, Adam also keeps an **exponentially decaying average of past gradients $m_t$, similar to momentum**. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a **heavy ball with friction**, which thus prefers **flat minima in the error surface**. We compute the decaying averages of past and past squared gradients $m_t$ and $v_t$ respectively as follows:\n",
    "\n",
    "\\begin{align} \n",
    "\\begin{split} \n",
    "m_t &= \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J( \\theta) \\\\ \n",
    "v_t &= \\beta_2 v_{t-1} + (1 - \\beta_2) \\nabla_\\theta J( \\theta)^2 \n",
    "\\end{split} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$m_{t}$ and $v_{t}$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method.\n",
    "Adam (almost)\n",
    "\n",
    "```python\n",
    "first_moment = 0\n",
    "second_moment = 0\n",
    "while True:\n",
    "    dx = gradient(J, x)\n",
    "    # Momentum:\n",
    "    first_moment = beta1 * first_moment + (1 - beta1) * dx\n",
    "    # AdaGrad/RMSProp\n",
    "    second_moment = beta2 * second_moment + (1 - beta2) * dx * dx\n",
    "    x -= learning_rate * first_moment / (np.sqrt(second_moment) + 1e-7)\n",
    "```\n",
    "\n",
    "As $m_{t}$ and $v_{t}$ are initialized as vectors of 0's, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. $\\beta_{1}$ and $\\beta_{2}$ are close to 1).\n",
    "They counteract these biases by computing bias-corrected first and second moment estimates:\n",
    "\n",
    "\\begin{align} \n",
    "\\hat{m}_t &= \\dfrac{m_t}{1 - \\beta^t_1} \\\\ \n",
    "\\hat{v}_t &= \\dfrac{v_t}{1 - \\beta^t_2}\n",
    "\\end{align}\n",
    "\n",
    "They then use these to update the parameters (Adam update rule):\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "$$\n",
    "\n",
    "- $\\hat{m}_t$ Accumulate gradient: velocity.\n",
    "- $\\hat{v}_t$ Element-wise scaling of the gradient based on the historical sum of squares in each dimension.\n",
    "\n",
    "- Choose Adam as default optimizer\n",
    "- Default values of 0.9 for $\\beta_1$, 0.999 for $\\beta_2$, and $10^{-7}$ for $\\epsilon$. \n",
    "- learning rate in a range between $1e-3$ and $5e-4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent challenges\n",
    "\n",
    "How to choose a learning rate? That is a topic on its own and beyond the scope of this post as well.\n",
    "\n",
    "See:\n",
    "- LeCun Y.A., Bottou L., Orr G.B., Müller KR. (2012) [Efficient BackProp](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf). In: Montavon G., Orr G.B., Müller KR. (eds) Neural Networks: Tricks of the Trade. Lecture Notes in Computer Science, vol 7700. Springer, Berlin, Heidelberg\n",
    "\n",
    "- Introduction to [Gradient Descent Algorithm](https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/) (along with variants) in Machine Learning: Gradient Descent with Momentum, ADAGRAD and ADAM.\n",
    "\n",
    "Vanilla mini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:\n",
    "\n",
    "- Choosing a proper **learning rate** can be difficult. A learning rate that is **too small** leads to **painfully slow convergence**, while a learning rate that is **too large** can hinder convergence and cause the loss function to **fluctuate** around the minimum or even to **diverge**.\n",
    "\n",
    "- **Learning rate schedules** try to **adjust the learning rate during training** by e.g. annealing, i.e. reducing the learning rate according to a pre-defined schedule or when the change in objective between epochs falls below a threshold. These schedules and thresholds, however, have to be defined in advance and are thus unable to adapt to a dataset's characteristics.\n",
    "\n",
    "- Additionally, the same learning rate applies to all parameter updates. **If our data is sparse and our features have very different frequencies**, we might not want to update all of them to the same extent, but perform **a larger update for rarely occurring features**.\n",
    "\n",
    "- Another key challenge of **minimizing highly non-convex error** functions common for neural networks is **avoiding** getting **trapped in their numerous suboptimal local minima**. These **saddle points (local minimas)** are usually surrounded by a plateau of the same error, which makes it **notoriously hard for SGD to escape**, as the gradient is close to zero in all dimensions.\n",
    "\n",
    "Recommendation:\n",
    "\n",
    "- Shuffle the examples (SGD)\n",
    "- Center the input variables by subtracting the mean\n",
    "- Normalize the input variable to a standard deviation of 1\n",
    "- Initializing the weight\n",
    "- Adaptive learning rates (momentum), using separate learning rate for each weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
