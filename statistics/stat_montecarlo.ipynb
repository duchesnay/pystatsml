{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling and Monte Carlo Methods\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Scipy Resampling and Monte Carlo Methods](https://docs.scipy.org/doc/scipy/tutorial/stats/resampling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Statistics\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "#import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pystatsml.plot_utils\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Monte-Carlo simulation of Random Walk Process\n",
    "\n",
    "\n",
    "### One-dimensional random walk (Brownian motion)\n",
    "\n",
    "More information: [Random Walks, Central Limit Theorem](https://www.youtube.com/watch?v=BUJCF900I0A)\n",
    "\n",
    "At each step $i$ the process moves with +1 or -1 with equal probability, ie, $X_i \\in \\{+1, -1\\}$ with $P(X_i=+1)=P(X_i=-1)=1/2$.\n",
    "Steps $X_i$'s are i.i.d..\n",
    "\n",
    "Let $S_n = \\sum^n_i X_i$, or $S_i$ (at time $i$) is $S_i = S_{i-1} + X_i$\n",
    "\n",
    "Realizations of random walks obtained by Monte Carlo simulation\n",
    "Plot Few random walks (trajectories), ie, $S_n$ for $n=0$ to $200$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)  # make the example reproducible\n",
    "\n",
    "n = 200 # trajectory depth\n",
    "nsamp = 50000 #nb of trajectories\n",
    "\n",
    "# X: each row (axis 0) contains one trajectory axis 1\n",
    "#Xn = np.array([np.random.choice(a=[-1, +1], size=n,\n",
    "#                                replace=True, p=np.ones(2) / 2)\n",
    "#               for i in range(nsamp)])\n",
    "\n",
    "Xn = np.array([np.random.choice(a=np.array([-1, +1]), size=n,\n",
    "                                replace=True, p=np.ones(2)/2)\n",
    "               for i in range(nsamp)])\n",
    "\n",
    "# Sum of random walks (trajectories)\n",
    "Sn = Xn.sum(axis=1)\n",
    "\n",
    "print(\"True Stat. Mean={:.03f}, Sd={:.02f}\".\\\n",
    "    format(0, np.sqrt(n) * 1))\n",
    "\n",
    "print(\"Est. Stat. Mean={:.03f}, Sd={:.02f}\".\\\n",
    "    format(Sn.mean(), Sn.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative sum of 100 random walks (trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sn_traj = Xn[:100, :].cumsum(axis=1)\n",
    "_ = pd.DataFrame(Sn_traj.T).plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of $S_n$ vs $\\mathcal{N}(0, \\sqrt(n))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low, x_high = Sn.mean()-3*Sn.std(), Sn.mean()+3*Sn.std()\n",
    "h_ = plt.hist(Sn, range=(x_low, x_high), density=True, bins=43, fill=False,\n",
    "             label=\"Histogram\")\n",
    "\n",
    "x_range = np.linspace(x_low, x_high, 30)\n",
    "prob_x_range = scipy.stats.norm.pdf(x_range, loc=Sn.mean(), scale=Sn.std())\n",
    "plt.plot(x_range, prob_x_range, 'r-', label=\"PDF: P(X)\")\n",
    "_ = plt.legend()\n",
    "#print(h_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Tests\n",
    "\n",
    "[Permutation test](https://en.wikipedia.org/wiki/Permutation_test):\n",
    "\n",
    "- The test involves two or more samples assuming that values can be **randomly permuted** under the **null hypothesis**.\n",
    "- The test is **Resampling procedure to estimate the distribution of a parameter or any statistic under the null hypothesis**, i.e., calculated on the permuted data. This parameter or any statistic is called the **estimator**.\n",
    "- **Statistical inference** is conducted by computing the proportion of permuted values of the estimator that are \"more extreme\" than the true one, providing an estimation of the *p-value*.\n",
    "- Permutation tests are a subset of non-parametric statistics, useful when the distribution of the estimator (under H0) is unknown or requires complicated formulas.\n",
    "\n",
    "\n",
    "Permutation test procedure\n",
    "\n",
    "![Bootstrapping procedure](images/stat_random_permutations.png)\n",
    "\n",
    "\n",
    "\n",
    "1. Estimate the observed parameter or statistic $\\hat{\\theta} = S(X)$ on the initial dataset $X$ of size $N$. We call it the observed statistic.\n",
    "2. Generate $R$ samples (called randomized samples) $[X_1, \\ldots X_r, \\ldots X_R]$ from the initial dataset by permutation of the values between the two samples.\n",
    "3. Distribution of the estimator under HO: For each random sample $r$, compute the estimator $\\hat{\\theta}_r = S(X_r)$. The set of $\\{\\hat{\\theta}_r\\}$ provides an estimate the distribution of $P(\\theta | H0)$ (under the null hypothesis).\n",
    "4. Compute statistics of the estimator under the null hypothesis using randomized estimates $\\hat{\\theta}_r$'s:\n",
    "\n",
    "- Mean (under $H0$):\n",
    "$$\n",
    "\\bar{\\theta}_R = \\frac{1}{r}\\sum_{r=1}^R \\hat{\\theta}_r\n",
    "$$\n",
    "\n",
    "- Standard Error $\\hat{SE}_{\\theta_R}$ (under $H0$):\n",
    "$$\n",
    "\\hat{SE}_{\\theta_R} = \\sqrt{\\frac{1}{R-1}\\sum_{r=1}^K (\\bar{\\theta}_R - \\hat{\\theta}_r)^2}\n",
    "$$\n",
    "\n",
    "\n",
    "- Compute p-value using the distribution (under $H0$):\n",
    "\n",
    "  * One-sided p-value:\n",
    "    $$\n",
    "    P(\\theta \\ge \\hat{\\theta} | H0) \\approx \\frac{\\mathbf{card}(\\hat{\\theta}_r \\ge \\hat{\\theta})}{R}\n",
    "    $$\n",
    "\n",
    "  * Two-sided p-value:\n",
    "    $$\n",
    "    P(\\theta \\le \\hat{\\theta}~\\text{or}~\\theta \\ge \\hat{\\theta} | H0) \\approx \\frac{\\mathbf{card}(\\hat{\\theta}_r \\le \\hat{\\theta}) + \\mathbf{card}(\\hat{\\theta}_r \\ge \\hat{\\theta})}{R}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(x, estimator, n_perms=1000):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array\n",
    "        the datasets\n",
    "    estimator : callable\n",
    "        the estimator function taking x as argument returning the estimator value\n",
    "        (scalar) \n",
    "    n_perms : int, optional\n",
    "        the number of permutation, by default 1000\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    Observed estimate\n",
    "    Mean of randomized estimates\n",
    "    Standard Error of randomized estimates\n",
    "    Two-sided p-value\n",
    "    Randomized distribution estimates (density_values, bins)\n",
    "    \"\"\"\n",
    "       \n",
    "    # 1. Estimate the observed parameter or statistic\n",
    "    theta = estimator(x)\n",
    "\n",
    "    # 2. Permuted samples\n",
    "    # Randomly pick the sign with the function:\n",
    "    # np.random.choice([-1, 1], size=len(x), replace=True)\n",
    "\n",
    "    x_R = [np.random.choice([-1, 1], size=len(x), replace=True) * x\n",
    "        for boot_i in range(n_perms)]\n",
    "\n",
    "    # 3. Distribution of the parameter or statistic under H0\n",
    "\n",
    "    thetas_R = np.array([estimator(x_r) for x_r in x_R])\n",
    "    theta_density_R, bins = np.histogram(thetas_R, bins=50, density=True)\n",
    "    dx = np.diff(bins)\n",
    "\n",
    "    # 4. Randomized Statistics\n",
    "\n",
    "    # Mean of randomized estimates\n",
    "    theta_mean_R = np.mean(thetas_R)\n",
    "\n",
    "    # Standard Error of randomized estimates\n",
    "    theta_se_R = np.sqrt(1 / (n_perms - 1) * \n",
    "                         np.sum((theta_mean_R - thetas_R) ** 2))\n",
    "\n",
    "    # 4. Compute two-sided p-value using the distribution under H0:\n",
    "\n",
    "    extream_vals = (thetas_R <= -np.abs(theta)) | (thetas_R >= np.abs(theta))\n",
    "    pval = np.sum(extream_vals) / n_perms\n",
    "    # We could use:\n",
    "    # (np.sum(thetas_R <= -np.abs(theta)) + \\\n",
    "    #  np.sum(thetas_R >=  np.abs(theta))) / n_perms\n",
    "    \n",
    "    return theta, theta_mean_R, theta_se_R, pval, (theta_density_R, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example, we load the monthly revenue figures of for 100 stores before and after a marketing campaigns. We compute the difference ($x_i = x_i^\\text{after} - x_i^\\text{before}$) for each store $i$. Under the null hypothesis, i.e., no effect of the campaigns, $x_i^\\text{after}$ and $x_i^\\text{before}$ could be permuted, which is equivalent to randomly switch the sign of $x_i$. Here we will focus on the sample mean $\\hat{\\theta} = S(X) = 1/n\\sum_i x_i$ as statistic of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/Monthly Revenue (in thousands).csv\")\n",
    "# Reshape Keep only the 30 first samples\n",
    "df = df.pivot(index='store_id', columns='time', values='revenue')[:30]\n",
    "df.after -= 3 # force to smaller effect size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.after - df.before\n",
    "\n",
    "plt.hist(x, fill=False)\n",
    "plt.axvline(x=0, color=\"b\", label=r'No effect: 0')\n",
    "plt.axvline(x=x.mean(), color=\"r\", ls='-', label=r'$\\bar{x}=%.2f$' % x.mean())\n",
    "plt.legend()\n",
    "_ = plt.title(r'Distribution of the sales changes $x_i = x_i^\\text{after} - \\\n",
    "    x_i^\\text{before}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15) # set the seed for reproducible results\n",
    "\n",
    "theta, theta_mean_R, theta_se_R, pval, (theta_density_R, bins) = \\\n",
    "    randomize(x=x, estimator=np.mean, n_perms=10000)\n",
    "\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\\\n",
    "    format(theta, theta_mean_R, pval, theta_se_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pystatsml.plot_utils.plot_pvalue_under_h0(stat_vals=bins[1:], stat_probs=theta_density_R,\n",
    "                     stat_obs=theta,  stat_h0=0, bar_width=np.diff(bins),\n",
    "                     thresh_low=-np.abs(theta), thresh_high=np.abs(theta))\n",
    "_ = plt.title(r'Randomized distribution of the estimator $\\hat{\\theta}_b$ (sample mean)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar procedure can be conducted with many statistic e.g., the t-statistic (same results):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttstat(x):\n",
    "    return (np.mean(x) - 0) / np.std(x, ddof=1) * np.sqrt(len(x))\n",
    "\n",
    "\n",
    "np.random.seed(15) # set the seed for reproducible results\n",
    "theta, theta_mean_R, theta_se_R, pval, (theta_density_R, bins) = \\\n",
    "    randomize(x=x, estimator=ttstat, n_perms=10000)\n",
    "\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\\\n",
    "    format(theta, theta_mean_R, pval, theta_se_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Or the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(15) # set the seed for reproducible results\n",
    "theta, theta_mean_R, theta_se_R, pval, (theta_density_R, bins) = \\\n",
    "    randomize(x=x, estimator=np.median, n_perms=10000)\n",
    "\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\\\n",
    "    format(theta, theta_mean_R, pval, theta_se_R))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)):\n",
    "\n",
    "- **Resampling procedure** to estimate the distribution of a statistic or parameter of interest, called the estimator.\n",
    "- Derive estimates of **variability for estimator** (bias, standard error, confidence intervals, etc.). \n",
    "- **Statistical inference** is conducted by looking the [confidence interval](https://www.researchgate.net/publication/296473825_Bootstrap_Confidence_Intervals_for_Noise_Indicators) **(CI) contains the null hypothesis**.\n",
    "- Nonparametric approach statistical inference, useful when model assumptions is in doubt, unknown or requires complicated formulas. \n",
    "- **Bootstrapping with replacement** has favorable performances ([Efron 1979](https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full), and [1986](https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815)) compared to prior methods like the jackknife that sample without replacement\n",
    "- **Regularize models** by fitting several models on bootstrap samples and averaging their predictions (see Baging and random-forest). See machine learning chapter.\n",
    "\n",
    "Note that compared to random permutation, bootstrapping sample the distribution under the **alternative hypothesis**, it doesn't consider the distribution under the null hypothesis.\n",
    "A great advantage of bootstrap is its **simplicity of the procedure**:\n",
    "\n",
    "![Bootstrapping procedure](images/stat_bootstrapping.png)\n",
    "\n",
    "1. Compute the estimator $\\hat{\\theta} = S(X)$ on the initial dataset $X$ of size $N$.\n",
    "2. Generate $B$ samples (called bootstrap samples) $[X_1, \\ldots X_b, \\ldots X_B]$ from the initial dataset by randomly drawing **with replacement** $N$ observations.\n",
    "3. For each sample $b$ compute the estimator $\\hat{\\theta}_b = S(X_b)$, which provides the bootstrap distribution $P_{\\theta_B}$ of the estimator.\n",
    "4. Compute statistics of the estimator on bootstrap estimates $\\hat{\\theta}_b$'s:\n",
    "\n",
    "- Bootstrap estimate (of the parameters):\n",
    "$$\n",
    "\\bar{\\theta}_B = \\frac{1}{B}\\sum_{b=1}^B \\hat{\\theta}_b\n",
    "$$\n",
    "\n",
    "- Bias = bootstrap estimate - estimate:\n",
    "$$\n",
    "\\hat{b}_{\\theta_B} = \\bar{\\theta}_B - \\hat{\\theta}\n",
    "$$\n",
    "\n",
    "- Standard error $\\hat{S}_{\\theta_B}$:\n",
    "$$\n",
    "\\hat{S}_{\\theta_B} = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B (\\bar{\\theta}_B - \\hat{\\theta}_b)^2}\n",
    "$$\n",
    "\n",
    "- Confidence interval using the estimated bootstrapped distribution of the estimator:\n",
    "$$\n",
    "CI_{95\\%}=[\\hat{\\theta}_1=Q(2.5\\%), \\hat{\\theta}_2=Q(97.5\\%)]~\\text{i.e., the 2.5\\%, 97.5\\% quantiles estimators out of the}~\\hat{\\theta}_b's\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application using the monthly revenue of 100 stores before and after a marketing campaigns, using the difference ($x_i = x_i^\\text{after} - x_i^\\text{before}$) for each store $i$. If the average difference $\\bar{x}=1/n \\sum_i x_i$ is positive (resp. negative), then the marketing campaigns will be considered as efficient (resp. detrimental). We will use bootstrapping to compute the confidence interval (CI) and see if 0 in comprised in the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.after - df.before\n",
    "S = np.mean\n",
    "\n",
    "# 1. Model parameters\n",
    "theta_hat = S(x)\n",
    "\n",
    "np.random.seed(15) # set the seed for reproducible results\n",
    "B = 1000 # Number of bootstrap\n",
    "\n",
    "# 2. Bootstrapped samples\n",
    "\n",
    "x_B = [np.random.choice(x, size=len(x), replace=True) for boot_i in range(B)]\n",
    "\n",
    "# 3. Bootstrap estimates and distribution\n",
    "\n",
    "theta_hats_B = np.array([S(x_b) for x_b in x_B])\n",
    "theta_density_B, bins = np.histogram(theta_hats_B, bins=50, density=True)\n",
    "dx = np.diff(bins)\n",
    "\n",
    "# 4. Bootstrap Statistics\n",
    "\n",
    "# Bootstrap estimate\n",
    "theta_bar_B = np.mean(theta_hats_B)\n",
    "\n",
    "# Bias = bootstrap estimate - estimate\n",
    "bias_hat_B = theta_bar_B - theta_hat\n",
    "\n",
    "# Standard Error\n",
    "se_hat_B = np.sqrt(1 / (B - 1) * np.sum((theta_bar_B - theta_hats_B) ** 2))\n",
    "\n",
    "# Confidence interval using the estimated bootstrapped distribution of estimator\n",
    "\n",
    "ci95 = np.quantile(a=theta_hats_B, q=[0.025, 0.975])\n",
    "\n",
    "print(\n",
    "    \"Est.: {:.2f}, Boot Est.: {:.2f}, bias: {:e},\\\n",
    "     Boot SE: {:.2f}, CI: [{:.5f}, {:.5f}]\"\\\n",
    "    .format(theta_hat, theta_bar_B, bias_hat_B, se_hat_B, ci95[0], ci95[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Zero is outside the CI, moreover $\\bar{X}$ is positive. Thus we can conclude the marketing campaign produced a significant increase of the sales.\n",
    "\n",
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bins[1:], theta_density_B, width=dx, fill=False, label=r'$P_{\\theta_B}$')\n",
    "plt.axvline(x=theta_hat, color='r', ls='--', lw=2, label=r'$\\hat{\\theta}$ (Obs. Stat.)')\n",
    "plt.axvline(x=theta_bar_B, color=\"b\", ls='-', label=r'$\\bar{\\theta}_B$')\n",
    "plt.axvline(x=ci95[0], color=\"b\", ls='--', label=r'$CI_{95\\%}$')\n",
    "plt.axvline(x=ci95[1], color=\"b\", ls='--')\n",
    "plt.axvline(x=0, color=\"k\", lw=2, label=r'No effect: 0')\n",
    "plt.legend()\n",
    "_ = plt.title(r'Bootstrap distribution of the estimator $\\hat{\\theta}_b$ (sample mean)')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystatsml_teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
