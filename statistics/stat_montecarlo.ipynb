{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling and Monte Carlo Methods\n",
    "\n",
    "Sources:\n",
    "\n",
    "- [Scipy Resampling and Monte Carlo Methods](https://docs.scipy.org/doc/scipy/tutorial/stats/resampling.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manipulate data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Statistics\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "# import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pystatsml.plot_utils\n",
    "\n",
    "# Plot parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig_w, fig_h = plt.rcParams.get('figure.figsize')\n",
    "plt.rcParams['figure.figsize'] = (fig_w, fig_h * .5)\n",
    "colors = sns.color_palette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Monte-Carlo simulation of Random Walk Process\n",
    "\n",
    "\n",
    "### One-dimensional random walk (Brownian motion)\n",
    "\n",
    "More information: [Random Walks, Central Limit Theorem](https://www.youtube.com/watch?v=BUJCF900I0A)\n",
    "\n",
    "At each step $i$ the process moves with +1 or -1 with equal probability, ie, $X_i \\in \\{+1, -1\\}$ with $P(X_i=+1)=P(X_i=-1)=1/2$.\n",
    "Steps $X_i$'s are i.i.d..\n",
    "\n",
    "Let $S_n = \\sum^n_i X_i$, or $S_i$ (at time $i$) is $S_i = S_{i-1} + X_i$\n",
    "\n",
    "Realizations of random walks obtained by Monte Carlo simulation\n",
    "Plot Few random walks (trajectories), ie, $S_n$ for $n=0$ to $200$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=42)  # make the example reproducible\n",
    "\n",
    "n = 200  # trajectory depth\n",
    "nsamp = 50000  # nb of trajectories\n",
    "\n",
    "# X: each row (axis 0) contains one trajectory axis 1\n",
    "# Xn = np.array([np.random.choice(a=[-1, +1], size=n,\n",
    "#                                replace=True, p=np.ones(2) / 2)\n",
    "#               for i in range(nsamp)])\n",
    "\n",
    "Xn = np.array([np.random.choice(a=np.array([-1, +1]), size=n,\n",
    "                                replace=True, p=np.ones(2)/2)\n",
    "               for i in range(nsamp)])\n",
    "\n",
    "# Sum of random walks (trajectories)\n",
    "Sn = Xn.sum(axis=1)\n",
    "\n",
    "print(\"True Stat. Mean={:.03f}, Sd={:.02f}\".\n",
    "      format(0, np.sqrt(n) * 1))\n",
    "\n",
    "print(\"Est. Stat. Mean={:.03f}, Sd={:.02f}\".\n",
    "      format(Sn.mean(), Sn.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot cumulative sum of 100 random walks (trajectories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sn_traj = Xn[:100, :].cumsum(axis=1)\n",
    "_ = pd.DataFrame(Sn_traj.T).plot(legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of $S_n$ vs $\\mathcal{N}(0, \\sqrt(n))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_low, x_high = Sn.mean()-3*Sn.std(), Sn.mean()+3*Sn.std()\n",
    "h_ = plt.hist(Sn, range=(x_low, x_high), density=True, bins=43, fill=False,\n",
    "              label=\"Histogram\")\n",
    "\n",
    "x_range = np.linspace(x_low, x_high, 30)\n",
    "prob_x_range = scipy.stats.norm.pdf(x_range, loc=Sn.mean(), scale=Sn.std())\n",
    "plt.plot(x_range, prob_x_range, 'r-', label=\"PDF: P(X)\")\n",
    "_ = plt.legend()\n",
    "# print(h_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Tests\n",
    "\n",
    "[Permutation test](https://en.wikipedia.org/wiki/Permutation_test):\n",
    "\n",
    "- The test involves two or more samples assuming that values can be **randomly permuted** under the **null hypothesis**.\n",
    "- The test is **Resampling procedure to estimate the distribution of a parameter or any statistic under the null hypothesis**, i.e., calculated on the permuted data. This parameter or any statistic is called the **estimator**.\n",
    "- **Statistical inference** is conducted by computing the proportion of permuted values of the estimator that are \"more extreme\" than the true one, providing an estimation of the *p-value*.\n",
    "- Permutation tests are a subset of non-parametric statistics, useful when the distribution of the estimator (under H0) is unknown or requires complicated formulas.\n",
    "\n",
    "\n",
    "![Permutation test procedure](images/stat_random_permutations.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Estimate the observed parameter or statistic**:\n",
    "$$\n",
    "T^{\\text{obs}}=S(X)\n",
    "$$\n",
    "on the initial dataset $X$ of size $N$. We call it the observed statistic.\n",
    "\n",
    "Application to mean of one sample.\n",
    "Note that for genericity purposes, the proposed Python implementation can take an iterable list of arrays and calculate several statistics for several variables at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(*x):\n",
    "    return np.mean(x[0], axis=0)\n",
    "\n",
    "\n",
    "# Can manage two variables, and return two statistics\n",
    "x = np.array([[1, -1], [2, -2], [3, -3], [4, -4]])\n",
    "print(mean(x))\n",
    "\n",
    "# In our case with one variable\n",
    "x = np.array([-1, 0, +1])\n",
    "stat_obs = mean(x)\n",
    "print(stat_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**2. Generate B samples (called randomized samples)** $[X_1, \\ldots X_b, \\ldots X_B]$ from the initial dataset using a adapted permutation scheme and compute the estimator (under H0):\n",
    "$$\n",
    "T^{(b)} = S(X_b).\n",
    "$$\n",
    "\n",
    "First, we need a permutation scheme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onesample_sign_permutation(*x):\n",
    "    \"\"\"Randomly change the sign of all datsets\"\"\"\n",
    "    return [np.random.choice([-1, 1], size=len(x_), replace=True) * x_\n",
    "            for x_ in x]\n",
    "\n",
    "\n",
    "x = np.array([-1, 0, +1])\n",
    "# 10 random sign permutation of [-1, 0, +1] and mean calculation\n",
    "# Mean([-1,0,1])= 0 or Mean([1,0,1])= 0.66 or Mean([-1,0,-1])= -0.66\n",
    "np.random.seed(42)\n",
    "stats_rnd = [float(mean(*onesample_sign_permutation(x)).round(3))\n",
    "             for perm in range(10)]\n",
    "print(stats_rnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a parallelized permutation procedure using [Joblib](https://joblib.readthedocs.io/en/stable/) whose code is available [here](https://github.com/duchesnay/pystatsml/blob/master/lib/pystatsml/resampling.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml import resampling\n",
    "\n",
    "stats_rnd = np.array(resampling.resample_estimate(x, estimator=mean,\n",
    "                        resample=onesample_sign_permutation, n_resamples=10))\n",
    "print(stats_rnd.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Permutation test: Compute statistics of the estimator** under the null hypothesis using randomized estimates $T^{(b)}$'s. \n",
    "\n",
    "- Mean (under $H0$):\n",
    "$$\n",
    "\\bar{T}_B = \\frac{1}{B}\\sum_{b=1}^B T^{(b)}\n",
    "$$\n",
    "\n",
    "- Standard Error $\\sigma_{T_B}$ (under $H0$):\n",
    "$$\n",
    "\\sigma_{T_B} = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B (\\bar{T}_B - T^{(b)})^2}\n",
    "$$\n",
    "\n",
    "\n",
    "- $T^{(b)}$'s provides an estimate the distribution of $P(T | H0)$ necessary to compute p-value using the distribution (under H0):\n",
    "\n",
    "  * One-sided p-value:\n",
    "    $$\n",
    "    P(T \\ge T^{\\text{obs}} | H0) \\approx \\frac{\\mathbf{card}(T^{(b)} \\ge T^{\\text{obs}})}{B}\n",
    "    $$\n",
    "\n",
    "  * Two-sided p-value:\n",
    "    $$\n",
    "    P(T\\le T^{\\text{obs}}~\\text{or}~T \\ge T^{\\text{obs}} | H0) \\approx \\frac{\\mathbf{card}(T^{(b)} \\le T^{\\text{obs}}) + \\mathbf{card}(T^{(b)} \\ge T^{\\text{obs}})}{B}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let randomized samples) $[X_1, \\ldots X_r, \\ldots X_rnd]$ from the initial dataset using a adapted permutation scheme and compute the estimator (under HO): $T^{(b)} = S(X_r)$.\n",
    "\n",
    "3. Permutation test: Compute statistics of the estimator under the null hypothesis using randomized estimates $T^{(b)}$'s. \n",
    "\n",
    "- Mean (under $H0$):\n",
    "$$\n",
    "\\bar{T}_B = \\frac{1}{r}\\sum_{r=1}^R T^{(b)}\n",
    "$$\n",
    "\n",
    "- Standard Error $\\sigma_{T_B}$ (under $H0$):\n",
    "$$\n",
    "\\sigma_{T_B} = \\sqrt{\\frac{1}{R-1}\\sum_{r=1}^K (\\bar{T}_B - T^{(b)})^2}\n",
    "$$\n",
    "\n",
    "\n",
    "- $T^{(b)}$'s provides an estimate the distribution of $P(T | H0)$ necessary to compute p-value using the distribution (under $H0$):\n",
    "\n",
    "  * One-sided p-value:\n",
    "    $$\n",
    "    P(T\\ge T^{\\text{obs}} | H0) \\approx \\frac{\\mathbf{card}(T^{(b)} \\ge T^{\\text{obs}})}{R}\n",
    "    $$\n",
    "\n",
    "  * Two-sided p-value:\n",
    "    $$\n",
    "    P(T\\le T^{\\text{obs}}~\\text{or}~T \\ge T^{\\text{obs}} | H0) \\approx \\frac{\\mathbf{card}(T^{(b)} \\le T^{\\text{obs}}) + \\mathbf{card}(T^{(b)} \\ge T^{\\text{obs}})}{R}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Permutation test: Compute statistics under H0, estimates distribution and\n",
    "# compute p-values\n",
    "\n",
    "def permutation_test(stat_obs, stats_rnd):\n",
    "    \"\"\"Compute permutation test using statistic under H1 (true statistic)\n",
    "    and statistics under H0 (randomized statistics) for several statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stat_obs : array of shape (nb_statistics)\n",
    "        statistic under H1 (true statistic)\n",
    "    stats_rnd : array of shape (nb_permutations, nb_statistics)\n",
    "        statistics under H0 (randomized statistics)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        p-value, statistics mean under HO, statistics stan under HO\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> np.random.seed(42)\n",
    "    >>> stats_rnd = np.random.randn(1000, 5)\n",
    "    >>> stat_obs = np.arange(5)\n",
    "    >>> pval, stat_mean_rnd, stat_se_rnd = permutation_test(stat_obs, stats_rnd)\n",
    "    >>> print(pval)\n",
    "    [1.    0.315 0.049 0.002 0.   ]\n",
    "    \"\"\"\n",
    "\n",
    "    n_perms = stats_rnd.shape[0]\n",
    "\n",
    "    # 1. Randomized Statistics\n",
    "\n",
    "    # Mean of randomized estimates\n",
    "    stat_mean_rnd = np.mean(stats_rnd, axis=0)\n",
    "\n",
    "    # Standard Error of randomized estimates\n",
    "    stat_se_rnd = np.sqrt(1 / (n_perms - 1) *\n",
    "                          np.sum((stat_mean_rnd - stats_rnd) ** 2, axis=0))\n",
    "\n",
    "    # 2. Compute two-sided p-value using the distribution under H0:\n",
    "\n",
    "    extreme_vals = (stats_rnd <= -np.abs(stat_obs)\n",
    "                    ) | (stats_rnd >= np.abs(stat_obs))\n",
    "    pval = np.sum(extreme_vals, axis=0) / n_perms\n",
    "    # We could use:\n",
    "    # (np.sum(stats_rnd <= -np.abs(stat_obs)) + \\\n",
    "    #  np.sum(stats_rnd >=  np.abs(stat_obs))) / n_perms\n",
    "\n",
    "    # 3. Distribution of the parameter or statistic under H0\n",
    "    # stat_density_rnd, bins = np.histogram(stats_rnd, bins=50, density=True)\n",
    "    # dx = np.diff(bins)\n",
    "\n",
    "    return pval, stat_mean_rnd, stat_se_rnd\n",
    "\n",
    "\n",
    "pval, stat_mean_rnd, stat_se_rnd = permutation_test(stat_obs, stats_rnd)\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\n",
    "      format(stat_obs, stat_mean_rnd, pval, stat_se_rnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Sample Sign Permutation\n",
    "\n",
    "Consider the monthly revenue figures of for 100 stores before and after a marketing campaigns. We compute the difference ($x_i = x_i^\\text{after} - x_i^\\text{before}$) for each store $i$. Under the null hypothesis, i.e., no effect of the campaigns, $x_i^\\text{after}$ and $x_i^\\text{before}$ could be permuted, which is equivalent to randomly switch the sign of $x_i$. Here we will focus on the sample mean $T^{\\text{obs}} = S(X) = 1/n\\sum_i x_i$ as statistic of interest.\n",
    "\n",
    "Read data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/Monthly Revenue (in thousands).csv\")\n",
    "# Reshape Keep only the 30 first samples\n",
    "df = df.pivot(index='store_id', columns='time', values='revenue')[:30]\n",
    "df.after -= 3  # force to smaller effect size\n",
    "\n",
    "x = df.after - df.before\n",
    "\n",
    "plt.hist(x, fill=False)\n",
    "plt.axvline(x=0, color=\"b\", label=r'No effect: 0')\n",
    "plt.axvline(x=x.mean(), color=\"r\", ls='-', label=r'$\\bar{x}=%.2f$' % x.mean())\n",
    "plt.legend()\n",
    "_ = plt.title(\n",
    "    r'Distribution of the sales changes $x_i = x_i^\\text{after} - x_i^\\text{before}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Estimate the observed parameter or statistic\n",
    "stat_obs = mean(x)\n",
    "\n",
    "# 2. Generate randomized samples and compute the estimator (under HO)\n",
    "stats_rnd = np.array(resampling.resample_estimate(x, estimator=mean,\n",
    "                        resample=onesample_sign_permutation, n_resamples=1000))\n",
    "\n",
    "# 3. Permutation test: Compute stats. under H0, and p-values\n",
    "pval, stat_mean_rnd, stat_se_rnd = permutation_test(stat_obs, stats_rnd)\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\n",
    "      format(stat_obs, stat_mean_rnd, pval, stat_se_rnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_density_rnd, bins = np.histogram(stats_rnd, bins=50, density=True)\n",
    "dx = np.diff(bins)\n",
    "\n",
    "pystatsml.plot_utils.plot_pvalue_under_h0(stat_vals=bins[1:],\n",
    "                    stat_probs=stats_density_rnd,\n",
    "                    stat_obs=stat_obs,  stat_h0=0, bar_width=np.diff(bins),\n",
    "                    thresh_low=-np.abs(stat_obs), thresh_high=np.abs(stat_obs))\n",
    "_ = plt.title(r'Randomized distribution of the estimator $T$ (sample mean)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar procedure can be conducted with many statistic e.g., the t-statistic (more sensitive than the mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_1samp(*x):\n",
    "    x = x[0]\n",
    "    return (np.mean(x, axis=0) - 0) / np.std(x, ddof=1, axis=0) * np.sqrt(len(x))\n",
    "\n",
    "\n",
    "# 1. Estimate the observed parameter or statistic\n",
    "stat_obs = ttest_1samp(x)\n",
    "\n",
    "# 2. Generate randomized samples and compute the estimator (under HO)\n",
    "stats_rnd = np.array(resampling.resample_estimate(x, estimator=ttest_1samp,\n",
    "                        resample=onesample_sign_permutation, n_resamples=1000))\n",
    "\n",
    "# 3. Permutation test: Compute stats. under H0, and p-values\n",
    "pval, stat_mean_rnd, stat_se_rnd = permutation_test(stat_obs, stats_rnd)\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\n",
    "      format(stat_obs, stat_mean_rnd, pval, stat_se_rnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Or the median  (less sensitive than the mean):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median(*x):\n",
    "    x = x[0]\n",
    "    return np.median(x, axis=0)\n",
    "\n",
    "\n",
    "# 1. Estimate the observed parameter or statistic\n",
    "stat_obs = median(x)\n",
    "\n",
    "# 2. Generate randomized samples and compute the estimator (under HO)\n",
    "stats_rnd = np.array(resampling.resample_estimate(x, estimator=median,\n",
    "                        resample=onesample_sign_permutation, n_resamples=1000))\n",
    "\n",
    "# 3. Permutation test: Compute stats. under H0, and p-values\n",
    "pval, stat_mean_rnd, stat_se_rnd = permutation_test(stat_obs, stats_rnd)\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\n",
    "      format(stat_obs, stat_mean_rnd, pval, stat_se_rnd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Samples Permutation\n",
    "\n",
    "$x$ is a variable $y \\in \\{0, 1\\}$ is a sample label for two groups. To obtain sample under HO\n",
    "we just have to permute the group's labels $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "from pystatsml import datasets\n",
    "\n",
    "x, y = datasets.make_twosamples(n_samples=30, n_features=1, n_informative=1,\n",
    "                                group_scale=1., noise_scale=1., shared_scale=0,\n",
    "                                random_state=0)\n",
    "\n",
    "print(scipy.stats.ttest_ind(x[y == 0], x[y == 1], equal_var=True))\n",
    "\n",
    "# Label permutation, expect label_permutation(x, label)\n",
    "\n",
    "\n",
    "def label_permutation(*x):\n",
    "    x, label = x[0], x[1]\n",
    "    label = np.random.permutation(label)\n",
    "    return x, label\n",
    "\n",
    "\n",
    "xr, yr = label_permutation(x, y)\n",
    "assert np.all(x == xr)\n",
    "print(\"Original labels:\", y)\n",
    "print(\"Permuted labels:\", yr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As statistic we use the [student statistic of two sample test with equal variance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twosample_ttest(*x):\n",
    "    x, label = x[0], x[1]\n",
    "    ttest = scipy.stats.ttest_ind(x[label == 0], x[label == 1],\n",
    "                                  equal_var=True, axis=0)\n",
    "\n",
    "    return ttest.statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Estimate the observed parameter or statistic\n",
    "stat_obs = twosample_ttest(x, y)\n",
    "\n",
    "# 2. Generate randomized samples and compute the estimator (under HO)\n",
    "\n",
    "# Sequential version:\n",
    "# stats_rnd = np.array([twosample_ttest(*label_permutation(x, y))\n",
    "#                       for perm in range(1000)])\n",
    "\n",
    "# Parallel version:\n",
    "stats_rnd = np.array(resampling.resample_estimate(x, y, estimator=twosample_ttest,\n",
    "                     resample=label_permutation, n_resamples=1000))\n",
    "\n",
    "# 3. Permutation test: Compute stats. under H0, and p-values\n",
    "pval, stat_mean_rnd, stat_se_rnd = permutation_test(stat_obs, stats_rnd)\n",
    "print(\"Estimate: {:.2f}, Mean(Estimate|HO): {:.4f}, p-val: {:e}, SE: {:.3f}\".\n",
    "      format(stat_obs, stat_mean_rnd, pval, stat_se_rnd))\n",
    "\n",
    "\n",
    "print(\"\\nNon permuted t-test:\")\n",
    "ttest = scipy.stats.ttest_ind(x[y == 0], x[y == 1], equal_var=True)\n",
    "print(\"Estimate: {:.2f}, p-val: {:e}\".\n",
    "      format(ttest.statistic, ttest.pvalue))\n",
    "\n",
    "print(\"\\nPermutation using scipy.stats.ttest_ind\")\n",
    "ttest = scipy.stats.ttest_ind(x[y == 0], x[y == 1], equal_var=True,\n",
    "                              method=scipy.stats.PermutationMethod())\n",
    "print(\"Estimate: {:.2f}, p-val: {:e}\".\n",
    "      format(ttest.statistic, ttest.pvalue))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation-Based Correction for Multiple Testing: Westfall and Young Correction\n",
    "\n",
    "[Westfall and Young (1993)](https://www.jstor.org/stable/2532216) Correction for Multiple Testing, also known as maxT, controls the **Family-Wise Error Rate (FWER)** in the context of **multiple hypothesis testing**, particularly in high-dimensional settings (e.g., genomics), where traditional methods like Bonferroni are overly conservative.\n",
    "\n",
    "- The method relies on **permutation testing** to empirically estimate the distribution of test statistics under the **global null hypothesis**.\n",
    "- It accounts for the **dependence structure** between tests, improving power compared to Bonferroni-type corrections.\n",
    "\n",
    "**Procedure**\n",
    "\n",
    "Let there be $P$ hypotheses $H_1, \\dots, H_P$ and corresponding test statistics $T_1, \\dots, T_P$:\n",
    "\n",
    "1. **Compute observed test statistics** $\\{T_j^{\\text{obs}}\\}_{j=1}^P$, for each hypothesis.\n",
    "\n",
    "2. **Permute the data** $B$ times (e.g., shuffle labels), recomputing all $P$ test statistics each time yielding to a $(B \\times P)$ table with all permuted statistics $\\{T_j^{(b)}\\}_{(b,j)}^{(B,P)}$.\n",
    "\n",
    "3. **For each permutation**, record the **maximum test statistic** across all hypotheses, yielding to a vector of $B$ maximum which value is given by:\n",
    "\n",
    "   $$\n",
    "   M^{(b)} = \\max_{j = 1, \\dots, P} T_j^{(b)}\n",
    "   $$\n",
    "\n",
    "4. For each hypothesis $j$, calculate the adjusted p-value, yielding to a vector of $P$ p-values which value is given by:\n",
    "\n",
    "   $$\n",
    "   \\tilde{p}_j = \\frac{1}{B} \\sum_{b=1}^B \\mathbf{card}(T_j^{\\text{obs}} \\leq M^{(b)})\n",
    "   $$\n",
    "\n",
    "   (For one-sided tests. For two-sided, take $|T_j|$.)\n",
    "\n",
    "5. Reject hypotheses with $\\tilde{p}_j \\leq \\alpha$.\n",
    "\n",
    "\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "* **Strong FWER control**, even under arbitrary dependence.\n",
    "* **More powerful** than Bonferroni or Holm in many dependent testing scenarios.\n",
    "* **Non-parametric**: does not rely on distributional assumptions.\n",
    "\n",
    "\n",
    "**Limitations**\n",
    "\n",
    "* **Computationally intensive**, especially with large $m$ and many permutations.\n",
    "* Requires **exchangeability** of data under the null (a key assumption for valid permutations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pystatsml import datasets\n",
    "\n",
    "n_informative = 100  # Number of features with signal (Positive features)\n",
    "n_features = 1000\n",
    "x, y = datasets.make_twosamples(n_samples=30, n_features=n_features, n_informative=n_informative,\n",
    "                                group_scale=1., noise_scale=1., shared_scale=1.5,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. **Compute observed test statistics**\n",
    "stat_obs = twosample_ttest(x, y)\n",
    "\n",
    "# 2. Randomized statistics\n",
    "stats_rnd = np.array(resampling.resample_estimate(x, y, estimator=twosample_ttest,\n",
    "                     resample=label_permutation, n_resamples=1000))\n",
    "\n",
    "def multipletests_westfall_young(stats_rnd, stat_obs):\n",
    "    \"\"\"Westfall and Young FWER correction for multiple comparisons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    stats_rnd : array (n_resamples, n_features)\n",
    "        Randomized Statistics\n",
    "    stats_true : array (n_features)\n",
    "        Observed Statistics\n",
    "    alpha : float, optional\n",
    "        by default 0.05\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Array (n_features) P-values corrected for multiple comparison\n",
    "    \"\"\"\n",
    "\n",
    "    # 3. **For each permutation**, record the **maximum test statistic**\n",
    "    stat_rnd_max = np.max(np.abs(stats_rnd), axis=1)\n",
    "\n",
    "    # 4. For each hypothesis $j$, calculate the adjusted p-value\n",
    "    pvalues_ws = np.array([np.sum(stat_rnd_max >= np.abs(\n",
    "        stat)) / stat_rnd_max.shape[0] for stat in stat_obs])\n",
    "\n",
    "    return pvalues_ws\n",
    "\n",
    "pvalues_ws = multipletests_westfall_young(stats_rnd, stat_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute p-values using:\n",
    "\n",
    "1. Un-corrected p-values: High false positives rate.\n",
    "2. Bonferroni correction: Reduced sensitivity (low True positives rate).\n",
    "3. Westfall and Young correction: Improved sensitivity with controlled false positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usual t-test with uncorrected p-values:\n",
    "import statsmodels.sandbox.stats.multicomp as multicomp\n",
    "ttest = scipy.stats.ttest_ind(x[y == 0], x[y == 1], equal_var=True)\n",
    "\n",
    "# Classical Bonferroni correction\n",
    "_, pvals_bonf, _, _ = multicomp.multipletests(ttest.pvalue, alpha=0.05,\n",
    "                                              method='bonferroni')\n",
    "\n",
    "def print_positve(title, pvalues, n_informative, n_features):\n",
    "    print('%s \\nPositive: %i/%i, True Positive: %i/%i, False Positive: %i/%i' %\n",
    "          (title,\n",
    "           np.sum(pvalues <= 0.05), n_features,\n",
    "           np.sum(pvalues[:n_informative] <= 0.05), n_informative,\n",
    "           np.sum(pvalues[n_informative:] <= 0.05), (n_features-n_informative)))\n",
    "\n",
    "print_positve(\"No correction\", ttest.pvalue, n_informative, n_features)\n",
    "print_positve(\"Bonferroni correction\", pvals_bonf, n_informative, n_features)\n",
    "print_positve(\"Westfall and Young\", pvalues_ws, n_informative, n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare P-Values distribution with Bonferroni correction for multiple comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "_, q_bonferroni = stats.probplot(pvals_bonf[:n_informative], fit=False)\n",
    "_, q_ws = stats.probplot(pvalues_ws[:n_informative], fit=False)\n",
    "ax = plt.scatter(q_bonferroni, q_ws, marker='o', color=colors[0])\n",
    "ax = plt.axline([0, 0], slope=1, color=\"gray\", ls='--', alpha=0.5)\n",
    "ax = plt.vlines(0.05, 0, 1, color=\"r\", ls='--', alpha=0.5)\n",
    "ax = plt.hlines(0.05, 0, 1, color=\"r\", ls='--', alpha=0.5)\n",
    "ax = plt.xlabel('Bonferroni Correction')\n",
    "ax = plt.ylabel('Westfall & Young Correction')\n",
    "ax = plt.title('Q-Q Plots of P-values distributions Corrected for Mult. Comp.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping\n",
    "\n",
    "[Bootstrapping](https://en.wikipedia.org/wiki/Bootstrapping_(statistics)):\n",
    "\n",
    "- **Resampling procedure** to estimate the distribution of a statistic or parameter of interest, called the estimator.\n",
    "- Derive estimates of **variability for estimator** (bias, standard error, confidence intervals, etc.). \n",
    "- **Statistical inference** is conducted by looking the [confidence interval](https://www.researchgate.net/publication/296473825_Bootstrap_Confidence_Intervals_for_Noise_Indicators) **(CI) contains the null hypothesis**.\n",
    "- Nonparametric approach statistical inference, useful when model assumptions is in doubt, unknown or requires complicated formulas. \n",
    "- **Bootstrapping with replacement** has favorable performances ([Efron 1979](https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full), and [1986](https://projecteuclid.org/download/pdf_1/euclid.ss/1177013815)) compared to prior methods like the jackknife that sample without replacement\n",
    "- **Regularize models** by fitting several models on bootstrap samples and averaging their predictions (see Baging and random-forest). See machine learning chapter.\n",
    "\n",
    "Note that compared to random permutation, bootstrapping sample the distribution under the **alternative hypothesis**, it doesn't consider the distribution under the null hypothesis.\n",
    "A great advantage of bootstrap is its **simplicity of the procedure**:\n",
    "\n",
    "![Bootstrapping procedure](images/stat_bootstrapping.png)\n",
    "\n",
    "1. Compute the estimator $T^{\\text{obs}} = S(X)$ on the initial dataset $X$ of size $N$.\n",
    "2. Generate $B$ samples (called bootstrap samples) $[X_1, \\ldots X_b, \\ldots X_B]$ from the initial dataset by randomly drawing **with replacement** $ of the N$ observations.\n",
    "3. For each sample $b$ compute the estimator $\\{T^{(b)}= S(X_b)\\}_{b=1}^B$, which provides the bootstrap distribution $P(T_B|H1)$ of the estimator (under the alternative hypothesis H1).\n",
    "4. Compute statistics of the estimator on bootstrap estimates $T^{(b)}$'s:\n",
    "\n",
    "- Bootstrap estimate (of the parameters):\n",
    "$$\n",
    "\\bar{T_B} = \\frac{1}{B}\\sum_{b=1}^B T^{(b)}\n",
    "$$\n",
    "\n",
    "- Bias = bootstrap estimate - estimate:\n",
    "$$\n",
    "\\hat{b_{T_B}} = \\bar{T}_B - T^{\\text{obs}}\n",
    "$$\n",
    "\n",
    "- Standard error $\\hat{\\sigma_{T_B}}$:\n",
    "$$\n",
    "\\hat{\\sigma_{T_B}} = \\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B (\\bar{T_B} - T^{(b)})^2}\n",
    "$$\n",
    "\n",
    "- Confidence interval using the estimated bootstrapped distribution of the estimator:\n",
    "\n",
    "$$\n",
    "CI_{95\\%}=[T^{\\text{obs}}_1=Q_{T^{(b)}}(2.5\\%), T^{\\text{obs}}_2=Q_{T^{(b)}}(97.5\\%)]~\\text{i.e., the 2.5\\%, 97.5\\% quantiles estimators out of the}~\\{T^{(b)}\\}_{b=1}^B\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Application using the monthly revenue of 100 stores before and after a marketing campaigns, using the difference ($x_i = x_i^\\text{after} - x_i^\\text{before}$) for each store $i$. If the average difference $\\bar{x}=1/n \\sum_i x_i$ is positive (resp. negative), then the marketing campaigns will be considered as efficient (resp. detrimental). We will use bootstrapping to compute the confidence interval (CI) and see if 0 in comprised in the CI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.after - df.before\n",
    "S = np.mean\n",
    "\n",
    "# 1. Model parameters\n",
    "stat_hat = S(x)\n",
    "\n",
    "np.random.seed(15)  # set the seed for reproducible results\n",
    "B = 1000  # Number of bootstrap\n",
    "\n",
    "# 2. Bootstrapped samples\n",
    "\n",
    "x_B = [np.random.choice(x, size=len(x), replace=True) for boot_i in range(B)]\n",
    "\n",
    "# 3. Bootstrap estimates and distribution\n",
    "\n",
    "stat_hats_B = np.array([S(x_b) for x_b in x_B])\n",
    "stat_density_B, bins = np.histogram(stat_hats_B, bins=50, density=True)\n",
    "dx = np.diff(bins)\n",
    "\n",
    "# 4. Bootstrap Statistics\n",
    "\n",
    "# Bootstrap estimate\n",
    "stat_bar_B = np.mean(stat_hats_B)\n",
    "\n",
    "# Bias = bootstrap estimate - estimate\n",
    "bias_hat_B = stat_bar_B - stat_hat\n",
    "\n",
    "# Standard Error\n",
    "se_hat_B = np.sqrt(1 / (B - 1) * np.sum((stat_bar_B - stat_hats_B) ** 2))\n",
    "\n",
    "# Confidence interval using the estimated bootstrapped distribution of estimator\n",
    "\n",
    "ci95 = np.quantile(a=stat_hats_B, q=[0.025, 0.975])\n",
    "\n",
    "print(\n",
    "    \"Est.: {:.2f}, Boot Est.: {:.2f}, bias: {:e},\\\n",
    "     Boot SE: {:.2f}, CI: [{:.5f}, {:.5f}]\"\n",
    "    .format(stat_hat, stat_bar_B, bias_hat_B, se_hat_B, ci95[0], ci95[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Zero is outside the CI, moreover $\\bar{X}$ is positive. Thus we can conclude the marketing campaign produced a significant increase of the sales.\n",
    "\n",
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(bins[1:], stat_density_B, width=dx, fill=False, label=r'$P_{T_B}$')\n",
    "plt.axvline(x=stat_hat, color='r', ls='--', lw=2,\n",
    "            label=r'$T^{\\text{obs}}$ (Obs. Stat.)')\n",
    "plt.axvline(x=stat_bar_B, color=\"b\", ls='-', label=r'$\\bar{T_B}$')\n",
    "plt.axvline(x=ci95[0], color=\"b\", ls='--', label=r'$CI_{95\\%}$')\n",
    "plt.axvline(x=ci95[1], color=\"b\", ls='--')\n",
    "plt.axvline(x=0, color=\"k\", lw=2, label=r'No effect: 0')\n",
    "plt.legend()\n",
    "_ = plt.title(\n",
    "    r'Bootstrap distribution of the estimator $\\hat{\\theta}_b$ (sample mean)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystatsml_teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
